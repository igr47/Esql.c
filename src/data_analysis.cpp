// ============================================
// Data analysis.h
// ============================================
#include "data_analysis.h"
#include <iostream>
#include <sstream>
#include <iomanip>
#include <algorithm>
#include <cmath>
#include <numeric>
#include <random>
#include <set>
#include <map>
#include <unordered_set>
#include <unordered_map>
#include <limits>
#include <queue>
#include <thread>
#include <future>
#include <mutex>
#include <shared_mutex>
#include <chrono>
#include <filesystem>
#include <fstream>
#include <cstring>
#include <cstdlib>
#include <regex>
#include <complex>
#include <fftw3.h>
#include <eigen3/Eigen/Eigen>
#include <eigen3/Eigen/Dense>
#include <eigen3/Eigen/Core>
#include <eigen3/Eigen/QR>
#include <eigen3/Eigen/SVD>
#include <eigen3/Eigen/Eigenvalues>
#include <boost/math/distributions.hpp>
#include <boost/math/special_functions.hpp>
#include <boost/accumulators/accumulators.hpp>
#include <boost/accumulators/statistics.hpp>
#include <boost/accumulators/statistics/stats.hpp>
#include <boost/accumulators/statistics/mean.hpp>
#include <boost/accumulators/statistics/variance.hpp>
#include <boost/accumulators/statistics/skewness.hpp>
#include <boost/accumulators/statistics/kurtosis.hpp>
#include <nlohmann/json.hpp>

// For parallel processing
#ifdef _OPENMP
#include <omp.h>
#endif

namespace esql {
namespace analysis {

using namespace std::chrono;
using json = nlohmann::json;
namespace fs = std::filesystem;

std::random_device rd;
static std::mt19937_64 gen(rd());
std::uniform_real_distribution<double> dis(0.0, 1.0);


// ============================================
// Implementation
// ============================================

// Constants for numerical stability
constexpr double EPSILON = 1e-12;
constexpr double SQRT_2_PI = 2.5066282746310002;
constexpr double INV_SQRT_2_PI = 0.3989422804014327;
constexpr double LN_2 = 0.6931471805599453;
constexpr double SQRT_2 = 1.4142135623730951;

// Thread-local random number generator
thread_local std::mt19937_64 rng(std::random_device{}());

// Cache for expensive computations
class StatisticsCache {
private:
    struct CacheEntry {
        std::vector<double> key;
        std::vector<double> quantiles;
        std::chrono::system_clock::time_point timestamp;
    };

    std::unordered_map<size_t, CacheEntry> cache_;
    std::mutex cache_mutex_;
    const size_t max_cache_size_ = 1000;

    size_t compute_hash(const std::vector<double>& values) {
        std::hash<double> hasher;
        size_t hash = 0;
        for (double val : values) {
            hash ^= hasher(val) + 0x9e3779b9 + (hash << 6) + (hash >> 2);
        }
        return hash;
    }

public:
    std::vector<double> get_or_compute_quantiles(const std::vector<double>& values,
                                                const std::vector<double>& qs) {
        size_t hash = compute_hash(values);
        {
            std::lock_guard<std::mutex> lock(cache_mutex_);
            auto it = cache_.find(hash);
            if (it != cache_.end()) {
                // Check if cache is still valid (less than 1 hour old)
                auto now = std::chrono::system_clock::now();
                if (std::chrono::duration_cast<std::chrono::hours>(now - it->second.timestamp).count() < 1) {
                    return it->second.quantiles;
                }
            }
        }

        // Compute quantiles
        std::vector<double> sorted = values;
        std::sort(sorted.begin(), sorted.end());
        std::vector<double> quantiles;
        quantiles.reserve(qs.size());

        for (double q : qs) {
            if (sorted.empty()) {
                quantiles.push_back(0.0);
            } else if (q <= 0.0) {
                quantiles.push_back(sorted.front());
            } else if (q >= 1.0) {
                quantiles.push_back(sorted.back());
            } else {
                double pos = q * (sorted.size() - 1);
                size_t idx = static_cast<size_t>(pos);
                double frac = pos - idx;

                if (idx + 1 < sorted.size()) {
                    quantiles.push_back(sorted[idx] * (1 - frac) + sorted[idx + 1] * frac);
                } else {
                    quantiles.push_back(sorted[idx]);
                }
            }
        }

        // Cache the result
        {
            std::lock_guard<std::mutex> lock(cache_mutex_);
            if (cache_.size() >= max_cache_size_) {
                // Remove oldest entry
                auto oldest = std::min_element(cache_.begin(), cache_.end(),
                    [](const auto& a, const auto& b) {
                        return a.second.timestamp < b.second.timestamp;
                    });
                if (oldest != cache_.end()) {
                    cache_.erase(oldest);
                }
            }
            cache_[hash] = {values, quantiles, std::chrono::system_clock::now()};
        }

        return quantiles;
    }

    void clear() {
        std::lock_guard<std::mutex> lock(cache_mutex_);
        cache_.clear();
    }
};

static StatisticsCache statistics_cache;

// Basic Statistics
double ProfessionalStatisticalCalculator::calculate_mean(const std::vector<double>& values) {
    if (values.empty()) return 0.0;

    // Use Kahan summation for numerical stability
    double sum = 0.0;
    double c = 0.0;
    for (double val : values) {
        double y = val - c;
        double t = sum + y;
        c = (t - sum) - y;
        sum = t;
    }
    return sum / values.size();
}

double ProfessionalStatisticalCalculator::calculate_trimmed_mean(const std::vector<double>& values, double trim_proportion) {
    if (values.empty() || trim_proportion < 0.0 || trim_proportion >= 0.5) {
        return calculate_mean(values);
    }

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    size_t n = sorted.size();
    size_t k = static_cast<size_t>(std::floor(trim_proportion * n));

    if (2 * k >= n) return 0.0;

    double sum = 0.0;
    for (size_t i = k; i < n - k; ++i) {
        sum += sorted[i];
    }

    return sum / (n - 2 * k);
}

double ProfessionalStatisticalCalculator::calculate_winsorized_mean(const std::vector<double>& values, double winsorize_proportion) {
    if (values.empty() || winsorize_proportion < 0.0 || winsorize_proportion >= 0.5) {
        return calculate_mean(values);
    }

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    size_t n = sorted.size();
    size_t k = static_cast<size_t>(std::floor(winsorize_proportion * n));

    if (2 * k >= n) return 0.0;

    double lower_bound = sorted[k];
    double upper_bound = sorted[n - k - 1];

    double sum = 0.0;
    for (size_t i = 0; i < n; ++i) {
        double val = sorted[i];
        if (val < lower_bound) val = lower_bound;
        else if (val > upper_bound) val = upper_bound;
        sum += val;
    }

    return sum / n;
}

double ProfessionalStatisticalCalculator::calculate_median(const std::vector<double>& values) {
    if (values.empty()) return 0.0;

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    size_t n = sorted.size();
    if (n % 2 == 0) {
        return (sorted[n/2 - 1] + sorted[n/2]) / 2.0;
    } else {
        return sorted[n/2];
    }
}

double ProfessionalStatisticalCalculator::calculate_quantile(const std::vector<double>& values, double q) {
    if (values.empty()) return 0.0;

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    if (q <= 0.0) return sorted.front();
    if (q >= 1.0) return sorted.back();

    double pos = q * (sorted.size() - 1);
    size_t idx = static_cast<size_t>(pos);
    double frac = pos - idx;

    if (idx + 1 < sorted.size()) {
        return sorted[idx] * (1 - frac) + sorted[idx + 1] * frac;
    }
    return sorted[idx];
}

std::vector<double> ProfessionalStatisticalCalculator::calculate_quantiles(const std::vector<double>& values,
                                                                          const std::vector<double>& qs) {
    return statistics_cache.get_or_compute_quantiles(values, qs);
}

double ProfessionalStatisticalCalculator::calculate_mode(const std::vector<double>& values) {
    if (values.empty()) return 0.0;

    std::map<double, size_t> frequency;
    for (double val : values) {
        frequency[val]++;
    }

    auto max_it = std::max_element(frequency.begin(), frequency.end(),
        [](const auto& a, const auto& b) { return a.second < b.second; });

    return max_it->first;
}

std::vector<double> ProfessionalStatisticalCalculator::calculate_modes(const std::vector<double>& values) {
    if (values.empty()) return {};

    std::map<double, size_t> frequency;
    for (double val : values) {
        frequency[val]++;
    }

    size_t max_freq = 0;
    for (const auto& [val, freq] : frequency) {
        if (freq > max_freq) max_freq = freq;
    }

    std::vector<double> modes;
    for (const auto& [val, freq] : frequency) {
        if (freq == max_freq) {
            modes.push_back(val);
        }
    }

    return modes;
}

// Dispersion Measures
double ProfessionalStatisticalCalculator::calculate_variance(const std::vector<double>& values, bool population) {
    if (values.size() < 2) return 0.0;

    double mean = calculate_mean(values);
    double sum_sq_diff = 0.0;
    double c = 0.0;

    // Two-pass algorithm for better numerical stability
    for (double val : values) {
        double diff = val - mean;
        sum_sq_diff += diff * diff;

        // Kahan compensation
        double t = sum_sq_diff + c;
        c = (t - sum_sq_diff) - diff * diff;
        sum_sq_diff = t;
    }

    size_t n = values.size();
    if (population) {
        return sum_sq_diff / n;
    } else {
        return sum_sq_diff / (n - 1);
    }
}

double ProfessionalStatisticalCalculator::calculate_std_dev(const std::vector<double>& values, bool population) {
    return std::sqrt(calculate_variance(values, population));
}

double ProfessionalStatisticalCalculator::calculate_mad(const std::vector<double>& values) {
    if (values.empty()) return 0.0;

    double median = calculate_median(values);
    std::vector<double> absolute_deviations;
    absolute_deviations.reserve(values.size());

    for (double val : values) {
        absolute_deviations.push_back(std::abs(val - median));
    }

    return calculate_median(absolute_deviations);
}

double ProfessionalStatisticalCalculator::calculate_iqr(const std::vector<double>& values) {
    if (values.empty()) return 0.0;

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    size_t n = sorted.size();
    double q1 = calculate_quantile(sorted, 0.25);
    double q3 = calculate_quantile(sorted, 0.75);

    return q3 - q1;
}

double ProfessionalStatisticalCalculator::calculate_range(const std::vector<double>& values) {
    if (values.empty()) return 0.0;

    auto [min_it, max_it] = std::minmax_element(values.begin(), values.end());
    return *max_it - *min_it;
}

double ProfessionalStatisticalCalculator::calculate_coefficient_of_variation(const std::vector<double>& values) {
    double mean = calculate_mean(values);
    if (std::abs(mean) < EPSILON) return 0.0;

    double std_dev = calculate_std_dev(values);
    return std_dev / mean;
}

// Shape Measures
double ProfessionalStatisticalCalculator::calculate_skewness(const std::vector<double>& values, bool fisher) {
    if (values.size() < 3) return 0.0;

    double mean = calculate_mean(values);
    double std_dev = calculate_std_dev(values);

    if (std_dev < EPSILON) return 0.0;

    double sum_cubed_diff = 0.0;
    for (double val : values) {
        double diff = val - mean;
        sum_cubed_diff += diff * diff * diff;
    }

    double n = static_cast<double>(values.size());
    double skewness = (sum_cubed_diff / n) / std::pow(std_dev, 3);

    if (fisher) {
        // Fisher's adjustment for small samples
        if (n > 3) {
            double adjustment = std::sqrt(n * (n - 1)) / (n - 2);
            skewness *= adjustment;
        }
    }

    return skewness;
}

double ProfessionalStatisticalCalculator::calculate_kurtosis(const std::vector<double>& values, bool fisher) {
    if (values.size() < 4) return 0.0;

    double mean = calculate_mean(values);
    double std_dev = calculate_std_dev(values);

    if (std_dev < EPSILON) return 0.0;

    double sum_fourth_diff = 0.0;
    for (double val : values) {
        double diff = val - mean;
        sum_fourth_diff += diff * diff * diff * diff;
    }

    double n = static_cast<double>(values.size());
    double kurtosis = (sum_fourth_diff / n) / std::pow(std_dev, 4);

    if (fisher) {
        // Fisher's excess kurtosis (subtract 3)
        kurtosis -= 3.0;
    }

    return kurtosis;
}

std::vector<double> ProfessionalStatisticalCalculator::calculate_moments(const std::vector<double>& values, size_t max_order) {
    if (values.empty()) return std::vector<double>(max_order, 0.0);

    double mean = calculate_mean(values);
    std::vector<double> moments(max_order, 0.0);

    for (size_t order = 1; order <= max_order; ++order) {
        double sum = 0.0;
        for (double val : values) {
            sum += std::pow(val, order);
        }
        moments[order - 1] = sum / values.size();
    }

    return moments;
}

std::vector<double> ProfessionalStatisticalCalculator::calculate_central_moments(const std::vector<double>& values, size_t max_order) {
    if (values.empty()) return std::vector<double>(max_order, 0.0);

    double mean = calculate_mean(values);
    std::vector<double> moments(max_order, 0.0);

    for (size_t order = 1; order <= max_order; ++order) {
        double sum = 0.0;
        for (double val : values) {
            sum += std::pow(val - mean, order);
        }
        moments[order - 1] = sum / values.size();
    }

    return moments;
}

std::vector<double> ProfessionalStatisticalCalculator::calculate_standardized_moments(const std::vector<double>& values, size_t max_order) {
    if (values.empty()) return std::vector<double>(max_order, 0.0);

    double mean = calculate_mean(values);
    double std_dev = calculate_std_dev(values);

    if (std_dev < EPSILON) return std::vector<double>(max_order, 0.0);

    std::vector<double> moments(max_order, 0.0);

    for (size_t order = 1; order <= max_order; ++order) {
        double sum = 0.0;
        for (double val : values) {
            sum += std::pow((val - mean) / std_dev, order);
        }
        moments[order - 1] = sum / values.size();
    }

    return moments;
}

std::vector<double> ProfessionalStatisticalCalculator::calculate_l_moments(const std::vector<double>& values, size_t max_order) {
    if (values.empty() || max_order == 0) return {};

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    size_t n = sorted.size();
    std::vector<double> l_moments(max_order, 0.0);

    // Precompute binomial coefficients
    std::vector<std::vector<double>> binom(n + 1, std::vector<double>(max_order + 1, 0.0));
    for (size_t i = 0; i <= n; ++i) {
        binom[i][0] = 1.0;
        for (size_t j = 1; j <= std::min(i, max_order); ++j) {
            binom[i][j] = binom[i-1][j-1] + binom[i-1][j];
        }
    }

    // Calculate probability-weighted moments
    std::vector<double> beta(max_order, 0.0);
    for (size_t r = 0; r < max_order; ++r) {
        double sum = 0.0;
        for (size_t i = 0; i < n; ++i) {
            double weight = 1.0;
            for (size_t j = 0; j < r; ++j) {
                weight *= static_cast<double>(i - j) / (n - 1 - j);
            }
            sum += weight * sorted[i];
        }
        beta[r] = sum / n;
    }

    // Convert to L-moments
    for (size_t r = 1; r <= max_order; ++r) {
        double sum = 0.0;
        for (size_t k = 0; k < r; ++k) {
            double sign = ((r - 1 - k) % 2 == 0) ? 1.0 : -1.0;
            sum += sign * binom[r-1][k] * binom[r-1+k][k] * beta[k];
        }
        l_moments[r-1] = sum;
    }

    return l_moments;
}

// Normality Tests
double ProfessionalStatisticalCalculator::shapiro_wilk(const std::vector<double>& values, double& p_value) {
    if (values.size() < 3 || values.size() > 5000) {
        p_value = 0.0;
        return 0.0;
    }

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    size_t n = sorted.size();

    // Calculate coefficients for Shapiro-Wilk test
    std::vector<double> a(n, 0.0);

    if (n <= 11) {
        // Use exact coefficients for small n
        static const std::map<size_t, std::vector<double>> small_coeffs = {
            {3, {0.7071}},
            {4, {0.6872, 0.1677}},
            {5, {0.6646, 0.2413}},
            {6, {0.6431, 0.2806, 0.0875}},
            {7, {0.6233, 0.3031, 0.1401}},
            {8, {0.6052, 0.3164, 0.1743, 0.0561}},
            {9, {0.5888, 0.3244, 0.1976, 0.0947}},
            {10, {0.5739, 0.3291, 0.2141, 0.1224, 0.0399}},
            {11, {0.5601, 0.3315, 0.2260, 0.1429, 0.0695}}
        };

        auto it = small_coeffs.find(n);
        if (it != small_coeffs.end()) {
            const auto& coeffs = it->second;
            for (size_t i = 0; i < coeffs.size(); ++i) {
                a[i] = coeffs[i];
                a[n - i - 1] = coeffs[i];
            }
        }
    } else {
        // Use approximation for larger n
        for (size_t i = 1; i <= n; ++i) {
            double u = boost::math::quantile(boost::math::normal_distribution<>(),
                                            static_cast<double>(i) / (n + 1));
            //a[i-1] = u / std::sqrt(std::inner_product(u, u, u, 0.0));
            a[i-1] = u / std::sqrt(u * u);
        }
    }

    // Calculate W statistic
    double sum_sq = 0.0;
    for (double val : sorted) {
        double diff = val - calculate_mean(values);
        sum_sq += diff * diff;
    }

    if (sum_sq < EPSILON) {
        p_value = 1.0;
        return 1.0;
    }

    double b = 0.0;
    for (size_t i = 0; i < n / 2; ++i) {
        b += a[i] * (sorted[n - i - 1] - sorted[i]);
    }

    if (n % 2 == 1) {
        b += a[n / 2] * sorted[n / 2];
    }

    double W = (b * b) / sum_sq;

    // Calculate p-value using Royston's approximation
    if (n >= 4 && n <= 11) {
        double mu = 0.0, sigma = 1.0;
        if (n == 4) { mu = -1.586; sigma = 0.756; }
        else if (n == 5) { mu = -1.332; sigma = 0.763; }
        else if (n == 6) { mu = -1.192; sigma = 0.765; }
        else if (n == 7) { mu = -1.092; sigma = 0.767; }
        else if (n == 8) { mu = -1.022; sigma = 0.769; }
        else if (n == 9) { mu = -0.966; sigma = 0.770; }
        else if (n == 10) { mu = -0.920; sigma = 0.772; }
        else if (n == 11) { mu = -0.882; sigma = 0.774; }

        double z = (W - mu) / sigma;
        p_value = 1.0 - boost::math::cdf(boost::math::normal_distribution<>(), z);
    } else if (n >= 12 && n <= 5000) {
        double u = std::log(1.0 - W);
        double mu, sigma, gamma;

        if (n <= 20) {
            mu = -1.5861 + 0.31082 * std::log(n) - 0.083751 * std::pow(std::log(n), 2) + 0.0038915 * std::pow(std::log(n), 3);
            sigma = std::exp(-0.4803 + 0.082676 * std::log(n) + 0.0030302 * std::pow(std::log(n), 2));
        } else {
            mu = -1.2725 + 1.0521 * std::log(n) - 0.243 * std::pow(std::log(n), 2) + 0.0012 * std::pow(std::log(n), 3);
            sigma = std::exp(-1.196 + 0.4969 * std::log(n) - 0.1247 * std::pow(std::log(n), 2) + 0.0022 * std::pow(std::log(n), 3));
        }

        if (n >= 4 && n <= 11) gamma = 0.0;
        else if (n >= 12 && n <= 20) gamma = 1.0;
        else gamma = 2.0;

        double z = (u - mu) / sigma;
        if (gamma != 0.0) {
            z = (std::exp(gamma * z) - 1.0) / gamma;
        }

        p_value = 1.0 - boost::math::cdf(boost::math::normal_distribution<>(), z);
    } else {
        p_value = 0.0;
    }

    return W;
}

double ProfessionalStatisticalCalculator::jarque_bera(const std::vector<double>& values, double& p_value) {
    if (values.size() < 4) {
        p_value = 0.0;
        return 0.0;
    }

    double skewness = calculate_skewness(values, true);
    double kurtosis = calculate_kurtosis(values, true);
    double n = static_cast<double>(values.size());

    double JB = n * (std::pow(skewness, 2) / 6.0 + std::pow(kurtosis, 2) / 24.0);

    // Chi-square distribution with 2 degrees of freedom
    p_value = 1.0 - boost::math::cdf(boost::math::chi_squared_distribution<>(2), JB);

    return JB;
}

double ProfessionalStatisticalCalculator::anderson_darling(const std::vector<double>& values, const std::string& distribution) {
    if (values.empty()) return 0.0;

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    size_t n = sorted.size();
    double A2 = 0.0;

    // Fit distribution parameters
    double mu = calculate_mean(values);
    double sigma = calculate_std_dev(values);

    for (size_t i = 0; i < n; ++i) {
        double x = sorted[i];
        double F = 0.0;

        if (distribution == "normal") {
            F = 0.5 * (1.0 + std::erf((x - mu) / (sigma * SQRT_2)));
        } else if (distribution == "exponential") {
            double lambda = 1.0 / mu;
            F = 1.0 - std::exp(-lambda * x);
        } else if (distribution == "lognormal") {
            double mu_log = 0.0, sigma_log = 1.0;
            if (x > 0) {
                std::vector<double> log_values;
                log_values.reserve(values.size());
                for (double val : values) {
                    if (val > 0) log_values.push_back(std::log(val));
                }
                if (!log_values.empty()) {
                    mu_log = calculate_mean(log_values);
                    sigma_log = calculate_std_dev(log_values);
                }
                F = 0.5 * (1.0 + std::erf((std::log(x) - mu_log) / (sigma_log * SQRT_2)));
            }
        }

        double term1 = std::log(F);
        double term2 = std::log(1.0 - F);

        if (!std::isfinite(term1)) term1 = -100.0;
        if (!std::isfinite(term2)) term2 = -100.0;

        A2 += (2.0 * i + 1.0) * (term1 + term2);
    }

    A2 = -n - A2 / n;

    // Adjust for sample size
    A2 *= (1.0 + 0.75 / n + 2.25 / (n * n));

    return A2;
}

double ProfessionalStatisticalCalculator::kolmogorov_smirnov(const std::vector<double>& values, const std::string& distribution) {
    if (values.empty()) return 0.0;

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    size_t n = sorted.size();
    double D = 0.0;

    // Fit distribution parameters
    double mu = calculate_mean(values);
    double sigma = calculate_std_dev(values);

    for (size_t i = 0; i < n; ++i) {
        double x = sorted[i];
        double F_empirical = static_cast<double>(i + 1) / n;
        double F_theoretical = 0.0;

        if (distribution == "normal") {
            F_theoretical = 0.5 * (1.0 + std::erf((x - mu) / (sigma * SQRT_2)));
        } else if (distribution == "exponential") {
            double lambda = 1.0 / mu;
            F_theoretical = 1.0 - std::exp(-lambda * x);
        } else if (distribution == "uniform") {
            auto [min_it, max_it] = std::minmax_element(values.begin(), values.end());
            double a = *min_it;
            double b = *max_it;
            if (b > a) {
                F_theoretical = (x - a) / (b - a);
            }
        }

        double D1 = std::abs(F_empirical - F_theoretical);
        double D2 = (i > 0) ? std::abs(static_cast<double>(i) / n - F_theoretical) : 0.0;

        D = std::max(D, std::max(D1, D2));
    }

    return D;
}

double ProfessionalStatisticalCalculator::cramer_von_mises(const std::vector<double>& values, const std::string& distribution) {
    if (values.empty()) return 0.0;

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    size_t n = sorted.size();
    double W2 = 0.0;

    // Fit distribution parameters
    double mu = calculate_mean(values);
    double sigma = calculate_std_dev(values);

    for (size_t i = 0; i < n; ++i) {
        double x = sorted[i];
        double F = 0.0;

        if (distribution == "normal") {
            F = 0.5 * (1.0 + std::erf((x - mu) / (sigma * SQRT_2)));
        } else if (distribution == "exponential") {
            double lambda = 1.0 / mu;
            F = 1.0 - std::exp(-lambda * x);
        }

        double term = (2.0 * i + 1.0) / (2.0 * n) - F;
        W2 += term * term;
    }

    W2 += 1.0 / (12.0 * n);

    return W2;
}

// Correlation Measures
double ProfessionalStatisticalCalculator::pearson_correlation(const std::vector<double>& x,
                                                             const std::vector<double>& y,
                                                             double& p_value,
                                                             double& confidence_lower,
                                                             double& confidence_upper) {
    if (x.size() != y.size() || x.size() < 2) {
        p_value = 0.0;
        confidence_lower = 0.0;
        confidence_upper = 0.0;
        return 0.0;
    }

    size_t n = x.size();

    // Calculate means
    double x_mean = calculate_mean(x);
    double y_mean = calculate_mean(y);

    // Calculate sums
    double sum_xy = 0.0, sum_xx = 0.0, sum_yy = 0.0;
    for (size_t i = 0; i < n; ++i) {
        double x_diff = x[i] - x_mean;
        double y_diff = y[i] - y_mean;
        sum_xy += x_diff * y_diff;
        sum_xx += x_diff * x_diff;
        sum_yy += y_diff * y_diff;
    }

    if (sum_xx < EPSILON || sum_yy < EPSILON) {
        p_value = 0.0;
        confidence_lower = 0.0;
        confidence_upper = 0.0;
        return 0.0;
    }

    double r = sum_xy / std::sqrt(sum_xx * sum_yy);

    // Calculate t-statistic for p-value
    if (n > 2) {
        double t = r * std::sqrt((n - 2) / (1 - r * r));

        // Two-tailed p-value using Student's t distribution
        boost::math::students_t dist(n - 2);
        p_value = 2.0 * (1.0 - boost::math::cdf(dist, std::abs(t)));

        // Confidence interval using Fisher's z-transform
        double z = 0.5 * std::log((1.0 + r) / (1.0 - r));
        double z_se = 1.0 / std::sqrt(n - 3);

        double z_critical = 1.96; // 95% confidence
        double z_lower = z - z_critical * z_se;
        double z_upper = z + z_critical * z_se;

        confidence_lower = (std::exp(2 * z_lower) - 1) / (std::exp(2 * z_lower) + 1);
        confidence_upper = (std::exp(2 * z_upper) - 1) / (std::exp(2 * z_upper) + 1);
    } else {
        p_value = 0.0;
        confidence_lower = r;
        confidence_upper = r;
    }

    return r;
}

double ProfessionalStatisticalCalculator::spearman_correlation(const std::vector<double>& x,
                                                              const std::vector<double>& y,
                                                              double& p_value) {
    if (x.size() != y.size() || x.size() < 2) {
        p_value = 0.0;
        return 0.0;
    }

    size_t n = x.size();

    // Get ranks
    auto get_ranks = [](const std::vector<double>& values) -> std::vector<double> {
        std::vector<size_t> indices(values.size());
        std::iota(indices.begin(), indices.end(), 0);

        std::stable_sort(indices.begin(), indices.end(),
            [&values](size_t i, size_t j) { return values[i] < values[j]; });

        std::vector<double> ranks(values.size());
        for (size_t i = 0; i < indices.size(); ++i) {
            // Handle ties
            size_t j = i;
            while (j + 1 < indices.size() &&
                   std::abs(values[indices[j]] - values[indices[j + 1]]) < EPSILON) {
                ++j;
            }

            double avg_rank = (i + j + 2) / 2.0;
            for (size_t k = i; k <= j; ++k) {
                ranks[indices[k]] = avg_rank;
            }

            i = j;
        }

        return ranks;
    };

    std::vector<double> ranks_x = get_ranks(x);
    std::vector<double> ranks_y = get_ranks(y);

    // Calculate Spearman's rho
    double rho = 0.0;
    double p_val = 0.0;
    pearson_correlation(ranks_x, ranks_y, p_val, rho, rho);

    // Adjust p-value for small samples
    if (n <= 10) {
        // Use exact distribution for small n
        static const std::map<size_t, std::vector<double>> critical_values = {
            {4, {1.000, 1.000, 1.000}},
            {5, {0.900, 1.000, 1.000}},
            {6, {0.829, 0.886, 0.943}},
            {7, {0.714, 0.786, 0.893}},
            {8, {0.643, 0.738, 0.833}},
            {9, {0.600, 0.683, 0.783}},
            {10, {0.564, 0.648, 0.745}}
        };

        auto it = critical_values.find(n);
        if (it != critical_values.end()) {
            const auto& cv = it->second;
            if (std::abs(rho) >= cv[0]) p_value = 0.05;
            if (std::abs(rho) >= cv[1]) p_value = 0.025;
            if (std::abs(rho) >= cv[2]) p_value = 0.01;
        }
    } else {
        p_value = p_val;
    }

    return rho;
}

double ProfessionalStatisticalCalculator::kendall_tau(const std::vector<double>& x,
                                                     const std::vector<double>& y,
                                                     double& p_value) {
    if (x.size() != y.size() || x.size() < 2) {
        p_value = 0.0;
        return 0.0;
    }

    size_t n = x.size();
    size_t concordant = 0, discordant = 0;

    // Count concordant and discordant pairs
    for (size_t i = 0; i < n; ++i) {
        for (size_t j = i + 1; j < n; ++j) {
            double x_diff = x[j] - x[i];
            double y_diff = y[j] - y[i];

            if (x_diff * y_diff > 0) {
                ++concordant;
            } else if (x_diff * y_diff < 0) {
                ++discordant;
            }
            // Ties are ignored
        }
    }

    if (concordant + discordant == 0) {
        p_value = 0.0;
        return 0.0;
    }

    double tau = static_cast<double>(concordant - discordant) /
                 static_cast<double>(concordant + discordant);

    // Calculate p-value using normal approximation
    if (n > 10) {
        double var_tau = (2.0 * (2.0 * n + 5.0)) / (9.0 * n * (n - 1.0));
        double z = tau / std::sqrt(var_tau);

        boost::math::normal_distribution<> normal;
        p_value = 2.0 * (1.0 - boost::math::cdf(normal, std::abs(z)));
    } else {
        // Use exact distribution for small n
        static const std::map<size_t, std::map<double, double>> exact_p_values = {
            {4, {{1.000, 0.167}, {0.667, 0.333}, {0.333, 0.500}}},
            {5, {{1.000, 0.042}, {0.600, 0.117}, {0.200, 0.242}}},
            {6, {{1.000, 0.008}, {0.600, 0.058}, {0.333, 0.133}}},
            {7, {{1.000, 0.001}, {0.619, 0.029}, {0.333, 0.072}}},
            {8, {{1.000, 0.000}, {0.643, 0.014}, {0.429, 0.042}}},
            {9, {{1.000, 0.000}, {0.667, 0.007}, {0.500, 0.024}}},
            {10, {{1.000, 0.000}, {0.600, 0.004}, {0.467, 0.014}}}
        };

        auto it_n = exact_p_values.find(n);
        if (it_n != exact_p_values.end()) {
            const auto& p_map = it_n->second;
            for (const auto& [critical_value, p_val] : p_map) {
                if (std::abs(tau) >= critical_value) {
                    p_value = p_val;
                    break;
                }
            }
        }
    }

    return tau;
}

double ProfessionalStatisticalCalculator::distance_correlation(const std::vector<double>& x,
                                                              const std::vector<double>& y) {
    if (x.size() != y.size() || x.size() < 2) {
        return 0.0;
    }

    size_t n = x.size();

    // Compute distance matrices
    Eigen::MatrixXd dist_x(n, n);
    Eigen::MatrixXd dist_y(n, n);

    for (size_t i = 0; i < n; ++i) {
        for (size_t j = 0; j < n; ++j) {
            dist_x(i, j) = std::abs(x[i] - x[j]);
            dist_y(i, j) = std::abs(y[i] - y[j]);
        }
    }

    // Double-centering
    Eigen::MatrixXd A = dist_x;
    Eigen::MatrixXd B = dist_y;

    Eigen::VectorXd row_means_x = A.rowwise().mean();
    Eigen::VectorXd col_means_x = A.colwise().mean();
    double grand_mean_x = A.mean();

    Eigen::VectorXd row_means_y = B.rowwise().mean();
    Eigen::VectorXd col_means_y = B.colwise().mean();
    double grand_mean_y = B.mean();

    for (size_t i = 0; i < n; ++i) {
        for (size_t j = 0; j < n; ++j) {
            A(i, j) -= row_means_x(i) + col_means_x(j) - grand_mean_x;
            B(i, j) -= row_means_y(i) + col_means_y(j) - grand_mean_y;
        }
    }

    // Compute distance covariance
    double dCov = std::sqrt((A.array() * B.array()).sum() / (n * n));

    // Compute distance variances
    double dVarX = std::sqrt((A.array() * A.array()).sum() / (n * n));
    double dVarY = std::sqrt((B.array() * B.array()).sum() / (n * n));

    if (dVarX < EPSILON || dVarY < EPSILON) {
        return 0.0;
    }

    return dCov / std::sqrt(dVarX * dVarY);
}

double ProfessionalStatisticalCalculator::maximal_information_coefficient(const std::vector<double>& x,
                                                                         const std::vector<double>& y) {
    if (x.size() != y.size() || x.size() < 2) {
        return 0.0;
    }

    size_t n = x.size();
    const size_t B_MAX = std::max(static_cast<size_t>(std::pow(n, 0.6)), static_cast<size_t>(2));

    double best_mic = 0.0;

    // Discretize and compute mutual information for different grid sizes
    for (size_t a = 2; a <= B_MAX; ++a) {
        for (size_t b = 2; b <= B_MAX; ++b) {
            if (a * b > B_MAX * B_MAX) continue;

            // Create grid
            std::vector<double> x_sorted = x;
            std::vector<double> y_sorted = y;
            std::sort(x_sorted.begin(), x_sorted.end());
            std::sort(y_sorted.begin(), y_sorted.end());

            // Compute mutual information for this grid
            std::vector<std::vector<size_t>> counts(a, std::vector<size_t>(b, 0));

            for (size_t i = 0; i < n; ++i) {
                // Find x bin
                size_t x_bin = 0;
                double x_val = x[i];
                for (size_t j = 0; j < a - 1; ++j) {
                    if (x_val >= x_sorted[(j * n) / a]) {
                        x_bin = j;
                    }
                }

                // Find y bin
                size_t y_bin = 0;
                double y_val = y[i];
                for (size_t k = 0; k < b - 1; ++k) {
                    if (y_val >= y_sorted[(k * n) / b]) {
                        y_bin = k;
                    }
                }

                ++counts[x_bin][y_bin];
            }

            // Compute mutual information
            std::vector<size_t> row_sums(a, 0);
            std::vector<size_t> col_sums(b, 0);
            size_t total = n;

            for (size_t i = 0; i < a; ++i) {
                for (size_t j = 0; j < b; ++j) {
                    row_sums[i] += counts[i][j];
                    col_sums[j] += counts[i][j];
                }
            }

            double mi = 0.0;
            for (size_t i = 0; i < a; ++i) {
                for (size_t j = 0; j < b; ++j) {
                    if (counts[i][j] > 0) {
                        double p_xy = static_cast<double>(counts[i][j]) / total;
                        double p_x = static_cast<double>(row_sums[i]) / total;
                        double p_y = static_cast<double>(col_sums[j]) / total;
                        mi += p_xy * std::log(p_xy / (p_x * p_y));
                    }
                }
            }

            // Normalize by log(min(a, b))
            double normalized_mi = mi / std::log(std::min(a, b));
            best_mic = std::max(best_mic, normalized_mi);
        }
    }

    return best_mic;
}

double ProfessionalStatisticalCalculator::hoeffding_d(const std::vector<double>& x,
                                                     const std::vector<double>& y) {
    if (x.size() != y.size() || x.size() < 2) {
        return 0.0;
    }

    size_t n = x.size();

    // Get ranks
    auto get_ranks = [](const std::vector<double>& values) -> std::vector<size_t> {
        std::vector<size_t> indices(values.size());
        std::iota(indices.begin(), indices.end(), 0);

        std::stable_sort(indices.begin(), indices.end(),
            [&values](size_t i, size_t j) { return values[i] < values[j]; });

        std::vector<size_t> ranks(values.size());
        for (size_t i = 0; i < indices.size(); ++i) {
            ranks[indices[i]] = i + 1;
        }

        return ranks;
    };

    std::vector<size_t> R = get_ranks(x);
    std::vector<size_t> S = get_ranks(y);

    // Compute Q (pair order)
    std::vector<size_t> Q(n, 0);
    for (size_t i = 0; i < n; ++i) {
        for (size_t j = 0; j < n; ++j) {
            if (x[j] <= x[i] && y[j] <= y[i]) {
                ++Q[i];
            }
        }
    }

    // Compute D statistic
    double D1 = 0.0, D2 = 0.0, D3 = 0.0;

    for (size_t i = 0; i < n; ++i) {
        D1 += static_cast<double>((Q[i] - 1) * (Q[i] - 2));
        D2 += static_cast<double>((R[i] - 1) * (R[i] - 2) * (S[i] - 1) * (S[i] - 2));
        D3 += static_cast<double>((R[i] - 2) * (S[i] - 2) * (Q[i] - 1));
    }

    double D = static_cast<double>(n - 2) * D1 + D2 - 2.0 * static_cast<double>(n - 2) * D3;
    double hoeffding = 30.0 * D / (static_cast<double>(n) * (n - 1.0) * (n - 2.0) * (n - 3.0) * (n - 4.0));

    return hoeffding;
}

// Information Theory Measures
double ProfessionalStatisticalCalculator::entropy(const std::vector<double>& values, size_t bins) {
    if (values.empty() || bins < 2) return 0.0;

    // Create histogram
    auto [min_it, max_it] = std::minmax_element(values.begin(), values.end());
    double min_val = *min_it;
    double max_val = *max_it;

    if (std::abs(max_val - min_val) < EPSILON) return 0.0;

    std::vector<size_t> histogram(bins, 0);
    double bin_width = (max_val - min_val) / bins;

    for (double val : values) {
        size_t bin_idx = static_cast<size_t>((val - min_val) / bin_width);
        if (bin_idx >= bins) bin_idx = bins - 1;
        ++histogram[bin_idx];
    }

    // Calculate entropy
    double entropy = 0.0;
    size_t total = values.size();

    for (size_t count : histogram) {
        if (count > 0) {
            double p = static_cast<double>(count) / total;
            entropy -= p * std::log(p);
        }
    }

    return entropy;
}

double ProfessionalStatisticalCalculator::mutual_information(const std::vector<double>& x,
                                                            const std::vector<double>& y,
                                                            size_t bins) {
    if (x.size() != y.size() || x.size() < 2 || bins < 2) {
        return 0.0;
    }

    size_t n = x.size();

    // Create 2D histogram
    auto [x_min_it, x_max_it] = std::minmax_element(x.begin(), x.end());
    auto [y_min_it, y_max_it] = std::minmax_element(y.begin(), y.end());

    double x_min = *x_min_it, x_max = *x_max_it;
    double y_min = *y_min_it, y_max = *y_max_it;

    if (std::abs(x_max - x_min) < EPSILON || std::abs(y_max - y_min) < EPSILON) {
        return 0.0;
    }

    double x_bin_width = (x_max - x_min) / bins;
    double y_bin_width = (y_max - y_min) / bins;

    std::vector<std::vector<size_t>> joint_hist(bins, std::vector<size_t>(bins, 0));
    std::vector<size_t> x_marginal(bins, 0);
    std::vector<size_t> y_marginal(bins, 0);

    for (size_t i = 0; i < n; ++i) {
        size_t x_bin = static_cast<size_t>((x[i] - x_min) / x_bin_width);
        size_t y_bin = static_cast<size_t>((y[i] - y_min) / y_bin_width);

        if (x_bin >= bins) x_bin = bins - 1;
        if (y_bin >= bins) y_bin = bins - 1;

        ++joint_hist[x_bin][y_bin];
        ++x_marginal[x_bin];
        ++y_marginal[y_bin];
    }

    // Calculate mutual information
    double mi = 0.0;

    for (size_t i = 0; i < bins; ++i) {
        for (size_t j = 0; j < bins; ++j) {
            if (joint_hist[i][j] > 0) {
                double p_xy = static_cast<double>(joint_hist[i][j]) / n;
                double p_x = static_cast<double>(x_marginal[i]) / n;
                double p_y = static_cast<double>(y_marginal[j]) / n;
                mi += p_xy * std::log(p_xy / (p_x * p_y));
            }
        }
    }

    return mi;
}

double ProfessionalStatisticalCalculator::conditional_entropy(const std::vector<double>& x,
                                                             const std::vector<double>& y,
                                                             size_t bins) {
    if (x.size() != y.size() || x.size() < 2 || bins < 2) {
        return 0.0;
    }

    double h_y = entropy(y, bins);
    double mi = mutual_information(x, y, bins);

    return h_y - mi;
}

double ProfessionalStatisticalCalculator::kl_divergence(const std::vector<double>& p,
                                                       const std::vector<double>& q) {
    if (p.size() != q.size() || p.empty()) {
        return 0.0;
    }

    double kl = 0.0;
    for (size_t i = 0; i < p.size(); ++i) {
        if (p[i] > 0 && q[i] > 0) {
            kl += p[i] * std::log(p[i] / q[i]);
        } else if (p[i] > 0 && q[i] <= 0) {
            return std::numeric_limits<double>::infinity();
        }
    }

    return kl;
}

double ProfessionalStatisticalCalculator::js_divergence(const std::vector<double>& p,
                                                       const std::vector<double>& q) {
    if (p.size() != q.size() || p.empty()) {
        return 0.0;
    }

    // Compute m = (p + q) / 2
    std::vector<double> m(p.size());
    for (size_t i = 0; i < p.size(); ++i) {
        m[i] = (p[i] + q[i]) / 2.0;
    }

    double kl_pm = kl_divergence(p, m);
    double kl_qm = kl_divergence(q, m);

    return (kl_pm + kl_qm) / 2.0;
}

// Distance Measures
double ProfessionalStatisticalCalculator::euclidean_distance(const std::vector<double>& a,
                                                           const std::vector<double>& b) {
    if (a.size() != b.size()) {
        throw std::invalid_argument("Vectors must have same size for Euclidean distance");
    }

    double sum = 0.0;
    for (size_t i = 0; i < a.size(); ++i) {
        double diff = a[i] - b[i];
        sum += diff * diff;
    }

    return std::sqrt(sum);
}

double ProfessionalStatisticalCalculator::manhattan_distance(const std::vector<double>& a,
                                                           const std::vector<double>& b) {
    if (a.size() != b.size()) {
        throw std::invalid_argument("Vectors must have same size for Manhattan distance");
    }

    double sum = 0.0;
    for (size_t i = 0; i < a.size(); ++i) {
        sum += std::abs(a[i] - b[i]);
    }

    return sum;
}

double ProfessionalStatisticalCalculator::chebyshev_distance(const std::vector<double>& a,
                                                           const std::vector<double>& b) {
    if (a.size() != b.size()) {
        throw std::invalid_argument("Vectors must have same size for Chebyshev distance");
    }

    double max_diff = 0.0;
    for (size_t i = 0; i < a.size(); ++i) {
        max_diff = std::max(max_diff, std::abs(a[i] - b[i]));
    }

    return max_diff;
}

double ProfessionalStatisticalCalculator::minkowski_distance(const std::vector<double>& a,
                                                           const std::vector<double>& b,
                                                           double p) {
    if (a.size() != b.size()) {
        throw std::invalid_argument("Vectors must have same size for Minkowski distance");
    }

    if (p < 1.0) {
        throw std::invalid_argument("p must be >= 1 for Minkowski distance");
    }

    double sum = 0.0;
    for (size_t i = 0; i < a.size(); ++i) {
        sum += std::pow(std::abs(a[i] - b[i]), p);
    }

    return std::pow(sum, 1.0 / p);
}

double ProfessionalStatisticalCalculator::mahalanobis_distance(const std::vector<double>& x,
                                                              const std::vector<double>& mean,
                                                              const std::vector<std::vector<double>>& covariance) {
    if (x.size() != mean.size() || x.size() != covariance.size()) {
        throw std::invalid_argument("Dimensions don't match for Mahalanobis distance");
    }

    size_t n = x.size();

    // Convert to Eigen vectors/matrix
    Eigen::VectorXd x_vec(n);
    Eigen::VectorXd mean_vec(n);
    Eigen::MatrixXd cov_mat(n, n);

    for (size_t i = 0; i < n; ++i) {
        x_vec(i) = x[i];
        mean_vec(i) = mean[i];
        for (size_t j = 0; j < n; ++j) {
            cov_mat(i, j) = covariance[i][j];
        }
    }

    // Compute difference
    Eigen::VectorXd diff = x_vec - mean_vec;

    // Check if covariance matrix is invertible
    Eigen::FullPivLU<Eigen::MatrixXd> lu(cov_mat);
    if (!lu.isInvertible()) {
        // Use pseudo-inverse or return large value
        return 1e6;
    }

    // Compute Mahalanobis distance
    Eigen::MatrixXd inv_cov = cov_mat.inverse();
    double distance = std::sqrt(diff.transpose() * inv_cov * diff);

    return distance;
}

double ProfessionalStatisticalCalculator::cosine_similarity(const std::vector<double>& a,
                                                           const std::vector<double>& b) {
    if (a.size() != b.size()) {
        throw std::invalid_argument("Vectors must have same size for cosine similarity");
    }

    double dot_product = 0.0;
    double norm_a = 0.0;
    double norm_b = 0.0;

    for (size_t i = 0; i < a.size(); ++i) {
        dot_product += a[i] * b[i];
        norm_a += a[i] * a[i];
        norm_b += b[i] * b[i];
    }

    norm_a = std::sqrt(norm_a);
    norm_b = std::sqrt(norm_b);

    if (norm_a < EPSILON || norm_b < EPSILON) {
        return 0.0;
    }

    return dot_product / (norm_a * norm_b);
}

double ProfessionalStatisticalCalculator::jaccard_similarity(const std::vector<double>& a,
                                                           const std::vector<double>& b) {
    if (a.size() != b.size()) {
        throw std::invalid_argument("Vectors must have same size for Jaccard similarity");
    }

    double intersection = 0.0;
    double union_ = 0.0;

    for (size_t i = 0; i < a.size(); ++i) {
        if (a[i] > 0 && b[i] > 0) {
            intersection += std::min(a[i], b[i]);
            union_ += std::max(a[i], b[i]);
        } else if (a[i] > 0 || b[i] > 0) {
            union_ += std::max(a[i], b[i]);
        }
    }

    if (union_ < EPSILON) {
        return 0.0;
    }

    return intersection / union_;
}

// Distribution Fitting
ProfessionalStatisticalCalculator::DistributionFit
ProfessionalStatisticalCalculator::fit_distribution(const std::vector<double>& values,
                                                   const std::string& distribution) {
    DistributionFit fit;
    fit.name = distribution;

    if (values.empty()) {
        return fit;
    }

    double n = static_cast<double>(values.size());

    if (distribution == "normal") {
        double mu = calculate_mean(values);
        double sigma = calculate_std_dev(values, true); // population std dev

        fit.parameters["mean"] = mu;
        fit.parameters["std_dev"] = sigma;

        // Compute log-likelihood
        double log_likelihood = 0.0;
        for (double val : values) {
            double exponent = -0.5 * std::pow((val - mu) / sigma, 2);
            log_likelihood += exponent - std::log(sigma * SQRT_2_PI);
        }
        fit.log_likelihood = log_likelihood;

    } else if (distribution == "lognormal") {
        // Check if all values are positive
        bool all_positive = std::all_of(values.begin(), values.end(),
                                       [](double x) { return x > 0; });
        if (!all_positive) {
            return fit;
        }

        // Transform to log space
        std::vector<double> log_values;
        log_values.reserve(values.size());
        for (double val : values) {
            log_values.push_back(std::log(val));
        }

        double mu = calculate_mean(log_values);
        double sigma = calculate_std_dev(log_values, true);

        fit.parameters["mu"] = mu;
        fit.parameters["sigma"] = sigma;

        // Compute log-likelihood
        double log_likelihood = 0.0;
        for (double val : values) {
            double x = std::log(val);
            double exponent = -0.5 * std::pow((x - mu) / sigma, 2);
            log_likelihood += exponent - std::log(val * sigma * SQRT_2_PI);
        }
        fit.log_likelihood = log_likelihood;

    } else if (distribution == "exponential") {
        // Check if all values are non-negative
        bool all_non_negative = std::all_of(values.begin(), values.end(),
                                           [](double x) { return x >= 0; });
        if (!all_non_negative) {
            return fit;
        }

        double lambda = 1.0 / calculate_mean(values);

        fit.parameters["lambda"] = lambda;
        fit.parameters["rate"] = lambda;

        // Compute log-likelihood
        double log_likelihood = 0.0;
        for (double val : values) {
            log_likelihood += std::log(lambda) - lambda * val;
        }
        fit.log_likelihood = log_likelihood;

    } else if (distribution == "gamma") {
        // Check if all values are positive
        bool all_positive = std::all_of(values.begin(), values.end(),
                                       [](double x) { return x > 0; });
        if (!all_positive) {
            return fit;
        }

        // Method of moments
        double mean = calculate_mean(values);
        double variance = calculate_variance(values, true);

        double alpha = (mean * mean) / variance;
        double beta = mean / variance;

        fit.parameters["alpha"] = alpha;
        fit.parameters["beta"] = beta;
        fit.parameters["shape"] = alpha;
        fit.parameters["rate"] = beta;

        // Compute log-likelihood using Stirling's approximation for gamma function
        double log_likelihood = 0.0;
        double log_gamma_alpha = alpha * std::log(alpha) - alpha + 0.5 * std::log(2 * M_PI / alpha);

        for (double val : values) {
            log_likelihood += alpha * std::log(beta) + (alpha - 1) * std::log(val) - beta * val - log_gamma_alpha;
        }
        fit.log_likelihood = log_likelihood;

    } else if (distribution == "beta") {
        // Check if all values are in [0, 1]
        bool all_in_range = std::all_of(values.begin(), values.end(),
                                       [](double x) { return x >= 0 && x <= 1; });
        if (!all_in_range) {
            return fit;
        }

        // Method of moments
        double mean = calculate_mean(values);
        double variance = calculate_variance(values, true);

        if (variance >= mean * (1 - mean)) {
            // Variance too large for beta distribution
            return fit;
        }

        double alpha = mean * ((mean * (1 - mean) / variance) - 1);
        double beta = (1 - mean) * ((mean * (1 - mean) / variance) - 1);

        fit.parameters["alpha"] = alpha;
        fit.parameters["beta"] = beta;

        // Compute log-likelihood using Stirling's approximation for beta function
        double log_likelihood = 0.0;
        double log_beta = std::lgamma(alpha) + std::lgamma(beta) - std::lgamma(alpha + beta);

        for (double val : values) {
            if (val > 0 && val < 1) {
                log_likelihood += (alpha - 1) * std::log(val) + (beta - 1) * std::log(1 - val) - log_beta;
            }
        }
        fit.log_likelihood = log_likelihood;

    } else if (distribution == "weibull") {
        // Check if all values are non-negative
        bool all_non_negative = std::all_of(values.begin(), values.end(),
                                           [](double x) { return x >= 0; });
        if (!all_non_negative) {
            return fit;
        }

        // Maximum likelihood estimation using Newton-Raphson
        double lambda = 1.0; // initial guess
        double k = 1.5; // initial guess

        const size_t max_iter = 100;
        const double tol = 1e-8;

        for (size_t iter = 0; iter < max_iter; ++iter) {
            // Compute gradients
            double sum_log_x = 0.0;
            double sum_xk_log_x = 0.0;
            double sum_xk = 0.0;

            for (double val : values) {
                if (val > 0) {
                    double log_x = std::log(val);
                    double xk = std::pow(val, k);
                    sum_log_x += log_x;
                    sum_xk_log_x += xk * log_x;
                    sum_xk += xk;
                }
            }

            double n_val = static_cast<double>(values.size());

            // Gradient for k
            double g_k = n_val / k + sum_log_x - (n_val / sum_xk) * sum_xk_log_x;

            // Hessian for k
            double h_k = -n_val / (k * k) - (n_val / (sum_xk * sum_xk)) *
                        (sum_xk * sum_xk_log_x * sum_xk_log_x / sum_xk -
                         sum_xk * sum_xk_log_x * sum_xk_log_x / sum_xk);

            // Update k
            double delta_k = -g_k / h_k;
            k += delta_k;

            // Update lambda
            lambda = std::pow(sum_xk / n_val, 1.0 / k);

            if (std::abs(delta_k) < tol) {
                break;
            }
        }

        fit.parameters["lambda"] = lambda;
        fit.parameters["k"] = k;
        fit.parameters["scale"] = lambda;
        fit.parameters["shape"] = k;

        // Compute log-likelihood
        double log_likelihood = 0.0;
        for (double val : values) {
            if (val > 0) {
                double term = std::pow(val / lambda, k);
                log_likelihood += std::log(k / lambda) + (k - 1) * std::log(val / lambda) - term;
            }
        }
        fit.log_likelihood = log_likelihood;
    }

    // Compute AIC and BIC
    fit.aic = -2 * fit.log_likelihood + 2 * fit.parameters.size();
    fit.bic = -2 * fit.log_likelihood + fit.parameters.size() * std::log(n);

    // Compute KS test
    fit.ks_statistic = kolmogorov_smirnov(values, distribution);

    // Compute KS p-value (approximate)
    double sqrt_n = std::sqrt(n);
    double lambda_ks = (sqrt_n + 0.12 + 0.11 / sqrt_n) * fit.ks_statistic;
    if (lambda_ks < 0.4) {
        fit.ks_p_value = 1.0;
    } else {
        double sum = 0.0;
        for (int j = 1; j <= 100; ++j) {
            sum += std::pow(-1.0, j - 1) * std::exp(-2.0 * j * j * lambda_ks * lambda_ks);
        }
        fit.ks_p_value = 2.0 * sum;
    }

    return fit;
}

std::vector<ProfessionalStatisticalCalculator::DistributionFit>
ProfessionalStatisticalCalculator::fit_multiple_distributions(const std::vector<double>& values) {
    std::vector<std::string> distributions = {
        "normal", "lognormal", "exponential", "gamma", "beta", "weibull"
    };

    std::vector<DistributionFit> fits;
    fits.reserve(distributions.size());

    for (const auto& dist : distributions) {
        fits.push_back(fit_distribution(values, dist));
    }

    // Sort by AIC (lower is better)
    std::sort(fits.begin(), fits.end(),
              [](const auto& a, const auto& b) { return a.aic < b.aic; });

    return fits;
}

ProfessionalStatisticalCalculator::DistributionFit
ProfessionalStatisticalCalculator::find_best_fit(const std::vector<double>& values) {
    auto fits = fit_multiple_distributions(values);
    if (fits.empty()) {
        return DistributionFit();
    }
    return fits[0];
}

// Time Series Analysis
std::vector<double> ProfessionalStatisticalCalculator::calculate_autocorrelation(
    const std::vector<double>& series, size_t max_lag) {

    if (series.size() < 2 || max_lag == 0) {
        return {};
    }

    size_t n = series.size();
    max_lag = std::min(max_lag, n - 1);

    double mean = calculate_mean(series);
    double variance = calculate_variance(series, true);

    if (variance < EPSILON) {
        return std::vector<double>(max_lag, 0.0);
    }

    std::vector<double> acf(max_lag, 0.0);

    for (size_t lag = 1; lag <= max_lag; ++lag) {
        double sum = 0.0;
        for (size_t i = lag; i < n; ++i) {
            sum += (series[i] - mean) * (series[i - lag] - mean);
        }
        acf[lag - 1] = sum / (n * variance);
    }

    return acf;
}

std::vector<double> ProfessionalStatisticalCalculator::calculate_partial_autocorrelation(
    const std::vector<double>& series, size_t max_lag) {

    if (series.size() < 2 || max_lag == 0) {
        return {};
    }

    size_t n = series.size();
    max_lag = std::min(max_lag, n - 1);

    // Use Durbin-Levinson algorithm
    std::vector<double> pacf(max_lag, 0.0);
    std::vector<double> phi(max_lag, 0.0);
    std::vector<double> v(max_lag, 0.0);

    // Initialize
    pacf[0] = calculate_autocorrelation(series, 1)[0];
    phi[0] = pacf[0];
    v[0] = 1.0 - phi[0] * phi[0];

    // Iterate
    for (size_t k = 1; k < max_lag; ++k) {
        double sum = 0.0;
        for (size_t j = 0; j < k; ++j) {
            sum += phi[j] * ((k + 1 < series.size()) ?
                   calculate_autocorrelation(series, k - j + 1)[k - j - 1] : 0.0);
        }

        pacf[k] = ((k + 1 < series.size()) ?
                  calculate_autocorrelation(series, k + 1)[k] : 0.0) - sum;
        pacf[k] /= v[k - 1];

        // Update phi and v
        std::vector<double> new_phi(k + 1);
        for (size_t j = 0; j < k; ++j) {
            new_phi[j] = phi[j] - pacf[k] * phi[k - j - 1];
        }
        new_phi[k] = pacf[k];

        phi = new_phi;
        v[k] = v[k - 1] * (1.0 - pacf[k] * pacf[k]);
    }

    return pacf;
}

double ProfessionalStatisticalCalculator::calculate_hurst_exponent(const std::vector<double>& series) {
    if (series.size() < 10) {
        return 0.5;
    }

    size_t n = series.size();

    // Calculate cumulative deviation from mean
    std::vector<double> mean_adjusted(n);
    double mean = calculate_mean(series);
    for (size_t i = 0; i < n; ++i) {
        mean_adjusted[i] = series[i] - mean;
    }

    std::vector<double> cumulative(n);
    cumulative[0] = mean_adjusted[0];
    for (size_t i = 1; i < n; ++i) {
        cumulative[i] = cumulative[i - 1] + mean_adjusted[i];
    }

    // Calculate R/S for different scales
    std::vector<double> scales;
    for (size_t scale = 10; scale <= n / 2; scale *= 2) {
        scales.push_back(scale);
    }

    if (scales.empty()) {
        return 0.5;
    }

    std::vector<double> rs_values;
    rs_values.reserve(scales.size());

    for (double scale : scales) {
        size_t m = static_cast<size_t>(scale);
        size_t num_windows = n / m;

        if (num_windows < 2) continue;

        double avg_rs = 0.0;

        for (size_t window = 0; window < num_windows; ++window) {
            size_t start = window * m;
            size_t end = start + m;

            // Range
            auto [min_it, max_it] = std::minmax_element(
                cumulative.begin() + start, cumulative.begin() + end);
            double R = *max_it - *min_it;

            // Standard deviation
            double S = calculate_std_dev(
                std::vector<double>(series.begin() + start, series.begin() + end));

            if (S > EPSILON) {
                avg_rs += R / S;
            }
        }

        avg_rs /= num_windows;
        rs_values.push_back(std::log(avg_rs));
    }

    // Linear regression of log(R/S) vs log(scale)
    if (rs_values.size() < 2) {
        return 0.5;
    }

    std::vector<double> log_scales;
    for (double scale : scales) {
        log_scales.push_back(std::log(scale));
    }

    // Simple linear regression
    double sum_x = 0.0, sum_y = 0.0, sum_xy = 0.0, sum_xx = 0.0;
    size_t m = rs_values.size();

    for (size_t i = 0; i < m; ++i) {
        sum_x += log_scales[i];
        sum_y += rs_values[i];
        sum_xy += log_scales[i] * rs_values[i];
        sum_xx += log_scales[i] * log_scales[i];
    }

    double slope = (m * sum_xy - sum_x * sum_y) / (m * sum_xx - sum_x * sum_x);

    return slope;
}

double ProfessionalStatisticalCalculator::calculate_lyapunov_exponent(const std::vector<double>& series) {
    if (series.size() < 100) {
        return 0.0;
    }

    // Using Rosenstein's algorithm
    size_t n = series.size();
    size_t min_neighbors = 5;
    size_t max_neighbors = 20;
    size_t tau = 1; // time delay
    size_t m = 3;   // embedding dimension

    // Create state space reconstruction
    std::vector<std::vector<double>> states;
    for (size_t i = 0; i < n - (m - 1) * tau; ++i) {
        std::vector<double> state(m);
        for (size_t j = 0; j < m; ++j) {
            state[j] = series[i + j * tau];
        }
        states.push_back(state);
    }

    size_t num_states = states.size();
    if (num_states < 2) {
        return 0.0;
    }

    // Find nearest neighbors and track divergence
    std::vector<double> divergences;

    for (size_t i = 0; i < num_states - 1; ++i) {
        // Find nearest neighbor (excluding immediate neighbors)
        double min_dist = std::numeric_limits<double>::max();
        size_t nearest_idx = 0;

        for (size_t j = 0; j < num_states; ++j) {
            if (std::abs(static_cast<int>(i) - static_cast<int>(j)) > 5) { // Theiler window
                double dist = euclidean_distance(states[i], states[j]);
                if (dist < min_dist) {
                    min_dist = dist;
                    nearest_idx = j;
                }
            }
        }

        // Track divergence over time
        if (nearest_idx < num_states - 1 && i < num_states - 1) {
            double initial_dist = euclidean_distance(states[i], states[nearest_idx]);
            double final_dist = euclidean_distance(states[i + 1], states[nearest_idx + 1]);

            if (initial_dist > EPSILON) {
                divergences.push_back(std::log(final_dist / initial_dist));
            }
        }
    }

    if (divergences.empty()) {
        return 0.0;
    }

    // Average divergence rate is the Lyapunov exponent
    return calculate_mean(divergences);
}

bool ProfessionalStatisticalCalculator::adf_test(const std::vector<double>& series,
                                                double& statistic,
                                                double& p_value) {
    if (series.size() < 10) {
        statistic = 0.0;
        p_value = 0.0;
        return false;
    }

    size_t n = series.size();

    // Create differenced series
    std::vector<double> diff(n - 1);
    for (size_t i = 1; i < n; ++i) {
        diff[i - 1] = series[i] - series[i - 1];
    }

    // Create lagged series
    std::vector<double> lagged(n - 1);
    for (size_t i = 0; i < n - 1; ++i) {
        lagged[i] = series[i];
    }

    // Regression: diff = alpha + beta*lagged + gamma*trend + error
    // We'll use simple OLS

    // Calculate means
    double mean_diff = calculate_mean(diff);
    double mean_lagged = calculate_mean(lagged);

    // Calculate regression coefficients
    double sum_xy = 0.0, sum_xx = 0.0;
    for (size_t i = 0; i < n - 1; ++i) {
        double x_diff = lagged[i] - mean_lagged;
        double y_diff = diff[i] - mean_diff;
        sum_xy += x_diff * y_diff;
        sum_xx += x_diff * x_diff;
    }

    if (sum_xx < EPSILON) {
        statistic = 0.0;
        p_value = 0.0;
        return false;
    }

    double beta = sum_xy / sum_xx;
    double alpha = mean_diff - beta * mean_lagged;

    // Calculate residuals and standard error
    double ssr = 0.0; // sum of squared residuals
    for (size_t i = 0; i < n - 1; ++i) {
        double predicted = alpha + beta * lagged[i];
        double residual = diff[i] - predicted;
        ssr += residual * residual;
    }

    double se = std::sqrt(ssr / (n - 3)); // n-3 degrees of freedom

    // ADF statistic
    statistic = beta / (se / std::sqrt(sum_xx));

    // Critical values for ADF test (simplified)
    // In production, use proper Dickey-Fuller distribution tables
    if (statistic < -3.43) {
        p_value = 0.01;
    } else if (statistic < -2.86) {
        p_value = 0.05;
    } else if (statistic < -2.57) {
        p_value = 0.10;
    } else {
        p_value = 0.50;
    }

    return (p_value < 0.05); // stationary if p < 0.05
}

bool ProfessionalStatisticalCalculator::kpss_test(const std::vector<double>& series,
                                                 double& statistic,
                                                 double& p_value) {
    if (series.size() < 10) {
        statistic = 0.0;
        p_value = 0.0;
        return false;
    }

    size_t n = series.size();

    // Calculate level of series
    double mean = calculate_mean(series);

    // Calculate partial sums
    std::vector<double> S(n);
    S[0] = series[0] - mean;
    for (size_t i = 1; i < n; ++i) {
        S[i] = S[i - 1] + (series[i] - mean);
    }

    // Calculate long-run variance using Newey-West estimator
    size_t l = static_cast<size_t>(4 * std::pow(n / 100.0, 2.0 / 9.0)); // bandwidth

    double s2 = 0.0;
    for (size_t i = 0; i < n; ++i) {
        s2 += (series[i] - mean) * (series[i] - mean);
    }
    s2 /= n;

    // Add autocorrelation terms
    for (size_t j = 1; j <= l; ++j) {
        double sum = 0.0;
        for (size_t i = j; i < n; ++i) {
            sum += (series[i] - mean) * (series[i - j] - mean);
        }
        s2 += 2.0 * (1.0 - static_cast<double>(j) / (l + 1)) * sum / n;
    }

    // KPSS statistic
    double sum_S2 = 0.0;
    for (size_t i = 0; i < n; ++i) {
        sum_S2 += S[i] * S[i];
    }

    statistic = sum_S2 / (n * n * s2);

    // Critical values for KPSS test (simplified)
    if (statistic > 0.739) {
        p_value = 0.01;
    } else if (statistic > 0.463) {
        p_value = 0.05;
    } else if (statistic > 0.347) {
        p_value = 0.10;
    } else {
        p_value = 0.50;
    }

    return (p_value >= 0.05); // stationary if we cannot reject null
}

// Outlier Detection
std::vector<size_t> ProfessionalStatisticalCalculator::detect_outliers_iqr(
    const std::vector<double>& values, double multiplier) {

    if (values.empty()) {
        return {};
    }

    double q1 = calculate_quantile(values, 0.25);
    double q3 = calculate_quantile(values, 0.75);
    double iqr = q3 - q1;

    if (iqr < EPSILON) {
        return {};
    }

    double lower_bound = q1 - multiplier * iqr;
    double upper_bound = q3 + multiplier * iqr;

    std::vector<size_t> outliers;
    for (size_t i = 0; i < values.size(); ++i) {
        if (values[i] < lower_bound || values[i] > upper_bound) {
            outliers.push_back(i);
        }
    }

    return outliers;
}

std::vector<size_t> ProfessionalStatisticalCalculator::detect_outliers_zscore(
    const std::vector<double>& values, double threshold) {

    if (values.size() < 2) {
        return {};
    }

    double mean = calculate_mean(values);
    double std_dev = calculate_std_dev(values);

    if (std_dev < EPSILON) {
        return {};
    }

    std::vector<size_t> outliers;
    for (size_t i = 0; i < values.size(); ++i) {
        double z_score = std::abs((values[i] - mean) / std_dev);
        if (z_score > threshold) {
            outliers.push_back(i);
        }
    }

    return outliers;
}

std::vector<size_t> ProfessionalStatisticalCalculator::detect_outliers_mad(
    const std::vector<double>& values, double threshold) {

    if (values.empty()) {
        return {};
    }

    double median = calculate_median(values);

    // Calculate Median Absolute Deviation
    std::vector<double> absolute_deviations;
    absolute_deviations.reserve(values.size());
    for (double val : values) {
        absolute_deviations.push_back(std::abs(val - median));
    }

    double mad = calculate_median(absolute_deviations);

    if (mad < EPSILON) {
        return {};
    }

    // Modified Z-score
    std::vector<size_t> outliers;
    for (size_t i = 0; i < values.size(); ++i) {
        double modified_z = 0.6745 * (values[i] - median) / mad;
        if (std::abs(modified_z) > threshold) {
            outliers.push_back(i);
        }
    }

    return outliers;
}

std::vector<size_t> ProfessionalStatisticalCalculator::detect_outliers_grubbs(
    const std::vector<double>& values, double alpha) {

    if (values.size() < 3) {
        return {};
    }

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    double mean = calculate_mean(values);
    double std_dev = calculate_std_dev(values);

    if (std_dev < EPSILON) {
        return {};
    }

    // Test for maximum
    double G_max = (sorted.back() - mean) / std_dev;

    // Test for minimum
    double G_min = (mean - sorted.front()) / std_dev;

    // Critical value from Grubbs' table
    size_t n = values.size();
    double t_critical = boost::math::quantile(
        boost::math::students_t_distribution<>(n - 2),
        1.0 - alpha / (2.0 * n));

    double G_critical = (n - 1) * t_critical /
                       std::sqrt(n * (n - 2 + t_critical * t_critical));

    std::vector<size_t> outliers;
    if (G_max > G_critical) {
        // Find index of maximum value
        auto max_it = std::max_element(values.begin(), values.end());
        outliers.push_back(std::distance(values.begin(), max_it));
    }

    if (G_min > G_critical) {
        // Find index of minimum value
        auto min_it = std::min_element(values.begin(), values.end());
        outliers.push_back(std::distance(values.begin(), min_it));
    }

    return outliers;
}

std::vector<size_t> ProfessionalStatisticalCalculator::detect_outliers_dixon(
    const std::vector<double>& values, double alpha) {

    if (values.size() < 3 || values.size() > 30) {
        return {}; // Dixon test is for 3-30 observations
    }

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    size_t n = values.size();

    // Calculate Q statistic based on sample size
    double Q = 0.0;
    if (n >= 3 && n <= 7) {
        // Test for single outlier
        Q = (sorted[1] - sorted[0]) / (sorted[n-1] - sorted[0]);
    } else if (n >= 8 && n <= 10) {
        // Test for single outlier
        Q = (sorted[1] - sorted[0]) / (sorted[n-2] - sorted[0]);
    } else if (n >= 11 && n <= 13) {
        // Test for single outlier
        Q = (sorted[2] - sorted[0]) / (sorted[n-2] - sorted[0]);
    } else if (n >= 14 && n <= 30) {
        // Test for single outlier
        Q = (sorted[2] - sorted[0]) / (sorted[n-3] - sorted[0]);
    }

    // Critical values for Dixon's Q test (simplified)
    std::map<size_t, std::vector<double>> critical_values = {
        {3, {0.970, 0.941, 0.886}},
        {4, {0.829, 0.765, 0.679}},
        {5, {0.710, 0.642, 0.557}},
        {6, {0.628, 0.560, 0.482}},
        {7, {0.569, 0.507, 0.434}},
        {8, {0.608, 0.554, 0.479}},
        {9, {0.564, 0.512, 0.441}},
        {10, {0.530, 0.477, 0.409}}
    };

    double Q_critical = 0.0;
    auto it = critical_values.find(n);
    if (it != critical_values.end()) {
        if (alpha <= 0.01) Q_critical = it->second[0];
        else if (alpha <= 0.05) Q_critical = it->second[1];
        else Q_critical = it->second[2];
    }

    std::vector<size_t> outliers;
    if (Q > Q_critical) {
        // Find index of minimum value
        auto min_it = std::min_element(values.begin(), values.end());
        outliers.push_back(std::distance(values.begin(), min_it));
    }

    // Test for maximum (similar logic)
    double Q_max = 0.0;
    if (n >= 3 && n <= 7) {
        Q_max = (sorted[n-1] - sorted[n-2]) / (sorted[n-1] - sorted[0]);
    } else if (n >= 8 && n <= 10) {
        Q_max = (sorted[n-1] - sorted[n-2]) / (sorted[n-1] - sorted[1]);
    } else if (n >= 11 && n <= 13) {
        Q_max = (sorted[n-1] - sorted[n-3]) / (sorted[n-1] - sorted[1]);
    } else if (n >= 14 && n <= 30) {
        Q_max = (sorted[n-1] - sorted[n-3]) / (sorted[n-1] - sorted[2]);
    }

    if (Q_max > Q_critical) {
        // Find index of maximum value
        auto max_it = std::max_element(values.begin(), values.end());
        outliers.push_back(std::distance(values.begin(), max_it));
    }

    return outliers;
}

// Statistical Tests
double ProfessionalStatisticalCalculator::t_test(const std::vector<double>& sample1,
                                                const std::vector<double>& sample2,
                                                bool paired, bool equal_var) {

    if (sample1.empty() || sample2.empty()) {
        return 0.0;
    }

    double n1 = static_cast<double>(sample1.size());
    double n2 = static_cast<double>(sample2.size());

    if (paired) {
        if (sample1.size() != sample2.size()) {
            throw std::invalid_argument("Paired t-test requires equal sample sizes");
        }

        // Calculate differences
        std::vector<double> differences;
        differences.reserve(sample1.size());
        for (size_t i = 0; i < sample1.size(); ++i) {
            differences.push_back(sample1[i] - sample2[i]);
        }

        double mean_diff = calculate_mean(differences);
        double std_diff = calculate_std_dev(differences);

        if (std_diff < EPSILON) {
            return 0.0;
        }

        return mean_diff / (std_diff / std::sqrt(n1));

    } else {
        double mean1 = calculate_mean(sample1);
        double mean2 = calculate_mean(sample2);
        double var1 = calculate_variance(sample1);
        double var2 = calculate_variance(sample2);

        if (equal_var) {
            // Pooled variance
            double pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2);
            if (pooled_var < EPSILON) {
                return 0.0;
            }
            return (mean1 - mean2) / std::sqrt(pooled_var * (1/n1 + 1/n2));
        } else {
            // Welch's t-test
            double se = std::sqrt(var1/n1 + var2/n2);
            if (se < EPSILON) {
                return 0.0;
            }
            return (mean1 - mean2) / se;
        }
    }
}

double ProfessionalStatisticalCalculator::f_test(const std::vector<double>& sample1,
                                                const std::vector<double>& sample2) {

    if (sample1.size() < 2 || sample2.size() < 2) {
        return 0.0;
    }

    double var1 = calculate_variance(sample1);
    double var2 = calculate_variance(sample2);

    if (var2 < EPSILON) {
        return std::numeric_limits<double>::infinity();
    }

    return var1 / var2;
}

double ProfessionalStatisticalCalculator::chi2_test(const std::vector<std::vector<double>>& contingency_table) {
    if (contingency_table.empty() || contingency_table[0].empty()) {
        return 0.0;
    }

    size_t rows = contingency_table.size();
    size_t cols = contingency_table[0].size();

    // Calculate row and column totals
    std::vector<double> row_totals(rows, 0.0);
    std::vector<double> col_totals(cols, 0.0);
    double grand_total = 0.0;

    for (size_t i = 0; i < rows; ++i) {
        if (contingency_table[i].size() != cols) {
            throw std::invalid_argument("Contingency table must have consistent column counts");
        }

        for (size_t j = 0; j < cols; ++j) {
            row_totals[i] += contingency_table[i][j];
            col_totals[j] += contingency_table[i][j];
            grand_total += contingency_table[i][j];
        }
    }

    // Calculate chi-square statistic
    double chi2 = 0.0;
    for (size_t i = 0; i < rows; ++i) {
        for (size_t j = 0; j < cols; ++j) {
            double expected = (row_totals[i] * col_totals[j]) / grand_total;
            if (expected > EPSILON) {
                double diff = contingency_table[i][j] - expected;
                chi2 += (diff * diff) / expected;
            }
        }
    }

    return chi2;
}

double ProfessionalStatisticalCalculator::anova(const std::vector<std::vector<double>>& groups) {
    if (groups.size() < 2) {
        return 0.0;
    }

    size_t k = groups.size();
    size_t total_n = 0;

    // Calculate group means and overall mean
    std::vector<double> group_means(k, 0.0);
    std::vector<size_t> group_sizes(k, 0);

    for (size_t i = 0; i < k; ++i) {
        group_sizes[i] = groups[i].size();
        total_n += group_sizes[i];
        group_means[i] = calculate_mean(groups[i]);
    }

    if (total_n < k) {
        return 0.0;
    }

    // Overall mean
    double overall_mean = 0.0;
    for (size_t i = 0; i < k; ++i) {
        overall_mean += group_means[i] * group_sizes[i];
    }
    overall_mean /= total_n;

    // Between-group sum of squares
    double ssb = 0.0;
    for (size_t i = 0; i < k; ++i) {
        double diff = group_means[i] - overall_mean;
        ssb += group_sizes[i] * diff * diff;
    }

    // Within-group sum of squares
    double ssw = 0.0;
    for (size_t i = 0; i < k; ++i) {
        double group_var = calculate_variance(groups[i]);
        ssw += (group_sizes[i] - 1) * group_var;
    }

    // F-statistic
    double msb = ssb / (k - 1);
    double msw = ssw / (total_n - k);

    if (msw < EPSILON) {
        return std::numeric_limits<double>::infinity();
    }

    return msb / msw;
}

double ProfessionalStatisticalCalculator::mann_whitney_u(const std::vector<double>& sample1,
                                                        const std::vector<double>& sample2) {

    if (sample1.empty() || sample2.empty()) {
        return 0.0;
    }

    // Combine and rank samples
    std::vector<std::pair<double, size_t>> combined;
    combined.reserve(sample1.size() + sample2.size());

    for (size_t i = 0; i < sample1.size(); ++i) {
        combined.emplace_back(sample1[i], 1);
    }
    for (size_t i = 0; i < sample2.size(); ++i) {
        combined.emplace_back(sample2[i], 2);
    }

    // Sort by value
    std::sort(combined.begin(), combined.end(),
              [](const auto& a, const auto& b) { return a.first < b.first; });

    // Assign ranks, handling ties
    size_t n = combined.size();
    std::vector<double> ranks(n, 0.0);

    for (size_t i = 0; i < n; ++i) {
        size_t j = i;
        while (j + 1 < n &&
               std::abs(combined[j].first - combined[j + 1].first) < EPSILON) {
            ++j;
        }

        double avg_rank = (i + j + 2) / 2.0;
        for (size_t k = i; k <= j; ++k) {
            ranks[k] = avg_rank;
        }

        i = j;
    }

    // Calculate U statistic
    double R1 = 0.0;
    for (size_t i = 0; i < n; ++i) {
        if (combined[i].second == 1) {
            R1 += ranks[i];
        }
    }

    double n1 = static_cast<double>(sample1.size());
    double n2 = static_cast<double>(sample2.size());
    double U1 = n1 * n2 + n1 * (n1 + 1) / 2.0 - R1;
    double U2 = n1 * n2 - U1;

    return std::min(U1, U2);
}

double ProfessionalStatisticalCalculator::kruskal_wallis(const std::vector<std::vector<double>>& groups) {

    if (groups.empty()) {
        return 0.0;
    }

    size_t k = groups.size();

    // Combine all observations
    std::vector<std::pair<double, size_t>> combined;
    for (size_t i = 0; i < k; ++i) {
        for (double val : groups[i]) {
            combined.emplace_back(val, i);
        }
    }

    if (combined.empty()) {
        return 0.0;
    }

    // Sort by value
    std::sort(combined.begin(), combined.end(),
              [](const auto& a, const auto& b) { return a.first < b.first; });

    // Assign ranks, handling ties
    size_t n = combined.size();
    std::vector<double> ranks(n, 0.0);

    for (size_t i = 0; i < n; ++i) {
        size_t j = i;
        while (j + 1 < n &&
               std::abs(combined[j].first - combined[j + 1].first) < EPSILON) {
            ++j;
        }

        double avg_rank = (i + j + 2) / 2.0;
        for (size_t k = i; k <= j; ++k) {
            ranks[k] = avg_rank;
        }

        i = j;
    }

    // Calculate rank sums for each group
    std::vector<double> rank_sums(k, 0.0);
    std::vector<size_t> group_sizes(k, 0);

    for (size_t i = 0; i < n; ++i) {
        size_t group = combined[i].second;
        rank_sums[group] += ranks[i];
        ++group_sizes[group];
    }

    // Calculate H statistic
    double H = 0.0;
    for (size_t i = 0; i < k; ++i) {
        if (group_sizes[i] > 0) {
            double mean_rank = rank_sums[i] / group_sizes[i];
            H += group_sizes[i] * mean_rank * mean_rank;
        }
    }

    H = 12.0 * H / (n * (n + 1)) - 3.0 * (n + 1);

    // Correction for ties
    double tie_correction = 0.0;
    for (size_t i = 0; i < n; ++i) {
        size_t j = i;
        while (j + 1 < n &&
               std::abs(combined[j].first - combined[j + 1].first) < EPSILON) {
            ++j;
        }

        size_t tie_size = j - i + 1;
        if (tie_size > 1) {
            tie_correction += tie_size * tie_size * tie_size - tie_size;
        }

        i = j;
    }

    if (tie_correction > 0) {
        H /= (1.0 - tie_correction / (n * n * n - n));
    }

    return H;
}

double ProfessionalStatisticalCalculator::erf_inv(double x) {
    // Approximation of inverse error function
    if (x <= -1.0 || x >= 1.0) {
        return std::copysign(std::numeric_limits<double>::infinity(), x);
    }

    double w = -std::log((1.0 - x) * (1.0 + x));
    double p;

    if (w < 6.25) {
        w -= 3.125;
        p =  -3.6444120640178196996e-21;
        p = p * w + -1.685059138182016589e-19;
        p = p * w + 1.2858480715256400167e-18;
        p = p * w + 1.115787767802518096e-17;
        p = p * w + -1.333171662854620906e-16;
        p = p * w + 2.0972767875968561637e-17;
        p = p * w + 6.6376381343583238325e-15;
        p = p * w + -4.0545662729752068639e-14;
        p = p * w + -8.1519341976054721522e-14;
        p = p * w + 2.6335093153082322977e-12;
        p = p * w + -1.2975133253453532498e-11;
        p = p * w + -5.4154120542946279317e-11;
        p = p * w + 1.051212273321532285e-09;
        p = p * w + -4.1126339803469836976e-09;
        p = p * w + -2.9070369957882005086e-08;
        p = p * w + 4.2347877827932403518e-07;
        p = p * w + -1.3654692000834678645e-06;
        p = p * w + -1.3882523362786468719e-05;
        p = p * w + 0.0001867342080340571352;
        p = p * w + -0.00074070253416626697512;
        p = p * w + -0.0060336708714301490533;
        p = p * w + 0.24015818242558961693;
        p = p * w + 1.6536545626831027356;
    } else if (w < 16.0) {
        w = std::sqrt(w) - 3.25;
        p = 2.2137376921775787049e-09;
        p = p * w + 9.0756561938885390979e-08;
        p = p * w + -2.7517406297064545428e-07;
        p = p * w + 1.8239629214389227755e-08;
        p = p * w + 1.5027403968909827627e-06;
        p = p * w + -4.013867526981545969e-06;
        p = p * w + 2.9234449089955446044e-06;
        p = p * w + 1.2475304481671778723e-05;
        p = p * w + -4.7318229009055733981e-05;
        p = p * w + 6.8284851459573175448e-05;
        p = p * w + 2.4031110387097893999e-05;
        p = p * w + -0.0003550375203628474796;
        p = p * w + 0.00095328937973738049703;
        p = p * w + -0.0016882755560235047313;
        p = p * w + 0.0014918758562677198584;
        p = p * w + -0.0003519507332314578658;
        p = p * w + -0.0001059117714486447699;
        p = p * w + 0.00016524200941160652312;
        p = p * w + -0.0000882826980529035381;
        p = p * w + 0.0000121586293559533847;
        p = p * w + 0.0000028214741632107016;
        p = p * w + -0.0000014768498086753052;
        p = p * w + 0.0000002510556516035861;
        p = p * w + -0.0000000146269435390557;
    } else if (w < 27.0) {
        w = std::sqrt(w) - 4.0;
        p = 1.1979303131339706716e-11;
        p = p * w + 1.7073493257715693008e-10;
        p = p * w + -4.3149998444758889273e-10;
        p = p * w + 5.2918553987410863296e-10;
        p = p * w + -1.2372726764463651932e-10;
        p = p * w + -2.5304451319708571059e-09;
        p = p * w + 1.3716910883005010837e-08;
        p = p * w + -2.8087610475778312276e-08;
        p = p * w + 2.6924740161396777375e-08;
        p = p * w + -1.5095118437487155729e-08;
        p = p * w + 4.8951591436417201542e-09;
        p = p * w + -8.7940248231174631767e-10;
        p = p * w + 8.4286253823270354056e-11;
        p = p * w + -3.3481708220583867579e-12;
    } else {
        w = std::sqrt(w) - 6.0;
        p = 1.6189546672890613e-17;
        p = p * w + 3.8917930988979562e-16;
        p = p * w + -4.4351910941332762e-15;
        p = p * w + 2.6625931069976927e-14;
        p = p * w + -9.7204274159814623e-14;
        p = p * w + 2.3647267657449696e-13;
        p = p * w + -4.0698819336423576e-13;
        p = p * w + 5.1887571301120573e-13;
        p = p * w + -4.9047783150134213e-13;
        p = p * w + 3.3489149034177975e-13;
        p = p * w + -1.6145342637749485e-13;
        p = p * w + 5.2492501799118722e-14;
        p = p * w + -1.0889211972115409e-14;
        p = p * w + 1.0546467753604934e-15;
    }

    return p * x;
}

// ============================================
// Continued ProfessionalStatisticalCalculator Implementation
// ============================================

// Mathematical special functions
double ProfessionalStatisticalCalculator::normal_cdf(double x) {
    return 0.5 * (1.0 + std::erf(x / SQRT_2));
}

double ProfessionalStatisticalCalculator::normal_quantile(double p) {
    if (p <= 0.0) return -std::numeric_limits<double>::infinity();
    if (p >= 1.0) return std::numeric_limits<double>::infinity();

    // Beasley-Springer-Moro algorithm
    const double a[] = {
        2.50662823884, -18.61500062529, 41.39119773534, -25.44106049637
    };
    const double b[] = {
        -8.47351093090, 23.08336743743, -21.06224101826, 3.13082909833
    };
    const double c[] = {
        0.3374754822726147, 0.9761690190917186, 0.1607979714918209,
        0.0276438810333863, 0.0038405729373609, 0.0003951896511919,
        0.0000321767881768, 0.0000002888167364, 0.0000003960315187
    };

    double q = p - 0.5;
    double r, xp;

    if (std::abs(q) <= 0.42) {
        r = q * q;
        xp = q * (((a[3] * r + a[2]) * r + a[1]) * r + a[0]) /
                  ((((b[3] * r + b[2]) * r + b[1]) * r + b[0]) * r + 1.0);
    } else {
        r = (q > 0.0) ? 1.0 - p : p;
        r = std::log(-std::log(r));
        xp = c[0] + r * (c[1] + r * (c[2] + r * (c[3] + r * (c[4] + r *
                (c[5] + r * (c[6] + r * (c[7] + r * c[8])))))));
        if (q < 0.0) xp = -xp;
    }

    return xp;
}

double ProfessionalStatisticalCalculator::gamma_function(double x) {
    // Lanczos approximation
    const double g = 5.0;
    const double coeff[] = {
        1.000000000190015,
        76.18009172947146,
        -86.50532032941677,
        24.01409824083091,
        -1.231739572450155,
        0.1208650973866179e-2,
        -0.5395239384953e-5
    };

    if (x < 0.5) {
        // Reflection formula
        return M_PI / (std::sin(M_PI * x) * gamma_function(1.0 - x));
    }

    x -= 1.0;
    double t = x + g + 0.5;
    double sum = coeff[0];

    for (int i = 1; i < 7; ++i) {
        sum += coeff[i] / (x + i);
    }

    return SQRT_2_PI * std::pow(t, x + 0.5) * std::exp(-t) * sum;
}

double ProfessionalStatisticalCalculator::beta_function(double a, double b) {
    return gamma_function(a) * gamma_function(b) / gamma_function(a + b);
}

double ProfessionalStatisticalCalculator::incomplete_gamma(double a, double x) {
    // Series expansion
    if (x < 0.0 || a <= 0.0) return 0.0;

    if (x == 0.0) return 0.0;
    if (x < a + 1.0) {
        // Series representation
        double ap = a;
        double sum = 1.0 / a;
        double del = sum;

        for (int n = 1; n <= 100; ++n) {
            ++ap;
            del *= x / ap;
            sum += del;
            if (std::abs(del) < std::abs(sum) * 1e-15) break;
        }

        return sum * std::exp(-x + a * std::log(x) - std::log(gamma_function(a)));
    } else {
        // Continued fraction representation
        double b = x + 1.0 - a;
        double c = 1.0 / 1e-30;
        double d = 1.0 / b;
        double h = d;

        for (int i = 1; i <= 100; ++i) {
            double an = -i * (i - a);
            b += 2.0;
            d = an * d + b;
            if (std::abs(d) < 1e-30) d = 1e-30;
            c = b + an / c;
            if (std::abs(c) < 1e-30) c = 1e-30;
            d = 1.0 / d;
            double del = d * c;
            h *= del;
            if (std::abs(del - 1.0) < 1e-15) break;
        }

        return 1.0 - h * std::exp(-x + a * std::log(x) - std::log(gamma_function(a)));
    }
}

double ProfessionalStatisticalCalculator::incomplete_beta(double a, double b, double x) {
    if (x <= 0.0) return 0.0;
    if (x >= 1.0) return 1.0;

    // Use continued fraction
    double bt = std::exp(std::lgamma(a + b) - std::lgamma(a) - std::lgamma(b) +
                        a * std::log(x) + b * std::log(1.0 - x));

    if (x < (a + 1.0) / (a + b + 2.0)) {
        // Direct continued fraction
        return bt * incomplete_beta_cf(a, b, x) / a;
    } else {
        // Use symmetry
        return 1.0 - bt * incomplete_beta_cf(b, a, 1.0 - x) / b;
    }
}

double ProfessionalStatisticalCalculator::incomplete_beta_cf(double a, double b, double x) {
    const double eps = 1e-15;
    const int max_iter = 100;

    double qab = a + b;
    double qap = a + 1.0;
    double qam = a - 1.0;
    double c = 1.0;
    double d = 1.0 - qab * x / qap;

    if (std::abs(d) < eps) d = eps;
    d = 1.0 / d;
    double h = d;

    for (int m = 1; m <= max_iter; ++m) {
	int m2 = 2 * m;
        double aa = m * (b - m) * x / ((qam + m2) * (a + m2));
        d = 1.0 + aa * d;
        if (std::abs(d) < eps) d = eps;
        c = 1.0 + aa / c;
        if (std::abs(c) < eps) c = eps;
        d = 1.0 / d;
        h *= d * c;

	aa = -(a + m) * (qab + m) * x / ((a + m2) * (qap + m2));
        d = 1.0 + aa * d;
        if (std::abs(d) < eps) d = eps;
        c = 1.0 + aa / c;
        if (std::abs(c) < eps) c = eps;
        d = 1.0 / d;
        double del = d * c;
        h *= del;

        if (std::abs(del - 1.0) <= eps) break;
    }

    return h;
}


// Helper for incomplete_beta
/*double incomplete_beta_cf(double a, double b, double x) {
    const double eps = 1e-15;
    const int max_iter = 100;

    double qab = a + b;
    double qap = a + 1.0;
    double qam = a - 1.0;
    double c = 1.0;
    double d = 1.0 - qab * x / qap;

    if (std::abs(d) < eps) d = eps;
    d = 1.0 / d;
    double h = d;

    for (int m = 1; m <= max_iter; ++m) {
        int m2 = 2 * m;
        double aa = m * (b - m) * x / ((qam + m2) * (a + m2));
        d = 1.0 + aa * d;
        if (std::abs(d) < eps) d = eps;
        c = 1.0 + aa / c;
        if (std::abs(c) < eps) c = eps;
        d = 1.0 / d;
        h *= d * c;

        aa = -(a + m) * (qab + m) * x / ((a + m2) * (qap + m2));
        d = 1.0 + aa * d;
        if (std::abs(d) < eps) d = eps;
        c = 1.0 + aa / c;
        if (std::abs(c) < eps) c = eps;
        d = 1.0 / d;
        double del = d * c;
        h *= del;

        if (std::abs(del - 1.0) <= eps) break;
    }

    return h;
}*/

// Distribution PDFs
double ProfessionalStatisticalCalculator::normal_pdf(double x, double mu, double sigma) {
    if (sigma <= 0.0) return 0.0;
    double z = (x - mu) / sigma;
    return INV_SQRT_2_PI * std::exp(-0.5 * z * z) / sigma;
}

double ProfessionalStatisticalCalculator::lognormal_pdf(double x, double mu, double sigma) {
    if (x <= 0.0 || sigma <= 0.0) return 0.0;
    double z = (std::log(x) - mu) / sigma;
    return INV_SQRT_2_PI * std::exp(-0.5 * z * z) / (x * sigma);
}

double ProfessionalStatisticalCalculator::exponential_pdf(double x, double lambda) {
    if (x < 0.0 || lambda <= 0.0) return 0.0;
    return lambda * std::exp(-lambda * x);
}

double ProfessionalStatisticalCalculator::gamma_pdf(double x, double alpha, double beta) {
    if (x < 0.0 || alpha <= 0.0 || beta <= 0.0) return 0.0;
    return std::pow(beta, alpha) * std::pow(x, alpha - 1.0) *
           std::exp(-beta * x) / gamma_function(alpha);
}

double ProfessionalStatisticalCalculator::beta_pdf(double x, double alpha, double beta) {
    if (x < 0.0 || x > 1.0 || alpha <= 0.0 || beta <= 0.0) return 0.0;
    return std::pow(x, alpha - 1.0) * std::pow(1.0 - x, beta - 1.0) /
           beta_function(alpha, beta);
}

double ProfessionalStatisticalCalculator::weibull_pdf(double x, double lambda, double k) {
    if (x < 0.0 || lambda <= 0.0 || k <= 0.0) return 0.0;
    if (x == 0.0 && k < 1.0) return std::numeric_limits<double>::infinity();
    return (k / lambda) * std::pow(x / lambda, k - 1.0) *
           std::exp(-std::pow(x / lambda, k));
}

// Maximum Likelihood Estimation implementations
std::map<std::string, double> ProfessionalStatisticalCalculator::mle_normal(const std::vector<double>& values) {
    std::map<std::string, double> params;
    if (values.empty()) return params;

    params["mean"] = calculate_mean(values);
    params["std_dev"] = calculate_std_dev(values, true);

    return params;
}

std::map<std::string, double> ProfessionalStatisticalCalculator::mle_lognormal(const std::vector<double>& values) {
    std::map<std::string, double> params;
    if (values.empty()) return params;

    // Check for positive values
    if (!std::all_of(values.begin(), values.end(), [](double x) { return x > 0; })) {
        return params;
    }

    std::vector<double> log_values;
    log_values.reserve(values.size());
    for (double val : values) {
        log_values.push_back(std::log(val));
    }

    params["mu"] = calculate_mean(log_values);
    params["sigma"] = calculate_std_dev(log_values, true);

    return params;
}

std::map<std::string, double> ProfessionalStatisticalCalculator::mle_exponential(const std::vector<double>& values) {
    std::map<std::string, double> params;
    if (values.empty()) return params;

    // Check for non-negative values
    if (!std::all_of(values.begin(), values.end(), [](double x) { return x >= 0; })) {
        return params;
    }

    params["lambda"] = 1.0 / calculate_mean(values);
    params["rate"] = params["lambda"];

    return params;
}

std::map<std::string, double> ProfessionalStatisticalCalculator::mle_gamma(const std::vector<double>& values) {
    std::map<std::string, double> params;
    if (values.size() < 2) return params;

    // Check for positive values
    if (!std::all_of(values.begin(), values.end(), [](double x) { return x > 0; })) {
        return params;
    }

    // Use method of moments as initial guess
    double mean = calculate_mean(values);
    double variance = calculate_variance(values, true);

    // Initial parameters
    double alpha = (mean * mean) / variance;
    double beta = mean / variance;

    // Newton-Raphson for MLE
    const size_t max_iter = 100;
    const double tol = 1e-8;
    double n = static_cast<double>(values.size());

    double sum_log = 0.0;
    for (double val : values) {
        sum_log += std::log(val);
    }
    double mean_log = sum_log / n;

    for (size_t iter = 0; iter < max_iter; ++iter) {
        // Compute digamma(alpha) and trigamma(alpha)
        double psi = digamma(alpha);
        double psi1 = trigamma(alpha);

        // Gradient
        double g = std::log(alpha) - psi + mean_log - std::log(mean) +
                   std::log(alpha) - std::log(alpha + beta);

        // Hessian
        double h = 1.0 / alpha - psi1 - 1.0 / (alpha + beta);

        if (std::abs(h) < EPSILON) break;

        double delta = -g / h;
        alpha += delta;

        if (std::abs(delta) < tol) break;
    }

    beta = alpha / mean;

    params["alpha"] = alpha;
    params["beta"] = beta;
    params["shape"] = alpha;
    params["rate"] = beta;

    return params;
}

std::map<std::string, double> ProfessionalStatisticalCalculator::mle_beta(const std::vector<double>& values) {
    std::map<std::string, double> params;
    if (values.size() < 2) return params;

    // Check values are in [0, 1]
    if (!std::all_of(values.begin(), values.end(), [](double x) { return x >= 0 && x <= 1; })) {
        return params;
    }

    double mean = calculate_mean(values);
    double variance = calculate_variance(values, true);

    if (variance >= mean * (1.0 - mean)) {
        // Variance too large
        return params;
    }

    // Method of moments
    double alpha = mean * ((mean * (1.0 - mean) / variance) - 1.0);
    double beta = (1.0 - mean) * ((mean * (1.0 - mean) / variance) - 1.0);

    // Ensure parameters are positive
    if (alpha <= 0.0 || beta <= 0.0) {
        return params;
    }

    params["alpha"] = alpha;
    params["beta"] = beta;

    return params;
}

std::map<std::string, double> ProfessionalStatisticalCalculator::mle_weibull(const std::vector<double>& values) {
    std::map<std::string, double> params;
    if (values.size() < 2) return params;

    // Check for non-negative values
    if (!std::all_of(values.begin(), values.end(), [](double x) { return x >= 0; })) {
        return params;
    }

    // Use method of moments as initial guess
    double mean = calculate_mean(values);
    double variance = calculate_variance(values, true);
    double cv = std::sqrt(variance) / mean;

    // Initial guess for k (shape)
    double k = 1.2 / cv;
    double lambda = mean / gamma_function(1.0 + 1.0 / k);

    // Newton-Raphson for MLE
    const size_t max_iter = 100;
    const double tol = 1e-8;
    double n = static_cast<double>(values.size());

    double sum_log = 0.0;
    double sum_xk_log_x = 0.0;
    double sum_xk = 0.0;

    for (double val : values) {
        if (val > 0) {
            sum_log += std::log(val);
        }
    }

    for (size_t iter = 0; iter < max_iter; ++iter) {
        // Update sums
        sum_xk_log_x = 0.0;
        sum_xk = 0.0;

        for (double val : values) {
            if (val > 0) {
                double xk = std::pow(val, k);
                sum_xk += xk;
                sum_xk_log_x += xk * std::log(val);
            }
        }

        // Update lambda
        lambda = std::pow(sum_xk / n, 1.0 / k);

        // Gradient for k
        double g = n / k + sum_log - (n / sum_xk) * sum_xk_log_x;

        // Hessian for k
        double h = -n / (k * k) - (n / (sum_xk * sum_xk)) *
                  (sum_xk * sum_xk_log_x * sum_xk_log_x / sum_xk -
                   sum_xk * sum_xk_log_x * sum_xk_log_x / sum_xk);

        if (std::abs(h) < EPSILON) break;

        double delta = -g / h;
        k += delta;

        if (std::abs(delta) < tol) break;
    }

    params["k"] = k;
    params["lambda"] = lambda;
    params["shape"] = k;
    params["scale"] = lambda;

    return params;
}

// Digamma and trigamma functions (needed for gamma MLE)
double ProfessionalStatisticalCalculator::digamma(double x) {
    // Euler-Maclaurin expansion
    const double c = 12.0;
    const double s = 1e-6;
    double r = 0.0;

    if (x <= 0.0) return 0.0;

    while (x < c) {
        r -= 1.0 / x;
        x += 1.0;
    }

    double f = 1.0 / (x * x);
    double t = f * (-1.0/12.0 + f * (1.0/120.0 + f * (-1.0/252.0 +
            f * (1.0/240.0 + f * (-1.0/132.0 + f * (691.0/32760.0 +
            f * (-1.0/12.0 + f * 3617.0/8160.0)))))));

    return r + std::log(x) - 0.5 / x + t;
}

double ProfessionalStatisticalCalculator::trigamma(double x) {
    // Asymptotic expansion
    const double c = 12.0;
    const double s = 1e-6;
    double r = 0.0;

    if (x <= 0.0) return 0.0;

    while (x < c) {
        r += 1.0 / (x * x);
        x += 1.0;
    }

    double f = 1.0 / (x * x);
    double t = f * (1.0 + f * (1.0/2.0 + f * (1.0/6.0 + f * (-1.0/30.0 +
            f * (1.0/42.0 + f * (-1.0/30.0 + f * (5.0/66.0 +
            f * (-691.0/2730.0 + f * 7.0/6.0))))))));

    return r + 1.0 / x + 0.5 * f + t;
}

// ============================================
// Struct Method Implementations
// ============================================

// ProfessionalColumnAnalysis methods
std::string ProfessionalColumnAnalysis::to_detailed_string() const {
    std::stringstream ss;
    ss << "Column Analysis: " << name << "\n";
    ss << "========================================\n";
    ss << "Type: " << detected_type << "\n";
    ss << "Total Count: " << total_count << "\n";
    ss << "Null Count: " << null_count << " (" << missing_percentage << "%)\n";
    ss << "Distinct Count: " << distinct_count << "\n";

    if (detected_type == "numeric" || detected_type == "integer" ||
        detected_type == "float" || detected_type == "double") {
        ss << "\nNumerical Statistics:\n";
        ss << "  Min: " << min_value << "\n";
        ss << "  Q1: " << q1 << "\n";
        ss << "  Median: " << median << "\n";
        ss << "  Q3: " << q3 << "\n";
        ss << "  Max: " << max_value << "\n";
        ss << "  Mean: " << mean << "\n";
        ss << "  Std Dev: " << std_dev << "\n";
        ss << "  Variance: " << variance << "\n";
        ss << "  Skewness: " << skewness << "\n";
        ss << "  Kurtosis: " << kurtosis << "\n";
        ss << "  IQR: " << iqr << "\n";
        ss << "  Range: " << range << "\n";
        ss << "  CV: " << coefficient_of_variation << "\n";
        ss << "  MAD: " << mad << "\n";

        ss << "\nNormality Tests:\n";
        ss << "  Shapiro-Wilk: " << shapiro_wilk << " (p=" << normality_p_value << ")\n";
        ss << "  Jarque-Bera: " << jarque_bera << "\n";
        ss << "  Anderson-Darling: " << anderson_darling << "\n";
        ss << "  Is Normal: " << (is_normal ? "Yes" : "No") << "\n";

        ss << "\nPercentiles:\n";
        ss << "  P10: " << p10 << "\n";
        ss << "  P90: " << p90 << "\n";
        ss << "  P95: " << p95 << "\n";
        ss << "  P99: " << p99 << "\n";

        if (has_outliers) {
            ss << "\nOutliers:\n";
            ss << "  Count: " << outliers.size() << " (" << outlier_percentage << "%)\n";
            ss << "  Indices: ";
            for (size_t i = 0; i < std::min(outlier_indices.size(), (size_t)10); ++i) {
                if (i > 0) ss << ", ";
                ss << outlier_indices[i];
            }
            if (outlier_indices.size() > 10) ss << ", ...";
            ss << "\n";
        }
    } else if (is_categorical) {
        ss << "\nCategorical Statistics:\n";
        ss << "  Cardinality Ratio: " << categorical_stats.cardinality_ratio << "\n";
        ss << "  Avg Category Length: " << categorical_stats.average_category_length << "\n";

        if (!top_categories.empty()) {
            ss << "  Top Categories:\n";
            for (size_t i = 0; i < std::min(top_categories.size(), (size_t)5); ++i) {
                ss << "    " << top_categories[i] << "\n";
            }
        }

        if (!categorical_stats.rare_categories.empty()) {
            ss << "  Rare Categories (<1%): " << categorical_stats.rare_categories.size() << "\n";
        }
    }

    ss << "\nData Quality Scores:\n";
    ss << "  Completeness: " << (quality.completeness_score * 100) << "%\n";
    ss << "  Consistency: " << (quality.consistency_score * 100) << "%\n";
    ss << "  Accuracy: " << (quality.accuracy_score * 100) << "%\n";
    ss << "  Overall: " << (quality.overall_score * 100) << "%\n";

    if (!quality.issues.empty()) {
        ss << "\nIssues:\n";
        for (const auto& issue : quality.issues) {
            ss << "  - " << issue << "\n";
        }
    }

    if (!quality.recommendations.empty()) {
        ss << "\nRecommendations:\n";
        for (const auto& rec : quality.recommendations) {
            ss << "  - " << rec << "\n";
        }
    }

    return ss.str();
}

json ProfessionalColumnAnalysis::to_json(bool detailed) const {
    json j;

    j["name"] = name;
    j["detected_type"] = detected_type;
    j["total_count"] = total_count;
    j["null_count"] = null_count;
    j["distinct_count"] = distinct_count;
    j["missing_percentage"] = missing_percentage;

    if (detailed) {
        j["min_value"] = min_value;
        j["max_value"] = max_value;
        j["range"] = range;
        j["mean"] = mean;
        j["median"] = median;
        j["std_dev"] = std_dev;
        j["variance"] = variance;
        j["skewness"] = skewness;
        j["kurtosis"] = kurtosis;
        j["jarque_bera"] = jarque_bera;
        j["shapiro_wilk"] = shapiro_wilk;
        j["anderson_darling"] = anderson_darling;
        j["q1"] = q1;
        j["q3"] = q3;
        j["iqr"] = iqr;
        j["p10"] = p10;
        j["p90"] = p90;
        j["p95"] = p95;
        j["p99"] = p99;
        j["mad"] = mad;
        j["coefficient_of_variation"] = coefficient_of_variation;
        j["entropy"] = entropy;
        j["is_normal"] = is_normal;
        j["is_categorical"] = is_categorical;
        j["has_outliers"] = has_outliers;
        j["outlier_percentage"] = outlier_percentage;

        // Add quality scores
        json quality_json;
        quality_json["completeness_score"] = quality.completeness_score;
        quality_json["consistency_score"] = quality.consistency_score;
        quality_json["accuracy_score"] = quality.accuracy_score;
        quality_json["timeliness_score"] = quality.timeliness_score;
        quality_json["validity_score"] = quality.validity_score;
        quality_json["uniqueness_score"] = quality.uniqueness_score;
        quality_json["overall_score"] = quality.overall_score;
        quality_json["issues"] = quality.issues;
        quality_json["recommendations"] = quality.recommendations;
        j["quality"] = quality_json;

        // Add histogram if available
        if (!histogram_counts.empty()) {
            json hist_json;
            for (size_t i = 0; i < histogram_bins.size() && i < histogram_counts.size(); ++i) {
                json bin_json;
                bin_json["bin"] = histogram_bins[i];
                bin_json["count"] = histogram_counts[i];
                hist_json.push_back(bin_json);
            }
            j["histogram"] = hist_json;
        }

        // Add outliers if available
        if (!outliers.empty()) {
            j["outliers"] = outliers;
            j["outlier_indices"] = outlier_indices;
        }

        // Add top categories for categorical columns
        if (!top_categories.empty()) {
            j["top_categories"] = top_categories;
        }
    }

    return j;
}

std::string ProfessionalColumnAnalysis::to_markdown() const {
    std::stringstream ss;
    ss << "### " << name << "\n\n";
    ss << "**Type:** " << detected_type << "  \n";
    ss << "**Total:** " << total_count << "  \n";
    ss << "**Nulls:** " << null_count << " (" << missing_percentage << "%)  \n";
    ss << "**Distinct:** " << distinct_count << "  \n";

    if (detected_type == "numeric" || detected_type == "integer" ||
        detected_type == "float" || detected_type == "double") {
        ss << "\n**Numerical Statistics**\n\n";
        ss << "| Statistic | Value |\n";
        ss << "|-----------|-------|\n";
        ss << "| Min | " << min_value << " |\n";
        ss << "| Q1 | " << q1 << " |\n";
        ss << "| Median | " << median << " |\n";
        ss << "| Q3 | " << q3 << " |\n";
        ss << "| Max | " << max_value << " |\n";
        ss << "| Mean | " << mean << " |\n";
        ss << "| Std Dev | " << std_dev << " |\n";
        ss << "| Skewness | " << skewness << " |\n";
        ss << "| Kurtosis | " << kurtosis << " |\n";
        ss << "| IQR | " << iqr << " |\n";
        ss << "| CV | " << coefficient_of_variation << " |\n";

        ss << "\n**Normality Tests**\n\n";
        ss << "| Test | Statistic | p-value |\n";
        ss << "|------|-----------|---------|\n";
        ss << "| Shapiro-Wilk | " << shapiro_wilk << " | " << normality_p_value << " |\n";
        ss << "| Jarque-Bera | " << jarque_bera << " | - |\n";

        ss << "\n**Data Quality:** " << (quality.overall_score * 100) << "%  \n";

        if (has_outliers) {
            ss << "\n**Outliers:** " << outliers.size() << " (" << outlier_percentage << "%)  \n";
        }
    }

    return ss.str();
}

std::string ProfessionalColumnAnalysis::to_csv() const {
    std::stringstream ss;
    ss << name << ",";
    ss << detected_type << ",";
    ss << total_count << ",";
    ss << null_count << ",";
    ss << missing_percentage << ",";
    ss << distinct_count << ",";
    ss << mean << ",";
    ss << median << ",";
    ss << std_dev << ",";
    ss << min_value << ",";
    ss << max_value << ",";
    ss << q1 << ",";
    ss << q3 << ",";
    ss << skewness << ",";
    ss << kurtosis << ",";
    ss << (has_outliers ? "Yes" : "No") << ",";
    ss << (quality.overall_score * 100);
    return ss.str();
}

// ProfessionalCorrelationAnalysis methods
std::string ProfessionalCorrelationAnalysis::to_detailed_string() const {
    std::stringstream ss;
    ss << "Correlation Analysis: " << column1 << " vs " << column2 << "\n";
    ss << "========================================\n";

    ss << "Pearson's r: " << pearson_r << " (r = " << pearson_r_squared << ")\n";
    ss << "p-value: " << pearson_p_value << " ";
    ss << (is_statistically_significant ? "(significant)" : "(not significant)") << "\n";
    ss << "95% CI: [" << pearson_confidence_lower << ", " << pearson_confidence_upper << "]\n";

    ss << "\nSpearman's : " << spearman_rho << "\n";
    ss << "p-value: " << spearman_p_value << "\n";

    ss << "\nKendall's : " << kendall_tau << "\n";
    ss << "p-value: " << kendall_p_value << "\n";

    ss << "\nDistance Correlation: " << distance_correlation << "\n";
    ss << "Mutual Information: " << mutual_information << "\n";
    ss << "Normalized MI: " << normalized_mutual_information << "\n";

    ss << "\nMIC: " << mic << "\n";
    ss << "MAS: " << mas << "\n";
    ss << "MEV: " << mev << "\n";
    ss << "MCN: " << mcn << "\n";

    ss << "\nRelationship: " << relationship_strength << " " << relationship_direction << "\n";
    ss << "Type: " << relationship_type << "\n";
    ss << "Effect Size (Cohen's d): " << effect_size << "\n";

    ss << "\nGranger Causality: " << (granger_causes ? "Yes" : "No") << "\n";
    if (granger_causes) {
        ss << "F-statistic: " << granger_f_statistic << "\n";
        ss << "p-value: " << granger_p_value << "\n";
    }

    ss << "\nCointegration: " << (cointegrated ? "Yes" : "No") << "\n";
    if (cointegrated) {
        ss << "p-value: " << cointegration_p_value << "\n";
    }

    return ss.str();
}

json ProfessionalCorrelationAnalysis::to_json() const {
    json j;

    j["column1"] = column1;
    j["column2"] = column2;

    j["pearson"] = {
        {"r", pearson_r},
        {"r_squared", pearson_r_squared},
        {"p_value", pearson_p_value},
        {"confidence_lower", pearson_confidence_lower},
        {"confidence_upper", pearson_confidence_upper}
    };

    j["spearman"] = {
        {"rho", spearman_rho},
        {"p_value", spearman_p_value}
    };

    j["kendall"] = {
        {"tau", kendall_tau},
        {"p_value", kendall_p_value}
    };

    j["distance_correlation"] = distance_correlation;
    j["mutual_information"] = mutual_information;
    j["normalized_mutual_information"] = normalized_mutual_information;
    j["adjusted_mutual_information"] = adjusted_mutual_information;

    j["maximal_information_coefficient"] = {
        {"mic", mic},
        {"mas", mas},
        {"mev", mev},
        {"mcn", mcn}
    };

    j["partial_correlation"] = partial_correlation;
    j["semi_partial_correlation"] = semi_partial_correlation;
    j["canonical_correlation"] = canonical_correlation;

    j["cross_correlation"] = {
        {"correlation", cross_correlation},
        {"lag", cross_correlation_lag}
    };

    j["granger_causality"] = {
        {"causes", granger_causes},
        {"f_statistic", granger_f_statistic},
        {"p_value", granger_p_value}
    };

    j["cointegration"] = {
        {"cointegrated", cointegrated},
        {"p_value", cointegration_p_value}
    };

    j["interpretation"] = {
        {"strength", relationship_strength},
        {"direction", relationship_direction},
        {"type", relationship_type},
        {"statistically_significant", is_statistically_significant},
        {"practically_significant", is_practically_significant},
        {"effect_size", effect_size}
    };

    return j;
}

// ProfessionalFeatureImportance methods
std::string ProfessionalFeatureImportance::to_detailed_string() const {
    std::stringstream ss;
    ss << "Feature Importance: " << feature_name << "\n";
    ss << "========================================\n";
    ss << "Type: " << feature_type << "\n";

    ss << "\nImportance Scores:\n";
    ss << "  Random Forest: " << scores.random_forest << "\n";
    ss << "  XGBoost: " << scores.xgboost << "\n";
    ss << "  SHAP: " << scores.shap << "\n";
    ss << "  Mutual Information: " << scores.mutual_information << "\n";
    ss << "  Pearson: " << scores.pearson << "\n";
    ss << "  Spearman: " << scores.spearman << "\n";
    ss << "  Lasso: " << scores.lasso_coefficient << "\n";

    ss << "\nStatistical Significance:\n";
    ss << "  Best p-value: " << significance.best_p_value << " (" << significance.best_method << ")\n";
    ss << "  Pearson significant: " << (significance.pearson_significant ? "Yes" : "No") << "\n";
    ss << "  MI significant: " << (significance.mi_significant ? "Yes" : "No") << "\n";

    ss << "\nStability Analysis:\n";
    ss << "  Variance across folds: " << stability.variance_across_folds << "\n";
    ss << "  Rank stability: " << stability.rank_stability << "\n";
    ss << "  Jaccard similarity: " << stability.jaccard_similarity << "\n";

    ss << "\nSHAP Analysis:\n";
    ss << "  Mean |SHAP|: " << shapley.mean_abs_shap << "\n";
    ss << "  Std SHAP: " << shapley.std_shap << "\n";
    ss << "  Min SHAP: " << shapley.min_shap << "\n";
    ss << "  Max SHAP: " << shapley.max_shap << "\n";

    if (!top_interactions.empty()) {
        ss << "\nTop Interactions:\n";
        for (const auto& [feature, score] : top_interactions) {
            ss << "  " << feature << ": " << score << "\n";
        }
    }

    ss << "\nNon-linear Effects:\n";
    ss << "  MIC: " << maximal_information_coefficient << "\n";
    ss << "  Distance Correlation: " << distance_correlation << "\n";
    ss << "  Hoeffding D: " << hoeffding_d << "\n";

    ss << "\nBusiness Impact:\n";
    ss << "  Business Value Score: " << business_value_score << "\n";
    ss << "  Data Collection Cost: " << data_collection_cost << "\n";
    ss << "  Stability Score: " << feature_stability_score << "\n";
    ss << "  Freshness Score: " << feature_freshness_score << "\n";

    ss << "\nFeature Engineering Suggestions:\n";
    for (const auto& suggestion : transformation_suggestions) {
        ss << "  - " << suggestion << "\n";
    }
    for (const auto& suggestion : interaction_suggestions) {
        ss << "  - " << suggestion << "\n";
    }
    for (const auto& suggestion : encoding_suggestions) {
        ss << "  - " << suggestion << "\n";
    }

    ss << "\nRecommended Action: ";
    switch (recommended_action) {
        case KEEP_AS_IS: ss << "Keep as is"; break;
        case TRANSFORM: ss << "Transform"; break;
        case CREATE_INTERACTION: ss << "Create interaction"; break;
        case BIN: ss << "Bin"; break;
        case ENCODE: ss << "Encode"; break;
        case DROP: ss << "Drop"; break;
        case MONITOR: ss << "Monitor"; break;
    }
    ss << "\n";

    return ss.str();
}

json ProfessionalFeatureImportance::to_json() const {
    json j;

    j["feature_name"] = feature_name;
    j["feature_type"] = feature_type;

    // Importance scores
    json scores_json;
    scores_json["pearson"] = scores.pearson;
    scores_json["spearman"] = scores.spearman;
    scores_json["mutual_information"] = scores.mutual_information;
    scores_json["chi_square"] = scores.chi_square;
    scores_json["anova_f"] = scores.anova_f;
    scores_json["lasso_coefficient"] = scores.lasso_coefficient;
    scores_json["random_forest"] = scores.random_forest;
    scores_json["xgboost"] = scores.xgboost;
    scores_json["lightgbm"] = scores.lightgbm;
    scores_json["shap"] = scores.shap;
    scores_json["lime"] = scores.lime;
    scores_json["permutation"] = scores.permutation;
    scores_json["boruta"] = scores.boruta;
    scores_json["relief"] = scores.relief;
    scores_json["mrmr"] = scores.mrmr;
    scores_json["fisher_score"] = scores.fisher_score;
    scores_json["laplacian_score"] = scores.laplacian_score;
    j["scores"] = scores_json;

    // Statistical significance
    json significance_json;
    significance_json["pearson_significant"] = significance.pearson_significant;
    significance_json["spearman_significant"] = significance.spearman_significant;
    significance_json["mi_significant"] = significance.mi_significant;
    significance_json["chi2_significant"] = significance.chi2_significant;
    significance_json["anova_significant"] = significance.anova_significant;
    significance_json["best_p_value"] = significance.best_p_value;
    significance_json["best_method"] = significance.best_method;
    j["statistical_significance"] = significance_json;

    // Stability
    json stability_json;
    stability_json["variance_across_folds"] = stability.variance_across_folds;
    stability_json["rank_stability"] = stability.rank_stability;
    stability_json["jaccard_similarity"] = stability.jaccard_similarity;
    stability_json["importance_across_folds"] = stability.importance_across_folds;
    stability_json["rank_across_folds"] = stability.rank_across_folds;
    j["stability"] = stability_json;

    // Interactions
    json interactions_json;
    for (const auto& [feature, score] : top_interactions) {
        interactions_json[feature] = score;
    }
    j["top_interactions"] = interactions_json;

    // Non-linear effects
    j["maximal_information_coefficient"] = maximal_information_coefficient;
    j["distance_correlation"] = distance_correlation;
    j["hoeffding_d"] = hoeffding_d;

    // Shapley analysis
    json shapley_json;
    shapley_json["mean_abs_shap"] = shapley.mean_abs_shap;
    shapley_json["std_shap"] = shapley.std_shap;
    shapley_json["min_shap"] = shapley.min_shap;
    shapley_json["max_shap"] = shapley.max_shap;
    shapley_json["shap_values"] = shapley.shap_values;
    shapley_json["shap_interaction_values"] = shapley.shap_interaction_values;
    j["shapley_analysis"] = shapley_json;

    // Business impact
    j["business_value_score"] = business_value_score;
    j["data_collection_cost"] = data_collection_cost;
    j["feature_stability_score"] = feature_stability_score;
    j["feature_freshness_score"] = feature_freshness_score;

    // Recommendations
    j["transformation_suggestions"] = transformation_suggestions;
    j["interaction_suggestions"] = interaction_suggestions;
    j["encoding_suggestions"] = encoding_suggestions;

    std::string action_str;
    switch (recommended_action) {
        case KEEP_AS_IS: action_str = "KEEP_AS_IS"; break;
        case TRANSFORM: action_str = "TRANSFORM"; break;
        case CREATE_INTERACTION: action_str = "CREATE_INTERACTION"; break;
        case BIN: action_str = "BIN"; break;
        case ENCODE: action_str = "ENCODE"; break;
        case DROP: action_str = "DROP"; break;
        case MONITOR: action_str = "MONITOR"; break;
    }
    j["recommended_action"] = action_str;

    return j;
}

// ProfessionalClusterAnalysis methods
std::string ProfessionalClusterAnalysis::to_detailed_string() const {
    std::stringstream ss;
    ss << "Cluster Analysis: " << cluster_label << " (ID: " << cluster_id << ")\n";
    ss << "========================================\n";
    ss << "Size: " << size << " (" << size_percentage << "%)\n";

    ss << "\nQuality Metrics:\n";
    ss << "  Silhouette Score: " << silhouette_score << "\n";
    ss << "  Davies-Bouldin Index: " << davies_bouldin_index << "\n";
    ss << "  Calinski-Harabasz Index: " << calinski_harabasz_index << "\n";
    ss << "  Dunn Index: " << dun_index << "\n";
    ss << "  C Index: " << c_index << "\n";
    ss << "  Gamma Index: " << gamma_index << "\n";

    ss << "\nInternal Cohesion:\n";
    ss << "  Within-cluster SS: " << within_cluster_sum_of_squares << "\n";
    ss << "  Avg intra-cluster distance: " << average_intra_cluster_distance << "\n";
    ss << "  Max intra-cluster distance: " << maximum_intra_cluster_distance << "\n";
    ss << "  Diameter: " << diameter << "\n";

    ss << "\nSeparation:\n";
    ss << "  Min inter-cluster distance: " << minimum_inter_cluster_distance << "\n";
    ss << "  Avg inter-cluster distance: " << average_inter_cluster_distance << "\n";

    ss << "\nDensity and Sparsity:\n";
    ss << "  Density: " << density << "\n";
    ss << "  Sparsity: " << sparsity << "\n";

    ss << "\nStability:\n";
    ss << "  Jaccard Stability: " << jaccard_stability << "\n";
    ss << "  Rand Stability: " << rand_stability << "\n";

    if (!defining_features.empty()) {
        ss << "\nDefining Features:\n";
        for (const auto& feature : defining_features) {
            ss << "  - " << feature << "\n";
        }
    }

    if (!feature_importances.empty()) {
        ss << "\nFeature Importances:\n";
        for (const auto& [feature, importance] : feature_importances) {
            ss << "  " << feature << ": " << importance << "\n";
        }
    }

    if (outlier_percentage > 0) {
        ss << "\nOutliers within cluster: " << outlier_percentage << "%\n";
        ss << "  Count: " << outlier_indices.size() << "\n";
    }

    if (!business_interpretation.empty()) {
        ss << "\nBusiness Interpretation:\n";
        ss << "  " << business_interpretation << "\n";
    }

    if (!key_characteristics.empty()) {
        ss << "\nKey Characteristics:\n";
        for (const auto& characteristic : key_characteristics) {
            ss << "  - " << characteristic << "\n";
        }
    }

    if (!action_items.empty()) {
        ss << "\nAction Items:\n";
        for (const auto& action : action_items) {
            ss << "  - " << action << "\n";
        }
    }

    return ss.str();
}

json ProfessionalClusterAnalysis::to_json() const {
    json j;

    j["cluster_id"] = cluster_id;
    j["cluster_label"] = cluster_label;
    j["size"] = size;
    j["size_percentage"] = size_percentage;

    // Centroid
    j["centroid"] = centroid;
    j["centroid_std"] = centroid_std;
    j["medoid"] = medoid;

    // Quality metrics
    j["quality_metrics"] = {
        {"silhouette_score", silhouette_score},
        {"davies_bouldin_index", davies_bouldin_index},
        {"calinski_harabasz_index", calinski_harabasz_index},
        {"dun_index", dun_index},
        {"c_index", c_index},
        {"gamma_index", gamma_index}
    };

    // Internal cohesion
    j["internal_cohesion"] = {
        {"within_cluster_sum_of_squares", within_cluster_sum_of_squares},
        {"average_intra_cluster_distance", average_intra_cluster_distance},
        {"maximum_intra_cluster_distance", maximum_intra_cluster_distance},
        {"diameter", diameter}
    };

    // Separation
    j["separation"] = {
        {"distances_to_other_clusters", distances_to_other_clusters},
        {"minimum_inter_cluster_distance", minimum_inter_cluster_distance},
        {"average_inter_cluster_distance", average_inter_cluster_distance}
    };

    // Density
    j["density"] = {
        {"density", density},
        {"sparsity", sparsity}
    };

    // Stability
    j["stability"] = {
        {"jaccard_stability", jaccard_stability},
        {"rand_stability", rand_stability}
    };

    // Profile
    json profile_json;
    for (const auto& [key, value] : profile.numeric_profile) {
        profile_json["numeric"][key] = value;
    }
    for (const auto& [key, value] : profile.categorical_profile) {
        profile_json["categorical"][key] = value;
    }
    for (const auto& [key, value] : profile.anomaly_scores) {
        profile_json["anomaly_scores"][key] = value;
    }
    j["profile"] = profile_json;

    // Features
    j["defining_features"] = defining_features;

    json feature_importances_json;
    for (const auto& [feature, importance] : feature_importances) {
        feature_importances_json[feature] = importance;
    }
    j["feature_importances"] = feature_importances_json;

    // Outliers
    j["outliers"] = {
        {"indices", outlier_indices},
        {"percentage", outlier_percentage}
    };

    // Business interpretation
    j["business_interpretation"] = business_interpretation;
    j["key_characteristics"] = key_characteristics;
    j["action_items"] = action_items;

    // Temporal stability
    j["temporal_stability"] = temporal_stability;
    j["size_over_time"] = size_over_time;

    return j;
}

// ProfessionalOutlierAnalysis methods
std::string ProfessionalOutlierAnalysis::to_detailed_string() const {
    std::stringstream ss;
    ss << "Outlier Analysis\n";
    ss << "========================================\n";
    ss << "Total Outliers: " << outlier_indices.size() << "\n";

    ss << "\nSeverity Classification:\n";
    ss << "  Critical: " << severity.critical_count << "\n";
    ss << "  High: " << severity.high_count << "\n";
    ss << "  Medium: " << severity.medium_count << "\n";
    ss << "  Low: " << severity.low_count << "\n";

    if (!root_cause.contributing_features.empty()) {
        ss << "\nRoot Cause Analysis:\n";
        ss << "  Likely Cause: " << root_cause.likely_cause << "\n";
        ss << "  Confidence: " << (root_cause.confidence * 100) << "%\n";
        ss << "  Contributing Features:\n";
        for (size_t i = 0; i < root_cause.contributing_features.size(); ++i) {
            ss << "    " << root_cause.contributing_features[i] << ": "
               << root_cause.feature_contributions[i] << "\n";
        }
    }

    ss << "\nImpact Analysis:\n";
    ss << "  On Mean: " << impact.on_mean << "\n";
    ss << "  On Variance: " << impact.on_variance << "\n";
    ss << "  On Correlation: " << impact.on_correlation << "\n";
    ss << "  On Model Performance: " << impact.on_model_performance << "\n";
    ss << "  On Statistical Tests: " << impact.on_statistical_tests << "\n";

    ss << "\nTemporal Patterns:\n";
    ss << "  Clustered in Time: " << (temporal_pattern.is_clustered_in_time ? "Yes" : "No") << "\n";
    ss << "  Temporal Autocorrelation: " << temporal_pattern.temporal_autocorrelation << "\n";
    ss << "  Seasonality Pattern: " << temporal_pattern.seasonality_pattern << "\n";

    ss << "\nRecommended Actions:\n";
    ss << "  Overall Strategy: " << recommendations.overall_strategy << "\n";
    ss << "  Investigate: " << recommendations.should_investigate.size() << " outliers\n";
    ss << "  Remove: " << recommendations.should_remove.size() << " outliers\n";
    ss << "  Cap: " << recommendations.should_cap.size() << " outliers\n";
    ss << "  Impute: " << recommendations.should_impute.size() << " outliers\n";
    ss << "  Keep: " << recommendations.should_keep.size() << " outliers\n";

    ss << "\nMultivariate Analysis:\n";
    ss << "  Affected Dimensions: " << multivariate.affected_dimensions.size() << "\n";
    ss << "  Mahalanobis Distances: " << multivariate.mahalanobis_distances.size() << "\n";
    ss << "  Cook's Distances: " << multivariate.cook_distances.size() << "\n";
    ss << "  Leverage Scores: " << multivariate.leverage_scores.size() << "\n";
    ss << "  Influence Scores: " << multivariate.influence_scores.size() << "\n";

    return ss.str();
}

json ProfessionalOutlierAnalysis::to_json() const {
    json j;

    // Detection results
    j["outlier_indices"] = outlier_indices;
    j["outlier_scores"] = outlier_scores;
    j["outlier_types"] = outlier_types;
    j["detection_methods"] = detection_methods;

    // Severity
    json severity_json;
    severity_json["critical_count"] = severity.critical_count;
    severity_json["high_count"] = severity.high_count;
    severity_json["medium_count"] = severity.medium_count;
    severity_json["low_count"] = severity.low_count;
    severity_json["critical_indices"] = severity.critical_indices;
    severity_json["high_indices"] = severity.high_indices;
    severity_json["medium_indices"] = severity.medium_indices;
    severity_json["low_indices"] = severity.low_indices;
    j["severity"] = severity_json;

    // Root cause
    json root_cause_json;
    root_cause_json["contributing_features"] = root_cause.contributing_features;
    root_cause_json["feature_contributions"] = root_cause.feature_contributions;
    root_cause_json["likely_cause"] = root_cause.likely_cause;
    root_cause_json["confidence"] = root_cause.confidence;
    j["root_cause"] = root_cause_json;

    // Impact
    json impact_json;
    impact_json["on_mean"] = impact.on_mean;
    impact_json["on_variance"] = impact.on_variance;
    impact_json["on_correlation"] = impact.on_correlation;
    impact_json["on_model_performance"] = impact.on_model_performance;
    impact_json["on_statistical_tests"] = impact.on_statistical_tests;
    j["impact"] = impact_json;

    // Temporal patterns
    json temporal_json;
    temporal_json["is_clustered_in_time"] = temporal_pattern.is_clustered_in_time;
    temporal_json["temporal_autocorrelation"] = temporal_pattern.temporal_autocorrelation;
    temporal_json["temporal_clusters"] = temporal_pattern.temporal_clusters;
    temporal_json["seasonality_pattern"] = temporal_pattern.seasonality_pattern;
    j["temporal_patterns"] = temporal_json;

    // Recommendations
    json recommendations_json;
    recommendations_json["should_investigate"] = recommendations.should_investigate;
    recommendations_json["should_remove"] = recommendations.should_remove;
    recommendations_json["should_cap"] = recommendations.should_cap;
    recommendations_json["should_impute"] = recommendations.should_impute;
    recommendations_json["should_keep"] = recommendations.should_keep;
    recommendations_json["overall_strategy"] = recommendations.overall_strategy;
    j["recommendations"] = recommendations_json;

    // Multivariate analysis
    json multivariate_json;
    multivariate_json["affected_dimensions"] = multivariate.affected_dimensions;
    multivariate_json["mahalanobis_distances"] = multivariate.mahalanobis_distances;
    multivariate_json["cook_distances"] = multivariate.cook_distances;
    multivariate_json["leverage_scores"] = multivariate.leverage_scores;
    multivariate_json["influence_scores"] = multivariate.influence_scores;
    j["multivariate_analysis"] = multivariate_json;

    return j;
}

// ProfessionalDistributionAnalysis methods
std::string ProfessionalDistributionAnalysis::to_detailed_string() const {
    std::stringstream ss;
    ss << "Distribution Analysis: " << column_name << "\n";
    ss << "========================================\n";

    if (!best_fit.name.empty()) {
        ss << "Best Fit: " << best_fit.name << "\n";
        ss << "AIC: " << best_fit.aic << "\n";
        ss << "BIC: " << best_fit.bic << "\n";
        ss << "Log-Likelihood: " << best_fit.log_likelihood << "\n";
        ss << "KS Statistic: " << best_fit.ks_statistic << " (p=" << best_fit.ks_p_value << ")\n";

        ss << "Parameters:\n";
        for (const auto& [param, value] : best_fit.parameters) {
            ss << "  " << param << ": " << value << "\n";
        }
    }

    ss << "\nGoodness of Fit Tests:\n";
    ss << "  Shapiro-Wilk: " << goodness_of_fit.shapiro_wilk << " (p=" << goodness_of_fit.shapiro_wilk_p << ")\n";
    ss << "  D'Agostino K: " << goodness_of_fit.dagostino_k2 << " (p=" << goodness_of_fit.dagostino_k2_p << ")\n";
    ss << "  Jarque-Bera: " << goodness_of_fit.jarque_bera << " (p=" << goodness_of_fit.jarque_bera_p << ")\n";
    ss << "  Passes Normality: " << (goodness_of_fit.passes_normality ? "Yes" : "No") << "\n";

    ss << "\nMoments:\n";
    ss << "  Mean: " << moments.mean << "\n";
    ss << "  Variance: " << moments.variance << "\n";
    ss << "  Skewness: " << moments.skewness << "\n";
    ss << "  Kurtosis: " << moments.kurtosis << "\n";
    ss << "  Excess Kurtosis: " << moments.excess_kurtosis << "\n";

    ss << "\nL-Moments:\n";
    ss << "  L1: " << l_moments.l1 << "\n";
    ss << "  L2: " << l_moments.l2 << "\n";
    ss << "  L3: " << l_moments.l3 << "\n";
    ss << "  L4: " << l_moments.l4 << "\n";
    ss << "  L-CV: " << l_moments.l_cv << "\n";
    ss << "  L-Skewness: " << l_moments.l_skew << "\n";
    ss << "  L-Kurtosis: " << l_moments.l_kurt << "\n";

    ss << "\nTail Analysis:\n";
    ss << "  Tail Index: " << tail_analysis.tail_index << "\n";
    ss << "  Hill Estimator: " << tail_analysis.hill_estimator << "\n";
    ss << "  Extreme Value Index: " << tail_analysis.extreme_value_index << "\n";
    ss << "  Heavy Tailed: " << (tail_analysis.heavy_tailed ? "Yes" : "No") << "\n";
    ss << "  Light Tailed: " << (tail_analysis.light_tailed ? "Yes" : "No") << "\n";
    ss << "  Expected Shortfall 95%: " << tail_analysis.expected_shortfall_95 << "\n";
    ss << "  Expected Shortfall 99%: " << tail_analysis.expected_shortfall_99 << "\n";
    ss << "  VaR 95%: " << tail_analysis.value_at_risk_95 << "\n";
    ss << "  VaR 99%: " << tail_analysis.value_at_risk_99 << "\n";

    ss << "\nModality:\n";
    ss << "  Number of Modes: " << modality.number_of_modes << "\n";
    ss << "  Is Multimodal: " << (modality.is_multimodal ? "Yes" : "No") << "\n";
    ss << "  Is Bimodal: " << (modality.is_bimodal ? "Yes" : "No") << "\n";
    ss << "  Is Unimodal: " << (modality.is_unimodal ? "Yes" : "No") << "\n";
    ss << "  Dip Statistic: " << modality.dip_statistic << " (p=" << modality.dip_p_value << ")\n";

    ss << "\nDistribution Properties:\n";
    ss << "  Is Symmetric: " << (is_symmetric ? "Yes" : "No") << "\n";
    ss << "  Is Uniform: " << (is_uniform ? "Yes" : "No") << "\n";
    ss << "  Is Exponential: " << (is_exponential ? "Yes" : "No") << "\n";
    ss << "  Is Power Law: " << (is_power_law ? "Yes" : "No") << "\n";
    ss << "  Is Poisson: " << (is_poisson ? "Yes" : "No") << "\n";
    ss << "  Is Binomial: " << (is_binomial ? "Yes" : "No") << "\n";

    ss << "\nTransformation Suggestions:\n";
    ss << "  Log Recommended: " << (transformations.log_recommended ? "Yes" : "No") << "\n";
    ss << "  Box-Cox Recommended: " << (transformations.box_cox_recommended ? "Yes" : "No") << "\n";
    ss << "  Yeo-Johnson Recommended: " << (transformations.yeo_johnson_recommended ? "Yes" : "No") << "\n";
    ss << "  Sqrt Recommended: " << (transformations.sqrt_recommended ? "Yes" : "No") << "\n";
    ss << "  Reciprocal Recommended: " << (transformations.reciprocal_recommended ? "Yes" : "No") << "\n";
    ss << "  Power Recommended: " << (transformations.power_recommended ? "Yes" : "No") << "\n";
    ss << "  Best Transformation: " << transformations.best_transformation << "\n";
    if (transformations.box_cox_recommended) {
        ss << "  Best Lambda (Box-Cox): " << transformations.best_lambda << "\n";
    }

    return ss.str();
}

json ProfessionalDistributionAnalysis::to_json() const {
    json j;

    j["column_name"] = column_name;

    // Fitted distributions
    json distributions_json;
    for (const auto& dist : fitted_distributions) {
        json dist_json;
        dist_json["name"] = dist.name;
        dist_json["parameters"] = dist.parameters;
        dist_json["log_likelihood"] = dist.log_likelihood;
        dist_json["aic"] = dist.aic;
        dist_json["bic"] = dist.bic;
        dist_json["ks_statistic"] = dist.ks_statistic;
        dist_json["ks_p_value"] = dist.ks_p_value;
        dist_json["ad_statistic"] = dist.ad_statistic;
        dist_json["ad_p_value"] = dist.ad_p_value;
        dist_json["cvm_statistic"] = dist.cvm_statistic;
        dist_json["cvm_p_value"] = dist.cvm_p_value;
        dist_json["chi2_statistic"] = dist.chi2_statistic;
        dist_json["chi2_p_value"] = dist.chi2_p_value;
        distributions_json.push_back(dist_json);
    }
    j["fitted_distributions"] = distributions_json;

    // Best fit
    if (!best_fit.name.empty()) {
        json best_fit_json;
        best_fit_json["name"] = best_fit.name;
        best_fit_json["parameters"] = best_fit.parameters;
        best_fit_json["log_likelihood"] = best_fit.log_likelihood;
        best_fit_json["aic"] = best_fit.aic;
        best_fit_json["bic"] = best_fit.bic;
        best_fit_json["ks_statistic"] = best_fit.ks_statistic;
        best_fit_json["ks_p_value"] = best_fit.ks_p_value;
        j["best_fit"] = best_fit_json;
    }

    // Goodness of fit
    json gof_json;
    gof_json["shapiro_wilk"] = goodness_of_fit.shapiro_wilk;
    gof_json["shapiro_wilk_p"] = goodness_of_fit.shapiro_wilk_p;
    gof_json["dagostino_k2"] = goodness_of_fit.dagostino_k2;
    gof_json["dagostino_k2_p"] = goodness_of_fit.dagostino_k2_p;
    gof_json["jarque_bera"] = goodness_of_fit.jarque_bera;
    gof_json["jarque_bera_p"] = goodness_of_fit.jarque_bera_p;
    gof_json["passes_normality"] = goodness_of_fit.passes_normality;
    j["goodness_of_fit"] = gof_json;

    // Moments
    json moments_json;
    moments_json["mean"] = moments.mean;
    moments_json["variance"] = moments.variance;
    moments_json["skewness"] = moments.skewness;
    moments_json["kurtosis"] = moments.kurtosis;
    moments_json["excess_kurtosis"] = moments.excess_kurtosis;
    moments_json["central_moments"] = moments.central_moments;
    moments_json["standardized_moments"] = moments.standardized_moments;
    moments_json["cumulants"] = moments.cumulants;
    j["moments"] = moments_json;

    // L-moments
    json lmoments_json;
    lmoments_json["l1"] = l_moments.l1;
    lmoments_json["l2"] = l_moments.l2;
    lmoments_json["l3"] = l_moments.l3;
    lmoments_json["l4"] = l_moments.l4;
    lmoments_json["l_cv"] = l_moments.l_cv;
    lmoments_json["l_skew"] = l_moments.l_skew;
    lmoments_json["l_kurt"] = l_moments.l_kurt;
    j["l_moments"] = lmoments_json;

    // Tail analysis
    json tail_json;
    tail_json["tail_index"] = tail_analysis.tail_index;
    tail_json["hill_estimator"] = tail_analysis.hill_estimator;
    tail_json["extreme_value_index"] = tail_analysis.extreme_value_index;
    tail_json["heavy_tailed"] = tail_analysis.heavy_tailed;
    tail_json["light_tailed"] = tail_analysis.light_tailed;
    tail_json["expected_shortfall_95"] = tail_analysis.expected_shortfall_95;
    tail_json["expected_shortfall_99"] = tail_analysis.expected_shortfall_99;
    tail_json["value_at_risk_95"] = tail_analysis.value_at_risk_95;
    tail_json["value_at_risk_99"] = tail_analysis.value_at_risk_99;
    j["tail_analysis"] = tail_json;

    // Modality
    json modality_json;
    modality_json["number_of_modes"] = modality.number_of_modes;
    modality_json["mode_locations"] = modality.mode_locations;
    modality_json["mode_heights"] = modality.mode_heights;
    modality_json["dip_statistic"] = modality.dip_statistic;
    modality_json["dip_p_value"] = modality.dip_p_value;
    modality_json["is_multimodal"] = modality.is_multimodal;
    modality_json["is_bimodal"] = modality.is_bimodal;
    modality_json["is_unimodal"] = modality.is_unimodal;
    j["modality"] = modality_json;

    // Transformations
    json transform_json;
    transform_json["log_recommended"] = transformations.log_recommended;
    transform_json["box_cox_recommended"] = transformations.box_cox_recommended;
    transform_json["yeo_johnson_recommended"] = transformations.yeo_johnson_recommended;
    transform_json["sqrt_recommended"] = transformations.sqrt_recommended;
    transform_json["reciprocal_recommended"] = transformations.reciprocal_recommended;
    transform_json["power_recommended"] = transformations.power_recommended;
    transform_json["best_lambda"] = transformations.best_lambda;
    transform_json["best_transformation"] = transformations.best_transformation;
    j["transformations"] = transform_json;

    // Distribution properties
    j["properties"] = {
        {"is_symmetric", is_symmetric},
        {"is_uniform", is_uniform},
        {"is_exponential", is_exponential},
        {"is_power_law", is_power_law},
        {"is_poisson", is_poisson},
        {"is_binomial", is_binomial}
    };

    return j;
}

// ProfessionalTimeSeriesAnalysis methods
std::string ProfessionalTimeSeriesAnalysis::to_detailed_string() const {
    std::stringstream ss;
    ss << "Time Series Analysis: " << value_column << " vs " << timestamp_column << "\n";
    ss << "========================================\n";

    ss << "Stationarity Tests:\n";
    ss << "  ADF Statistic: " << stationarity.adf_statistic << " (p=" << stationarity.adf_p_value << ")\n";
    ss << "  KPSS Statistic: " << stationarity.kpss_statistic << " (p=" << stationarity.kpss_p_value << ")\n";
    ss << "  PP Statistic: " << stationarity.pp_statistic << " (p=" << stationarity.pp_p_value << ")\n";
    ss << "  Stationary (ADF): " << (stationarity.is_stationary_adf ? "Yes" : "No") << "\n";
    ss << "  Stationary (KPSS): " << (stationarity.is_stationary_kpss ? "Yes" : "No") << "\n";
    ss << "  Differencing Order: " << stationarity.differencing_order << "\n";

    ss << "\nDecomposition:\n";
    ss << "  Trend Strength: " << decomposition.trend_strength << "\n";
    ss << "  Seasonality Strength: " << decomposition.seasonality_strength << "\n";
    ss << "  Residual Strength: " << decomposition.residual_strength << "\n";
    ss << "  Seasonal Period: " << decomposition.seasonal_period << "\n";
    ss << "  Period Length: " << decomposition.seasonal_period_length << "\n";

    ss << "\nAutocorrelation:\n";
    ss << "  Autocorrelation Time: " << autocorrelation.autocorrelation_time << "\n";
    ss << "  Hurst Exponent: " << autocorrelation.hurst_exponent << "\n";
    ss << "  Lyapunov Exponent: " << autocorrelation.lyapunov_exponent << "\n";
    ss << "  Long Memory: " << (autocorrelation.is_long_memory ? "Yes" : "No") << "\n";
    ss << "  Short Memory: " << (autocorrelation.is_short_memory ? "Yes" : "No") << "\n";

    ss << "\nSpectral Analysis:\n";
    ss << "  Total Power: " << spectral.total_power << "\n";
    ss << "  Peak Frequency: " << spectral.peak_frequency << "\n";
    ss << "  Peak Power: " << spectral.peak_power << "\n";
    ss << "  Dominant Frequencies: " << spectral.dominant_frequencies.size() << "\n";

    ss << "\nVolatility:\n";
    ss << "  Unconditional Variance: " << volatility.unconditional_variance << "\n";
    ss << "  Conditional Variance: " << volatility.conditional_variance << "\n";
    ss << "  ARCH Effect: " << volatility.arch_effect << "\n";
    ss << "  GARCH Effect: " << volatility.garch_effect << "\n";
    ss << "  Volatility Clustering: " << (volatility.has_volatility_clustering ? "Yes" : "No") << "\n";

    ss << "\nForecastability:\n";
    ss << "  Entropy Rate: " << forecastability.entropy_rate << "\n";
    ss << "  Predictability: " << forecastability.predictability << "\n";
    ss << "  Sample Entropy: " << forecastability.sample_entropy << "\n";
    ss << "  Approximate Entropy: " << forecastability.approximate_entropy << "\n";
    ss << "  Is Chaotic: " << (forecastability.is_chaotic ? "Yes" : "No") << "\n";
    ss << "  Is Predictable: " << (forecastability.is_predictable ? "Yes" : "No") << "\n";
    ss << "  Forecast Horizon: " << forecastability.forecast_horizon << "\n";

    ss << "\nSeasonality:\n";
    ss << "  Has Seasonality: " << (seasonality.has_seasonality ? "Yes" : "No") << "\n";
    ss << "  Seasonal Type: " << seasonality.seasonal_type << "\n";
    ss << "  Seasonal Periods: " << seasonality.seasonal_periods.size() << "\n";
    ss << "  Seasonal Strengths: ";
    for (size_t i = 0; i < std::min(seasonality.seasonal_strengths.size(), (size_t)3); ++i) {
        if (i > 0) ss << ", ";
        ss << seasonality.seasonal_strengths[i];
    }
    if (seasonality.seasonal_strengths.size() > 3) ss << ", ...";
    ss << "\n";

    ss << "\nAnomalies:\n";
    ss << "  Point Anomalies: " << anomalies.point_anomaly_indices.size() << "\n";
    ss << "  Collective Anomalies: " << anomalies.collective_anomaly_ranges.size() << "\n";
    ss << "  Trend Change Points: " << anomalies.trend_change_points.size() << "\n";
    ss << "  Seasonal Change Points: " << anomalies.seasonal_change_points.size() << "\n";

    ss << "\nModel Suggestions:\n";
    ss << "  Best Model Type: " << model_suggestions.best_model_type << "\n";
    ss << "  ARIMA Recommended: " << (model_suggestions.arima_recommended ? "Yes" : "No") << "\n";
    ss << "  ETS Recommended: " << (model_suggestions.ets_recommended ? "Yes" : "No") << "\n";
    ss << "  Prophet Recommended: " << (model_suggestions.prophet_recommended ? "Yes" : "No") << "\n";
    ss << "  LSTM Recommended: " << (model_suggestions.lstm_recommended ? "Yes" : "No") << "\n";
    ss << "  TBATS Recommended: " << (model_suggestions.tbats_recommended ? "Yes" : "No") << "\n";

    return ss.str();
}

json ProfessionalTimeSeriesAnalysis::to_json() const {
    json j;

    j["timestamp_column"] = timestamp_column;
    j["value_column"] = value_column;

    // Stationarity
    json stationarity_json;
    stationarity_json["adf"] = {
        {"statistic", stationarity.adf_statistic},
        {"p_value", stationarity.adf_p_value},
        {"is_stationary", stationarity.is_stationary_adf}
    };
    stationarity_json["kpss"] = {
        {"statistic", stationarity.kpss_statistic},
        {"p_value", stationarity.kpss_p_value},
        {"is_stationary", stationarity.is_stationary_kpss}
    };
    stationarity_json["pp"] = {
        {"statistic", stationarity.pp_statistic},
        {"p_value", stationarity.pp_p_value},
        {"is_stationary", stationarity.is_stationary_pp}
    };
    stationarity_json["differencing_order"] = stationarity.differencing_order;
    j["stationarity"] = stationarity_json;

    // Decomposition
    json decomposition_json;
    decomposition_json["trend_strength"] = decomposition.trend_strength;
    decomposition_json["seasonality_strength"] = decomposition.seasonality_strength;
    decomposition_json["residual_strength"] = decomposition.residual_strength;
    decomposition_json["seasonal_period"] = decomposition.seasonal_period;
    decomposition_json["seasonal_period_length"] = decomposition.seasonal_period_length;
    j["decomposition"] = decomposition_json;

    // Autocorrelation
    json autocorrelation_json;
    autocorrelation_json["acf_values"] = autocorrelation.acf_values;
    autocorrelation_json["acf_confidence"] = autocorrelation.acf_confidence;
    autocorrelation_json["pacf_values"] = autocorrelation.pacf_values;
    autocorrelation_json["pacf_confidence"] = autocorrelation.pacf_confidence;
    autocorrelation_json["autocorrelation_time"] = autocorrelation.autocorrelation_time;
    autocorrelation_json["hurst_exponent"] = autocorrelation.hurst_exponent;
    autocorrelation_json["lyapunov_exponent"] = autocorrelation.lyapunov_exponent;
    autocorrelation_json["is_long_memory"] = autocorrelation.is_long_memory;
    autocorrelation_json["is_short_memory"] = autocorrelation.is_short_memory;
    j["autocorrelation"] = autocorrelation_json;

    // Spectral analysis
    json spectral_json;
    spectral_json["periodogram"] = spectral.periodogram;
    spectral_json["spectral_density"] = spectral.spectral_density;
    spectral_json["dominant_frequencies"] = spectral.dominant_frequencies;
    spectral_json["power_at_frequencies"] = spectral.power_at_frequencies;
    spectral_json["total_power"] = spectral.total_power;
    spectral_json["peak_frequency"] = spectral.peak_frequency;
    spectral_json["peak_power"] = spectral.peak_power;
    j["spectral"] = spectral_json;

    // Volatility
    json volatility_json;
    volatility_json["unconditional_variance"] = volatility.unconditional_variance;
    volatility_json["conditional_variance"] = volatility.conditional_variance;
    volatility_json["arch_effect"] = volatility.arch_effect;
    volatility_json["garch_effect"] = volatility.garch_effect;
    volatility_json["has_volatility_clustering"] = volatility.has_volatility_clustering;
    volatility_json["volatility_clusters"] = volatility.volatility_clusters;
    j["volatility"] = volatility_json;

    // Forecastability
    json forecastability_json;
    forecastability_json["entropy_rate"] = forecastability.entropy_rate;
    forecastability_json["predictability"] = forecastability.predictability;
    forecastability_json["sample_entropy"] = forecastability.sample_entropy;
    forecastability_json["approximate_entropy"] = forecastability.approximate_entropy;
    forecastability_json["is_chaotic"] = forecastability.is_chaotic;
    forecastability_json["is_predictable"] = forecastability.is_predictable;
    forecastability_json["forecast_horizon"] = forecastability.forecast_horizon;
    j["forecastability"] = forecastability_json;

    // Seasonality
    json seasonality_json;
    seasonality_json["has_seasonality"] = seasonality.has_seasonality;
    seasonality_json["seasonal_periods"] = seasonality.seasonal_periods;
    seasonality_json["seasonal_strengths"] = seasonality.seasonal_strengths;
    seasonality_json["seasonal_type"] = seasonality.seasonal_type;
    seasonality_json["seasonal_indices"] = seasonality.seasonal_indices;
    j["seasonality"] = seasonality_json;

    // Anomalies
    json anomalies_json;
    anomalies_json["point_anomaly_indices"] = anomalies.point_anomaly_indices;

    json collective_json;
    for (const auto& range : anomalies.collective_anomaly_ranges) {
        json range_json;
        range_json["start"] = range.first;
        range_json["end"] = range.second;
        collective_json.push_back(range_json);
    }
    anomalies_json["collective_anomaly_ranges"] = collective_json;

    anomalies_json["trend_change_points"] = anomalies.trend_change_points;
    anomalies_json["seasonal_change_points"] = anomalies.seasonal_change_points;
    j["anomalies"] = anomalies_json;

    // Model suggestions
    json model_suggestions_json;
    model_suggestions_json["arima_recommended"] = model_suggestions.arima_recommended;
    model_suggestions_json["ets_recommended"] = model_suggestions.ets_recommended;
    model_suggestions_json["prophet_recommended"] = model_suggestions.prophet_recommended;
    model_suggestions_json["lstm_recommended"] = model_suggestions.lstm_recommended;
    model_suggestions_json["tbats_recommended"] = model_suggestions.tbats_recommended;
    model_suggestions_json["best_model_type"] = model_suggestions.best_model_type;
    model_suggestions_json["model_parameters"] = model_suggestions.model_parameters;
    j["model_suggestions"] = model_suggestions_json;

    return j;
}

// ProfessionalDataQualityReport methods
std::string ProfessionalDataQualityReport::to_detailed_string() const {
    std::stringstream ss;
    ss << "Data Quality Report\n";
    ss << "========================================\n";

    ss << "\nQuality Scores:\n";
    ss << "  Completeness: " << (scores.completeness * 100) << "%\n";
    ss << "  Consistency: " << (scores.consistency * 100) << "%\n";
    ss << "  Accuracy: " << (scores.accuracy * 100) << "%\n";
    ss << "  Timeliness: " << (scores.timeliness * 100) << "%\n";
    ss << "  Validity: " << (scores.validity * 100) << "%\n";
    ss << "  Uniqueness: " << (scores.uniqueness * 100) << "%\n";
    ss << "  Integrity: " << (scores.integrity * 100) << "%\n";
    ss << "  Freshness: " << (scores.freshness * 100) << "%\n";
    ss << "  Lineage: " << (scores.lineage * 100) << "%\n";
    ss << "  Overall: " << (scores.overall * 100) << "%\n";

    ss << "\nCompleteness:\n";
    ss << "  Total Cells: " << completeness.total_cells << "\n";
    ss << "  Missing Cells: " << completeness.missing_cells << "\n";
    ss << "  Missing Percentage: " << completeness.missing_percentage << "%\n";
    ss << "  Missing Pattern: " << completeness.missing_pattern << "\n";
    ss << "  Row Completeness: " << completeness.row_completeness << "%\n";
    ss << "  Columns with Missing: " << completeness.columns_with_missing.size() << "\n";

    ss << "\nConsistency:\n";
    ss << "  Type Inconsistencies: " << consistency.type_inconsistencies.size() << "\n";
    ss << "  Range Violations: " << consistency.range_violations.size() << "\n";
    ss << "  Format Violations: " << consistency.format_violations.size() << "\n";
    ss << "  Constraint Violations: " << consistency.constraint_violations.size() << "\n";
    ss << "  Referential Integrity Violations: " << consistency.referential_integrity_violations.size() << "\n";
    ss << "  Total Violations: " << consistency.total_violations << "\n";

    ss << "\nAccuracy:\n";
    ss << "  Ground Truth Accuracy: " << (accuracy.ground_truth_accuracy * 100) << "%\n";
    ss << "  Cross-source Agreement: " << (accuracy.cross_source_agreement * 100) << "%\n";
    ss << "  Self-consistency: " << (accuracy.self_consistency * 100) << "%\n";
    ss << "  Accuracy Issues: " << accuracy.accuracy_issues.size() << "\n";

    ss << "\nUniqueness:\n";
    ss << "  Total Rows: " << uniqueness.total_rows << "\n";
    ss << "  Duplicate Rows: " << uniqueness.duplicate_rows << "\n";
    ss << "  Duplicate Percentage: " << uniqueness.duplicate_percentage << "%\n";
    ss << "  Duplicate Groups: " << uniqueness.duplicate_groups.size() << "\n";
    ss << "  Has Primary Key: " << (uniqueness.has_primary_key ? "Yes" : "No") << "\n";
    ss << "  Candidate Keys: " << uniqueness.candidate_keys.size() << "\n";

    ss << "\nValidity:\n";
    ss << "  Business Rule Violations: " << validity.business_rule_violations.size() << "\n";
    ss << "  Domain Violations: " << validity.domain_violations.size() << "\n";
    ss << "  Pattern Violations: " << validity.pattern_violations.size() << "\n";
    ss << "  Total Invalid Values: " << validity.total_invalid_values << "\n";
    ss << "  Validity Rate: " << (validity.validity_rate * 100) << "%\n";

    ss << "\nTimeliness:\n";
    ss << "  Data Freshness: " << timeliness.data_freshness << " hours\n";
    ss << "  Update Frequency: " << timeliness.update_frequency << " updates/day\n";
    ss << "  Latency: " << timeliness.latency << " hours\n";
    ss << "  Meets SLA: " << (timeliness.meets_sla ? "Yes" : "No") << "\n";

    ss << "\nIssues:\n";
    ss << "  Critical: " << issues.critical.size() << "\n";
    ss << "  High: " << issues.high.size() << "\n";
    ss << "  Medium: " << issues.medium.size() << "\n";
    ss << "  Low: " << issues.low.size() << "\n";
    ss << "  Total: " << issues.total_issues << "\n";

    ss << "\nRecommendations:\n";
    ss << "  Immediate: " << recommendations.immediate.size() << "\n";
    for (const auto& rec : recommendations.immediate) {
        ss << "    - " << rec << "\n";
    }
    ss << "  Short-term: " << recommendations.short_term.size() << "\n";
    ss << "  Long-term: " << recommendations.long_term.size() << "\n";
    ss << "  Monitoring: " << recommendations.monitoring.size() << "\n";

    ss << "\nMetadata:\n";
    ss << "  Analysis Timestamp: " << metadata.analysis_timestamp << "\n";
    ss << "  Row Count: " << metadata.row_count << "\n";
    ss << "  Column Count: " << metadata.column_count << "\n";
    ss << "  Total Cells: " << metadata.total_cells << "\n";
    ss << "  Data Source: " << metadata.data_source << "\n";
    ss << "  Schema Version: " << metadata.schema_version << "\n";

    return ss.str();
}

json ProfessionalDataQualityReport::to_json() const {
    json j;

    // Scores
    json scores_json;
    scores_json["completeness"] = scores.completeness;
    scores_json["consistency"] = scores.consistency;
    scores_json["accuracy"] = scores.accuracy;
    scores_json["timeliness"] = scores.timeliness;
    scores_json["validity"] = scores.validity;
    scores_json["uniqueness"] = scores.uniqueness;
    scores_json["integrity"] = scores.integrity;
    scores_json["freshness"] = scores.freshness;
    scores_json["lineage"] = scores.lineage;
    scores_json["overall"] = scores.overall;

    json weighted_json;
    weighted_json["completeness"] = scores.weighted.completeness;
    weighted_json["consistency"] = scores.weighted.consistency;
    weighted_json["accuracy"] = scores.weighted.accuracy;
    weighted_json["timeliness"] = scores.weighted.timeliness;
    weighted_json["validity"] = scores.weighted.validity;
    weighted_json["uniqueness"] = scores.weighted.uniqueness;
    weighted_json["overall"] = scores.weighted.overall;
    scores_json["weighted"] = weighted_json;

    j["scores"] = scores_json;

    // Completeness
    json completeness_json;
    completeness_json["total_cells"] = completeness.total_cells;
    completeness_json["missing_cells"] = completeness.missing_cells;
    completeness_json["missing_percentage"] = completeness.missing_percentage;
    completeness_json["columns_with_missing"] = completeness.columns_with_missing;

    json column_missing_json;
    for (const auto& [col, pct] : completeness.column_missing_percentages) {
        column_missing_json[col] = pct;
    }
    completeness_json["column_missing_percentages"] = column_missing_json;

    completeness_json["missing_pattern"] = completeness.missing_pattern;
    completeness_json["rows_with_missing"] = completeness.rows_with_missing;
    completeness_json["row_completeness"] = completeness.row_completeness;
    j["completeness"] = completeness_json;

    // Consistency
    json consistency_json;
    consistency_json["type_inconsistencies"] = consistency.type_inconsistencies;
    consistency_json["range_violations"] = consistency.range_violations;
    consistency_json["format_violations"] = consistency.format_violations;
    consistency_json["constraint_violations"] = consistency.constraint_violations;
    consistency_json["referential_integrity_violations"] = consistency.referential_integrity_violations;
    consistency_json["total_violations"] = consistency.total_violations;
    j["consistency"] = consistency_json;

    // Accuracy
    json accuracy_json;
    accuracy_json["ground_truth_accuracy"] = accuracy.ground_truth_accuracy;
    accuracy_json["cross_source_agreement"] = accuracy.cross_source_agreement;
    accuracy_json["self_consistency"] = accuracy.self_consistency;
    accuracy_json["accuracy_issues"] = accuracy.accuracy_issues;

    json column_accuracy_json;
    for (const auto& [col, score] : accuracy.column_accuracy_scores) {
        column_accuracy_json[col] = score;
    }
    accuracy_json["column_accuracy_scores"] = column_accuracy_json;
    j["accuracy"] = accuracy_json;

    // Uniqueness
    json uniqueness_json;
    uniqueness_json["total_rows"] = uniqueness.total_rows;
    uniqueness_json["duplicate_rows"] = uniqueness.duplicate_rows;
    uniqueness_json["duplicate_percentage"] = uniqueness.duplicate_percentage;
    uniqueness_json["candidate_keys"] = uniqueness.candidate_keys;
    uniqueness_json["has_primary_key"] = uniqueness.has_primary_key;

    json duplicate_groups_json;
    for (const auto& group : uniqueness.duplicate_groups) {
        duplicate_groups_json.push_back(group);
    }
    uniqueness_json["duplicate_groups"] = duplicate_groups_json;
    j["uniqueness"] = uniqueness_json;

    // Validity
    json validity_json;
    validity_json["business_rule_violations"] = validity.business_rule_violations;
    validity_json["domain_violations"] = validity.domain_violations;
    validity_json["pattern_violations"] = validity.pattern_violations;
    validity_json["total_invalid_values"] = validity.total_invalid_values;
    validity_json["validity_rate"] = validity.validity_rate;
    j["validity"] = validity_json;

    // Timeliness
    json timeliness_json;
    timeliness_json["data_freshness"] = timeliness.data_freshness;
    timeliness_json["update_frequency"] = timeliness.update_frequency;
    timeliness_json["latency"] = timeliness.latency;
    timeliness_json["meets_sla"] = timeliness.meets_sla;
    j["timeliness"] = timeliness_json;

    // Issues
    json issues_json;
    issues_json["critical"] = issues.critical;
    issues_json["high"] = issues.high;
    issues_json["medium"] = issues.medium;
    issues_json["low"] = issues.low;
    issues_json["total_issues"] = issues.total_issues;
    j["issues"] = issues_json;

    // Recommendations
    json recommendations_json;
    recommendations_json["immediate"] = recommendations.immediate;
    recommendations_json["short_term"] = recommendations.short_term;
    recommendations_json["long_term"] = recommendations.long_term;
    recommendations_json["monitoring"] = recommendations.monitoring;
    j["recommendations"] = recommendations_json;

    // Metadata
    json metadata_json;
    metadata_json["analysis_timestamp"] = metadata.analysis_timestamp;
    metadata_json["row_count"] = metadata.row_count;
    metadata_json["column_count"] = metadata.column_count;
    metadata_json["total_cells"] = metadata.total_cells;
    metadata_json["data_source"] = metadata.data_source;
    metadata_json["schema_version"] = metadata.schema_version;
    j["metadata"] = metadata_json;

    return j;
}

std::string ProfessionalDataQualityReport::to_markdown_report() const {
    std::stringstream ss;

    ss << "# Data Quality Report\n\n";

    // Scorecard
    ss << "## Quality Scorecard\n\n";
    ss << "| Dimension | Score | Status |\n";
    ss << "|-----------|-------|--------|\n";

    auto add_score_row = [&](const std::string& dimension, double score) {
        std::string status;
        if (score >= 0.9) status = " Excellent";
        else if (score >= 0.7) status = " Good";
        else if (score >= 0.5) status = " Fair";
        else status = " Poor";

        ss << "| " << dimension << " | " << std::fixed << std::setprecision(1)
           << (score * 100) << "% | " << status << " |\n";
    };

    add_score_row("Completeness", scores.completeness);
    add_score_row("Consistency", scores.consistency);
    add_score_row("Accuracy", scores.accuracy);
    add_score_row("Timeliness", scores.timeliness);
    add_score_row("Validity", scores.validity);
    add_score_row("Uniqueness", scores.uniqueness);
    ss << "| **Overall** | **" << std::fixed << std::setprecision(1)
       << (scores.overall * 100) << "%** | **"
       << (scores.overall >= 0.7 ? " PASS" : " FAIL") << "** |\n";

    // Key findings
    ss << "\n## Key Findings\n\n";

    if (completeness.missing_percentage > 5.0) {
        ss << " **Missing Data**: " << completeness.missing_percentage
           << "% of values are missing\n";
    }

    if (uniqueness.duplicate_percentage > 1.0) {
        ss << " **Duplicate Data**: " << uniqueness.duplicate_percentage
           << "% of rows are duplicates\n";
    }

    if (validity.validity_rate < 0.95) {
        ss << " **Invalid Data**: " << (100 - validity.validity_rate * 100)
           << "% of values violate business rules\n";
    }

    if (consistency.total_violations > 0) {
        ss << " **Inconsistencies**: " << consistency.total_violations
           << " consistency violations found\n";
    }

    // Recommendations
    ss << "\n## Recommendations\n\n";

    if (!recommendations.immediate.empty()) {
        ss << "### Immediate Actions (Next 24 hours)\n\n";
        for (const auto& rec : recommendations.immediate) {
            ss << "- " << rec << "\n";
        }
        ss << "\n";
    }

    if (!recommendations.short_term.empty()) {
        ss << "### Short-term Actions (Next week)\n\n";
        for (const auto& rec : recommendations.short_term) {
            ss << "- " << rec << "\n";
        }
        ss << "\n";
    }

    if (!recommendations.long_term.empty()) {
        ss << "### Long-term Actions (Next quarter)\n\n";
        for (const auto& rec : recommendations.long_term) {
            ss << "- " << rec << "\n";
        }
        ss << "\n";
    }

    // Metadata
    ss << "## Report Metadata\n\n";
    ss << "- **Analysis Timestamp**: " << metadata.analysis_timestamp << "\n";
    ss << "- **Data Source**: " << metadata.data_source << "\n";
    ss << "- **Row Count**: " << metadata.row_count << "\n";
    ss << "- **Column Count**: " << metadata.column_count << "\n";
    ss << "- **Total Cells**: " << metadata.total_cells << "\n";
    ss << "- **Schema Version**: " << metadata.schema_version << "\n";

    return ss.str();
}

std::string ProfessionalDataQualityReport::to_html_report() const {
    std::stringstream ss;

    ss << R"(<!DOCTYPE html>
<html>
<head>
    <title>Data Quality Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .scorecard { border-collapse: collapse; width: 100%; margin-bottom: 30px; }
        .scorecard th, .scorecard td { border: 1px solid #ddd; padding: 12px; text-align: left; }
        .scorecard th { background-color: #f4f4f4; }
        .score-excellent { background-color: #d4edda; }
        .score-good { background-color: #fff3cd; }
        .score-fair { background-color: #f8d7da; }
        .score-poor { background-color: #dc3545; color: white; }
        .section { margin-bottom: 30px; }
        .issue { padding: 10px; margin: 5px 0; border-left: 4px solid #dc3545; background-color: #f8f9fa; }
        .recommendation { padding: 10px; margin: 5px 0; border-left: 4px solid #28a745; background-color: #f8f9fa; }
        .metric { display: inline-block; margin-right: 20px; }
        .metric-value { font-size: 24px; font-weight: bold; }
        .metric-label { font-size: 12px; color: #666; }
    </style>
</head>
<body>
    <h1>Data Quality Report</h1>
    <div class="section">
        <h2>Quality Scorecard</h2>
        <table class="scorecard">
            <tr>
                <th>Dimension</th>
                <th>Score</th>
                <th>Status</th>
            </tr>)";

    auto add_score_row_html = [&](const std::string& dimension, double score) {
        std::string status_class;
        std::string status_text;

        if (score >= 0.9) {
            status_class = "score-excellent";
            status_text = "Excellent";
        } else if (score >= 0.7) {
            status_class = "score-good";
            status_text = "Good";
        } else if (score >= 0.5) {
            status_class = "score-fair";
            status_text = "Fair";
        } else {
            status_class = "score-poor";
            status_text = "Poor";
        }

        ss << "<tr><td>" << dimension << "</td><td class='" << status_class << "'>"
           << std::fixed << std::setprecision(1) << (score * 100)
           << "%</td><td>" << status_text << "</td></tr>\n";
    };

    add_score_row_html("Completeness", scores.completeness);
    add_score_row_html("Consistency", scores.consistency);
    add_score_row_html("Accuracy", scores.accuracy);
    add_score_row_html("Timeliness", scores.timeliness);
    add_score_row_html("Validity", scores.validity);
    add_score_row_html("Uniqueness", scores.uniqueness);

    ss << R"(        <tr>
                <td><strong>Overall</strong></td>
                <td><strong>)" << std::fixed << std::setprecision(1)
       << (scores.overall * 100) << R"(%</strong></td>
                <td><strong>)" << (scores.overall >= 0.7 ? " PASS" : " FAIL")
       << R"(</strong></td>
            </tr>
        </table>
    </div>

    <div class="section">
        <h2>Key Metrics</h2>
        <div class="metric">
            <div class="metric-value">)" << std::fixed << std::setprecision(1)
       << completeness.missing_percentage << R"(%</div>
            <div class="metric-label">Missing Data</div>
        </div>
        <div class="metric">
            <div class="metric-value">)" << std::fixed << std::setprecision(1)
       << uniqueness.duplicate_percentage << R"(%</div>
            <div class="metric-label">Duplicate Rows</div>
        </div>
        <div class="metric">
            <div class="metric-value">)" << consistency.total_violations
       << R"(</div>
            <div class="metric-label">Consistency Issues</div>
        </div>
    </div>

    <div class="section">
        <h2>Critical Issues</h2>)";

    for (const auto& issue : issues.critical) {
        ss << "<div class='issue'><strong>CRITICAL:</strong> " << issue << "</div>\n";
    }

    for (const auto& issue : issues.high) {
        ss << "<div class='issue'><strong>HIGH:</strong> " << issue << "</div>\n";
    }

    ss << R"(    </div>

    <div class="section">
        <h2>Recommendations</h2>
        <h3>Immediate Actions</h3>)";

    for (const auto& rec : recommendations.immediate) {
        ss << "<div class='recommendation'>" << rec << "</div>\n";
    }

    ss << R"(        <h3>Short-term Actions</h3>)";

    for (const auto& rec : recommendations.short_term) {
        ss << "<div class='recommendation'>" << rec << "</div>\n";
    }

    ss << R"(    </div>

    <div class="section">
        <h2>Report Metadata</h2>
        <p><strong>Analysis Timestamp:</strong> )" << metadata.analysis_timestamp
       << R"(</p>
        <p><strong>Data Source:</strong> )" << metadata.data_source
       << R"(</p>
        <p><strong>Row Count:</strong> )" << metadata.row_count
       << R"(</p>
        <p><strong>Column Count:</strong> )" << metadata.column_count
       << R"(</p>
        <p><strong>Total Cells:</strong> )" << metadata.total_cells
       << R"(</p>
        <p><strong>Schema Version:</strong> )" << metadata.schema_version
       << R"(</p>
    </div>
</body>
</html>)";

    return ss.str();
}

// ProfessionalComprehensiveAnalysisReport methods
json ProfessionalComprehensiveAnalysisReport::to_json(bool detailed) const {
    json j;

    // Metadata
    j["metadata"] = {
        {"analysis_id", analysis_id},
        {"table_name", table_name},
        {"analysis_timestamp", analysis_timestamp},
        {"analysis_duration_ms", analysis_duration.count()},
        {"row_count", row_count},
        {"column_count", column_count}
    };

    // Column analyses
    json column_analyses_json;
    for (const auto& [col_name, analysis] : column_analyses) {
        column_analyses_json[col_name] = analysis.to_json(detailed);
    }
    j["column_analyses"] = column_analyses_json;

    // Correlations
    json correlations_json;
    for (const auto& corr : correlations) {
        correlations_json.push_back(corr.to_json());
    }
    j["correlations"] = correlations_json;

    // Feature importance
    json feature_importance_json;
    for (const auto& importance : feature_importance) {
        feature_importance_json.push_back(importance.to_json());
    }
    j["feature_importance"] = feature_importance_json;

    // Clusters
    json clusters_json;
    for (const auto& cluster : clusters) {
        clusters_json.push_back(cluster.to_json());
    }
    j["clusters"] = clusters_json;

    // Outliers
    json outliers_json;
    for (const auto& outlier : outliers) {
        outliers_json.push_back(outlier.to_json());
    }
    j["outliers"] = outliers_json;

    // Distributions
    json distributions_json;
    for (const auto& dist : distributions) {
        distributions_json.push_back(dist.to_json());
    }
    j["distributions"] = distributions_json;

    // Time series analyses
    json time_series_json;
    for (const auto& ts : time_series_analyses) {
        time_series_json.push_back(ts.to_json());
    }
    j["time_series_analyses"] = time_series_json;

    // Data quality
    j["data_quality"] = data_quality.to_json();

    // Advanced analyses
    if (detailed) {
        json advanced_json;

        // PCA
        advanced_json["principal_components"] = advanced.principal_components;
        advanced_json["explained_variance_ratio"] = advanced.explained_variance_ratio;
        advanced_json["total_variance_explained"] = advanced.total_variance_explained;

        // Association rules
        json association_rules_json;
        for (const auto& rule : advanced.association_rules) {
            json rule_json;
            rule_json["antecedent"] = rule.antecedent;
            rule_json["consequent"] = rule.consequent;
            rule_json["support"] = rule.support;
            rule_json["confidence"] = rule.confidence;
            rule_json["lift"] = rule.lift;
            rule_json["conviction"] = rule.conviction;
            association_rules_json.push_back(rule_json);
        }
        advanced_json["association_rules"] = association_rules_json;

        // Causal relationships
        json causal_json;
        for (const auto& rel : advanced.causal_relationships) {
            json rel_json;
            rel_json["cause"] = rel.cause;
            rel_json["effect"] = rel.effect;
            rel_json["treatment_effect"] = rel.treatment_effect;
            rel_json["confidence_interval_lower"] = rel.confidence_interval_lower;
            rel_json["confidence_interval_upper"] = rel.confidence_interval_upper;
            rel_json["p_value"] = rel.p_value;
            causal_json.push_back(rel_json);
        }
        advanced_json["causal_relationships"] = causal_json;

        j["advanced_analyses"] = advanced_json;
    }

    // Insights
    json insights_json;
    insights_json["data_quality"] = insights.data_quality;
    insights_json["statistical"] = insights.statistical;
    insights_json["business"] = insights.business;
    insights_json["predictive"] = insights.predictive;
    insights_json["anomaly"] = insights.anomaly;
    insights_json["optimization"] = insights.optimization;
    j["insights"] = insights_json;

    // Recommendations
    json recommendations_json;

    auto add_recommendations = [](json& target, const std::vector<Recommendations::Recommendation>& recs) {
        json recs_json;
        for (const auto& rec : recs) {
            json rec_json;
            rec_json["title"] = rec.title;
            rec_json["description"] = rec.description;
            rec_json["category"] = rec.category;
            rec_json["priority"] = rec.priority;
            rec_json["expected_impact"] = rec.expected_impact;
            rec_json["implementation_effort"] = rec.implementation_effort;
            rec_json["steps"] = rec.steps;
            rec_json["dependencies"] = rec.dependencies;
            recs_json.push_back(rec_json);
        }
        target = recs_json;
    };

    add_recommendations(recommendations_json["all"], recommendations.all);
    add_recommendations(recommendations_json["critical"], recommendations.critical);
    add_recommendations(recommendations_json["high"], recommendations.high);
    add_recommendations(recommendations_json["medium"], recommendations.medium);
    add_recommendations(recommendations_json["low"], recommendations.low);

    j["recommendations"] = recommendations_json;

    // Performance
    json performance_json;
    performance_json["memory_usage_mb"] = performance.memory_usage_mb;
    performance_json["cpu_usage_percent"] = performance.cpu_usage_percent;
    performance_json["processing_time_ms"] = performance.processing_time.count();
    performance_json["within_sla"] = performance.within_sla;

    json component_times_json;
    for (const auto& [component, time] : performance.component_times) {
        component_times_json[component] = time.count();
    }
    performance_json["component_times"] = component_times_json;

    j["performance"] = performance_json;

    return j;
}

std::string ProfessionalComprehensiveAnalysisReport::to_markdown_report(bool detailed) const {
    std::stringstream ss;

    ss << "# Comprehensive Data Analysis Report\n\n";
    ss << "**Table:** " << table_name << "  \n";
    ss << "**Rows:** " << row_count << "  \n";
    ss << "**Columns:** " << column_count << "  \n";
    ss << "**Analysis ID:** " << analysis_id << "  \n";
    ss << "**Timestamp:** " << analysis_timestamp << "  \n";
    ss << "**Duration:** " << analysis_duration.count() << "ms  \n\n";

    // Executive Summary
    ss << "## Executive Summary\n\n";

    // Data Quality Summary
    double overall_quality = data_quality.scores.overall * 100;
    ss << "### Data Quality: " << std::fixed << std::setprecision(1) << overall_quality << "%\n\n";

    if (overall_quality >= 90.0) {
        ss << " **Excellent data quality.** Data is clean, complete, and ready for analysis.\n\n";
    } else if (overall_quality >= 70.0) {
        ss << " **Good data quality with some issues.** Minor cleaning required.\n\n";
    } else if (overall_quality >= 50.0) {
        ss << " **Fair data quality.** Significant cleaning required before analysis.\n\n";
    } else {
        ss << " **Poor data quality.** Extensive data cleaning and validation required.\n\n";
    }

    // Key Metrics
    ss << "### Key Metrics\n\n";
    ss << "| Metric | Value |\n";
    ss << "|--------|-------|\n";
    ss << "| Missing Data | " << std::fixed << std::setprecision(1)
       << data_quality.completeness.missing_percentage << "% |\n";
    ss << "| Duplicate Rows | " << std::fixed << std::setprecision(1)
       << data_quality.uniqueness.duplicate_percentage << "% |\n";
    ss << "| Data Completeness | " << std::fixed << std::setprecision(1)
       << (data_quality.scores.completeness * 100) << "% |\n";
    ss << "| Data Consistency | " << std::fixed << std::setprecision(1)
       << (data_quality.scores.consistency * 100) << "% |\n";

    if (!clusters.empty()) {
        ss << "| Clusters Found | " << clusters.size() << " |\n";
    }

    if (!outliers.empty() && !outliers[0].outlier_indices.empty()) {
        ss << "| Outliers Detected | " << outliers[0].outlier_indices.size()
           << " |\n";
    }

    // Top Insights
    ss << "\n## Top Insights\n\n";

    // Data Quality Insights
    if (!insights.data_quality.empty()) {
        ss << "### Data Quality Insights\n\n";
        for (size_t i = 0; i < std::min(insights.data_quality.size(), (size_t)3); ++i) {
            ss << i+1 << ". " << insights.data_quality[i] << "\n";
        }
        ss << "\n";
    }

    // Statistical Insights
    if (!insights.statistical.empty()) {
        ss << "### Statistical Insights\n\n";
        for (size_t i = 0; i < std::min(insights.statistical.size(), (size_t)3); ++i) {
            ss << i+1 << ". " << insights.statistical[i] << "\n";
        }
        ss << "\n";
    }

    // Business Insights
    if (!insights.business.empty()) {
        ss << "### Business Insights\n\n";
        for (size_t i = 0; i < std::min(insights.business.size(), (size_t)3); ++i) {
            ss << i+1 << ". " << insights.business[i] << "\n";
        }
        ss << "\n";
    }

    // Critical Recommendations
    if (!recommendations.critical.empty()) {
        ss << "## Critical Recommendations\n\n";
        ss << "These issues should be addressed immediately:\n\n";

        for (const auto& rec : recommendations.critical) {
            ss << "### " << rec.title << "\n\n";
            ss << rec.description << "\n\n";
            ss << "**Priority:** " << rec.priority << "  \n";
            ss << "**Expected Impact:** " << std::fixed << std::setprecision(1)
               << (rec.expected_impact * 100) << "%  \n";
            ss << "**Implementation Effort:** " << std::fixed << std::setprecision(1)
               << (rec.implementation_effort * 100) << "%  \n\n";

            if (!rec.steps.empty()) {
                ss << "**Steps:**\n";
                for (const auto& step : rec.steps) {
                    ss << "1. " << step << "\n";
                }
                ss << "\n";
            }
        }
    }

    // High Priority Recommendations
    if (!recommendations.high.empty()) {
        ss << "## High Priority Recommendations\n\n";

        for (const auto& rec : recommendations.high) {
            ss << "- **" << rec.title << "**: " << rec.description << "\n";
        }
        ss << "\n";
    }

    // Performance Summary
    ss << "## Performance Summary\n\n";
    ss << "| Metric | Value |\n";
    ss << "|--------|-------|\n";
    ss << "| Total Processing Time | " << performance.processing_time.count() << "ms |\n";
    ss << "| Memory Usage | " << performance.memory_usage_mb << " MB |\n";
    ss << "| CPU Usage | " << performance.cpu_usage_percent << "% |\n";
    ss << "| Within SLA | " << (performance.within_sla ? " Yes" : " No") << " |\n";

    // Detailed Analysis (if requested)
    if (detailed) {
        ss << "\n## Detailed Analysis\n\n";

        // Column Statistics Summary
        ss << "### Column Statistics\n\n";
        ss << "| Column | Type | Nulls | Distinct | Mean | Std Dev |\n";
        ss << "|--------|------|-------|----------|------|---------|\n";

        for (const auto& [col_name, analysis] : column_analyses) {
            if (analysis.detected_type == "numeric" || analysis.detected_type == "integer" ||
                analysis.detected_type == "float" || analysis.detected_type == "double") {
                ss << "| " << col_name << " | " << analysis.detected_type << " | "
                   << analysis.null_count << " | " << analysis.distinct_count << " | "
                   << std::fixed << std::setprecision(2) << analysis.mean << " | "
                   << std::fixed << std::setprecision(2) << analysis.std_dev << " |\n";
            } else {
                ss << "| " << col_name << " | " << analysis.detected_type << " | "
                   << analysis.null_count << " | " << analysis.distinct_count << " | "
                   << "N/A | N/A |\n";
            }
        }

        // Top Correlations
        if (!correlations.empty()) {
            ss << "\n### Top Correlations\n\n";
            ss << "| Feature 1 | Feature 2 | Pearson's r | Strength |\n";
            ss << "|-----------|-----------|-------------|----------|\n";

            // Sort by absolute correlation strength
            std::vector<ProfessionalCorrelationAnalysis> sorted_corrs = correlations;
            std::sort(sorted_corrs.begin(), sorted_corrs.end(),
                     [](const auto& a, const auto& b) {
                         return std::abs(a.pearson_r) > std::abs(b.pearson_r);
                     });

            size_t count = 0;
            for (const auto& corr : sorted_corrs) {
                if (std::abs(corr.pearson_r) > 0.3 && corr.is_statistically_significant) {
                    ss << "| " << corr.column1 << " | " << corr.column2 << " | "
                       << std::fixed << std::setprecision(3) << corr.pearson_r << " | "
                       << corr.relationship_strength << " |\n";
                    count++;
                    if (count >= 10) break; // Show top 10
                }
            }

            if (count == 0) {
                ss << "*No strong correlations found*\n";
            }
        }
    }

    // Next Steps
    ss << "\n## Next Steps\n\n";
    ss << "1. Review critical recommendations and address urgent issues\n";
    ss << "2. Implement high-priority improvements\n";
    ss << "3. Schedule medium and low-priority enhancements\n";
    ss << "4. Monitor data quality metrics regularly\n";
    ss << "5. Re-run analysis after implementing changes\n";

    return ss.str();
}

std::string ProfessionalComprehensiveAnalysisReport::to_html_report(bool detailed) const {
    std::stringstream ss;

    ss << R"(<!DOCTYPE html>
<html>
<head>
    <title>Comprehensive Data Analysis Report - )" << table_name << R"(</title>
    <style>
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 40px; background-color: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background-color: white; padding: 30px; border-radius: 10px; box-shadow: 0 0 20px rgba(0,0,0,0.1); }
        .header { border-bottom: 2px solid #007bff; padding-bottom: 20px; margin-bottom: 30px; }
        .metric-card { background-color: #f8f9fa; border-radius: 8px; padding: 20px; margin-bottom: 20px; border-left: 4px solid #007bff; }
        .metric-value { font-size: 32px; font-weight: bold; color: #007bff; }
        .metric-label { font-size: 14px; color: #6c757d; text-transform: uppercase; letter-spacing: 1px; }
        .insight-card { background-color: #fff3cd; border-radius: 8px; padding: 15px; margin-bottom: 15px; border-left: 4px solid #ffc107; }
        .recommendation-card { background-color: #d4edda; border-radius: 8px; padding: 15px; margin-bottom: 15px; border-left: 4px solid #28a745; }
        .critical-card { background-color: #f8d7da; border-radius: 8px; padding: 15px; margin-bottom: 15px; border-left: 4px solid #dc3545; }
        table { width: 100%; border-collapse: collapse; margin-bottom: 30px; }
        th { background-color: #007bff; color: white; padding: 12px; text-align: left; }
        td { padding: 12px; border-bottom: 1px solid #ddd; }
        tr:hover { background-color: #f5f5f5; }
        .score-excellent { color: #28a745; font-weight: bold; }
        .score-good { color: #ffc107; font-weight: bold; }
        .score-fair { color: #fd7e14; font-weight: bold; }
        .score-poor { color: #dc3545; font-weight: bold; }
        .section { margin-bottom: 40px; }
        .section-title { color: #495057; border-bottom: 2px solid #dee2e6; padding-bottom: 10px; margin-bottom: 20px; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 style="color: #007bff;">Comprehensive Data Analysis Report</h1>
            <h2 style="color: #6c757d;">)" << table_name << R"(</h2>
            <p><strong>Analysis ID:</strong> )" << analysis_id << R"(</p>
            <p><strong>Timestamp:</strong> )" << analysis_timestamp << R"(</p>
            <p><strong>Duration:</strong> )" << analysis_duration.count() << R"(ms</p>
        </div>

        <div class="section">
            <h3 class="section-title">Executive Summary</h3>

            <div class="metric-card">
                <div class="metric-value">)" << std::fixed << std::setprecision(1)
       << (data_quality.scores.overall * 100) << R"(%</div>
                <div class="metric-label">Overall Data Quality Score</div>
            </div>

            <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin-bottom: 30px;">
                <div class="metric-card">
                    <div class="metric-value">)" << std::fixed << std::setprecision(1)
       << data_quality.completeness.missing_percentage << R"(%</div>
                    <div class="metric-label">Missing Data</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">)" << std::fixed << std::setprecision(1)
       << data_quality.uniqueness.duplicate_percentage << R"(%</div>
                    <div class="metric-label">Duplicate Rows</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">)" << column_analyses.size() << R"(</div>
                    <div class="metric-label">Columns Analyzed</div>
                </div>
            </div>
        </div>

        <div class="section">
            <h3 class="section-title">Top Insights</h3>)";

    // Add insights
    size_t insight_count = 0;
    for (const auto& insight : insights.data_quality) {
        if (insight_count >= 5) break;
        ss << "<div class='insight-card'><strong>Data Quality:</strong> " << insight << "</div>\n";
        insight_count++;
    }

    for (const auto& insight : insights.statistical) {
        if (insight_count >= 5) break;
        ss << "<div class='insight-card'><strong>Statistical:</strong> " << insight << "</div>\n";
        insight_count++;
    }

    for (const auto& insight : insights.business) {
        if (insight_count >= 5) break;
        ss << "<div class='insight-card'><strong>Business:</strong> " << insight << "</div>\n";
        insight_count++;
    }

    ss << R"(        </div>

        <div class="section">
            <h3 class="section-title">Critical Recommendations</h3>)";

    for (const auto& rec : recommendations.critical) {
        ss << "<div class='critical-card'>\n";
        ss << "<h4>" << rec.title << "</h4>\n";
        ss << "<p>" << rec.description << "</p>\n";
        ss << "<p><strong>Priority:</strong> " << rec.priority << "</p>\n";
        ss << "<p><strong>Expected Impact:</strong> " << std::fixed << std::setprecision(1)
           << (rec.expected_impact * 100) << "%</p>\n";
        ss << "</div>\n";
    }

    ss << R"(        </div>

        <div class="section">
            <h3 class="section-title">Data Quality Overview</h3>
            <table>
                <tr>
                    <th>Dimension</th>
                    <th>Score</th>
                    <th>Status</th>
                </tr>
                <tr>
                    <td>Completeness</td>
                    <td class=')" << (data_quality.scores.completeness >= 0.9 ? "score-excellent" :
                                     data_quality.scores.completeness >= 0.7 ? "score-good" :
                                     data_quality.scores.completeness >= 0.5 ? "score-fair" : "score-poor")
       << R"('>)" << std::fixed << std::setprecision(1) << (data_quality.scores.completeness * 100) << R"(%</td>
                    <td>)" << (data_quality.scores.completeness >= 0.9 ? " Excellent" :
                             data_quality.scores.completeness >= 0.7 ? " Good" :
                             data_quality.scores.completeness >= 0.5 ? " Fair" : " Poor") << R"(</td>
                </tr>
                <tr>
                    <td>Consistency</td>
                    <td class=')" << (data_quality.scores.consistency >= 0.9 ? "score-excellent" :
                                     data_quality.scores.consistency >= 0.7 ? "score-good" :
                                     data_quality.scores.consistency >= 0.5 ? "score-fair" : "score-poor")
       << R"('>)" << std::fixed << std::setprecision(1) << (data_quality.scores.consistency * 100) << R"(%</td>
                    <td>)" << (data_quality.scores.consistency >= 0.9 ? " Excellent" :
                             data_quality.scores.consistency >= 0.7 ? " Good" :
                             data_quality.scores.consistency >= 0.5 ? " Fair" : " Poor") << R"(</td>
                </tr>
                <tr>
                    <td>Accuracy</td>
                    <td class=')" << (data_quality.scores.accuracy >= 0.9 ? "score-excellent" :
                                     data_quality.scores.accuracy >= 0.7 ? "score-good" :
                                     data_quality.scores.accuracy >= 0.5 ? "score-fair" : "score-poor")
       << R"('>)" << std::fixed << std::setprecision(1) << (data_quality.scores.accuracy * 100) << R"(%</td>
                    <td>)" << (data_quality.scores.accuracy >= 0.9 ? " Excellent" :
                             data_quality.scores.accuracy >= 0.7 ? " Good" :
                             data_quality.scores.accuracy >= 0.5 ? " Fair" : " Poor") << R"(</td>
                </tr>
                <tr>
                    <td><strong>Overall</strong></td>
                    <td class=')" << (data_quality.scores.overall >= 0.9 ? "score-excellent" :
                                     data_quality.scores.overall >= 0.7 ? "score-good" :
                                     data_quality.scores.overall >= 0.5 ? "score-fair" : "score-poor")
       << R"('><strong>)" << std::fixed << std::setprecision(1) << (data_quality.scores.overall * 100) << R"(%</strong></td>
                    <td><strong>)" << (data_quality.scores.overall >= 0.7 ? " PASS" : " FAIL") << R"(</strong></td>
                </tr>
            </table>
        </div>)";

    if (detailed) {
        ss << R"(
        <div class="section">
            <h3 class="section-title">Detailed Statistics</h3>
            <table>
                <tr>
                    <th>Column</th>
                    <th>Type</th>
                    <th>Nulls</th>
                    <th>Distinct</th>
                    <th>Mean</th>
                    <th>Std Dev</th>
                </tr>)";

        for (const auto& [col_name, analysis] : column_analyses) {
            ss << "<tr>\n";
            ss << "<td>" << col_name << "</td>\n";
            ss << "<td>" << analysis.detected_type << "</td>\n";
            ss << "<td>" << analysis.null_count << "</td>\n";
            ss << "<td>" << analysis.distinct_count << "</td>\n";

            if (analysis.detected_type == "numeric" || analysis.detected_type == "integer" ||
                analysis.detected_type == "float" || analysis.detected_type == "double") {
                ss << "<td>" << std::fixed << std::setprecision(2) << analysis.mean << "</td>\n";
                ss << "<td>" << std::fixed << std::setprecision(2) << analysis.std_dev << "</td>\n";
            } else {
                ss << "<td>N/A</td>\n";
                ss << "<td>N/A</td>\n";
            }

            ss << "</tr>\n";
        }

        ss << R"(            </table>
        </div>)";
    }

    ss << R"(
        <div class="section">
            <h3 class="section-title">Performance Metrics</h3>
            <table>
                <tr>
                    <th>Metric</th>
                    <th>Value</th>
                </tr>
                <tr>
                    <td>Total Processing Time</td>
                    <td>)" << performance.processing_time.count() << R"(ms</td>
                </tr>
                <tr>
                    <td>Memory Usage</td>
                    <td>)" << performance.memory_usage_mb << R"( MB</td>
                </tr>
                <tr>
                    <td>CPU Usage</td>
                    <td>)" << performance.cpu_usage_percent << R"(%</td>
                </tr>
                <tr>
                    <td>Within SLA</td>
                    <td>)" << (performance.within_sla ? " Yes" : " No") << R"(</td>
                </tr>
            </table>
        </div>

        <div class="section">
            <h3 class="section-title">Next Steps</h3>
            <div class="recommendation-card">
                <p><strong>Immediate Actions:</strong></p>
                <ol>
                    <li>Review critical recommendations and address urgent issues</li>
                    <li>Implement high-priority improvements</li>
                    <li>Schedule medium and low-priority enhancements</li>
                </ol>
            </div>
            <div class="recommendation-card">
                <p><strong>Ongoing Monitoring:</strong></p>
                <ol>
                    <li>Monitor data quality metrics regularly</li>
                    <li>Set up automated data quality checks</li>
                    <li>Re-run analysis after implementing changes</li>
                </ol>
            </div>
        </div>
    </div>
</body>
</html>)";

    return ss.str();
}

// Note: to_pdf_report and to_excel_report would require external libraries
// These are stubs that would need proper implementation
std::string ProfessionalComprehensiveAnalysisReport::to_pdf_report(bool detailed) const {
    // This would require a PDF generation library like wkhtmltopdf, PDFLib, etc.
    // For now, return HTML that can be converted to PDF
    return to_html_report(detailed);
}

std::string ProfessionalComprehensiveAnalysisReport::to_excel_report(bool detailed) const {
    // This would require an Excel generation library like libxlsxwriter
    // For now, return CSV format
    std::stringstream ss;

    // Data quality summary
    ss << "Data Quality Summary\n";
    ss << "Dimension,Score\n";
    ss << "Completeness," << data_quality.scores.completeness << "\n";
    ss << "Consistency," << data_quality.scores.consistency << "\n";
    ss << "Accuracy," << data_quality.scores.accuracy << "\n";
    ss << "Timeliness," << data_quality.scores.timeliness << "\n";
    ss << "Validity," << data_quality.scores.validity << "\n";
    ss << "Uniqueness," << data_quality.scores.uniqueness << "\n";
    ss << "Overall," << data_quality.scores.overall << "\n\n";

    // Column statistics
    ss << "Column Statistics\n";
    ss << "Column,Type,Total,Nulls,Missing%,Distinct,Mean,StdDev,Min,Median,Max\n";

    for (const auto& [col_name, analysis] : column_analyses) {
        ss << col_name << ",";
        ss << analysis.detected_type << ",";
        ss << analysis.total_count << ",";
        ss << analysis.null_count << ",";
        ss << analysis.missing_percentage << ",";
        ss << analysis.distinct_count << ",";

        if (analysis.detected_type == "numeric" || analysis.detected_type == "integer" ||
            analysis.detected_type == "float" || analysis.detected_type == "double") {
            ss << analysis.mean << ",";
            ss << analysis.std_dev << ",";
            ss << analysis.min_value << ",";
            ss << analysis.median << ",";
            ss << analysis.max_value;
        } else {
            ss << "N/A,N/A,N/A,N/A,N/A";
        }

        ss << "\n";
    }

    return ss.str();
}

bool ProfessionalComprehensiveAnalysisReport::save_to_file(const std::string& filename,
                                                          const std::string& format) const {
    try {
        std::ofstream file(filename);
        if (!file.is_open()) {
            return false;
        }

        if (format == "json") {
            file << to_json(true).dump(2);
        } else if (format == "markdown") {
            file << to_markdown_report(true);
        } else if (format == "html") {
            file << to_html_report(true);
        } else if (format == "csv") {
            file << to_excel_report(true);
        } else {
            // Default to JSON
            file << to_json(true).dump(2);
        }

        file.close();
        return true;

    } catch (const std::exception& e) {
        std::cerr << "Error saving report to file: " << e.what() << std::endl;
        return false;
    }
}

bool ProfessionalComprehensiveAnalysisReport::load_from_file(const std::string& filename) {
    try {
        std::ifstream file(filename);
        if (!file.is_open()) {
            return false;
        }

        std::string content((std::istreambuf_iterator<char>(file)),
                           std::istreambuf_iterator<char>());

        json j = json::parse(content);

        // Load metadata
        if (j.contains("metadata")) {
            analysis_id = j["metadata"].value("analysis_id", "");
            table_name = j["metadata"].value("table_name", "");
            analysis_timestamp = j["metadata"].value("analysis_timestamp", "");
            analysis_duration = milliseconds(j["metadata"].value("analysis_duration_ms", 0));
            row_count = j["metadata"].value("row_count", 0);
            column_count = j["metadata"].value("column_count", 0);
        }

        // Note: Full deserialization would require implementing from_json methods
        // for all structs. This is a simplified version.

        return true;

    } catch (const std::exception& e) {
        std::cerr << "Error loading report from file: " << e.what() << std::endl;
        return false;
    }
}

// ============================================
// ProfessionalDataAnalyzer Implementation
// ============================================

ProfessionalDataAnalyzer::ProfessionalDataAnalyzer(const Config& config)
    : config_(config) {

    // Create temp directory if it doesn't exist
    if (!fs::exists(config_.temp_directory)) {
        fs::create_directories(config_.temp_directory);
    }

    // Initialize performance metrics
    performance_metrics_.total_analyses = 0;
    performance_metrics_.successful_analyses = 0;
    performance_metrics_.failed_analyses = 0;
    performance_metrics_.total_processing_time = milliseconds(0);
    performance_metrics_.average_processing_time_per_row = 0.0;
    performance_metrics_.peak_memory_usage_mb = 0;
    performance_metrics_.cache_hit_rate = 0.0;
}

ProfessionalDataAnalyzer::~ProfessionalDataAnalyzer() {
    // Clean up temp directory
    try {
        if (fs::exists(config_.temp_directory) && config_.temp_directory != "/") {
            // Only remove if it's our temp directory
            if (config_.temp_directory.find("esql_analysis") != std::string::npos) {
                fs::remove_all(config_.temp_directory);
            }
        }
    } catch (...) {
        // Ignore cleanup errors
    }

    // Cancel all active analyses
    {
        std::lock_guard<std::mutex> lock(analysis_mutex_);
        for (auto& [id, progress] : active_analyses_) {
            if (progress.future.valid()) {
                // Try to cancel if possible
                // Note: std::future doesn't have cancel, but we can mark as cancelled
                progress.status.status = "cancelled";
            }
        }
        active_analyses_.clear();
    }
}

// Main analysis method
ProfessionalComprehensiveAnalysisReport ProfessionalDataAnalyzer::analyze_data(
    const std::vector<std::unordered_map<std::string, Datum>>& data,
    const std::string& target_column,
    const std::vector<std::string>& feature_columns,
    const std::string& analysis_type,
    const std::map<std::string, std::string>& options) {

    auto analysis_start = high_resolution_clock::now();
    std::string analysis_id = generate_analysis_id();

    try {
        // Validate input
        if (!validate_input_data(data)) {
            throw std::runtime_error("Invalid input data");
        }

        // Check resource availability
        size_t estimated_memory = estimate_memory_usage(data.size(), data.empty() ? 0 : data[0].size());
        if (!check_resource_availability(estimated_memory, config_.num_threads)) {
            throw std::runtime_error("Insufficient resources for analysis");
        }

        // Create progress tracker
        {
            std::lock_guard<std::mutex> lock(analysis_mutex_);
            AnalysisProgress progress;
            progress.analysis_id = analysis_id;
            progress.status.status = "running";
            progress.status.progress = 0.0;
            progress.start_time = analysis_start;
            progress.status.total_rows = data.size();
            //active_analyses_[analysis_id] = progress;

            active_analyses_.emplace(analysis_id, std::move(progress));
        }

        log_analysis_progress(analysis_id, "Starting analysis", 0.0);

        // Generate cache key
        std::string cache_key = generate_cache_key(data, target_column, feature_columns,
                                                  analysis_type, options);

        // Check cache
        if (config_.enable_caching) {
            std::lock_guard<std::mutex> lock(cache_mutex_);
            auto it = cache_.find(cache_key);
            if (it != cache_.end()) {
                // Check if cache is still valid (less than 1 hour old)
                auto now = system_clock::now();
                if (duration_cast<hours>(now - it->second.timestamp).count() < 1) {
                    log_analysis_progress(analysis_id, "Cache hit", 100.0);

                    {
                        std::lock_guard<std::mutex> lock(analysis_mutex_);
                        auto it2 = active_analyses_.find(analysis_id);
                        if (it2 != active_analyses_.end()) {
                            it2->second.status.status = "completed";
                            it2->second.status.progress = 100.0;
                        }
                    }

                    // Update cache hit rate
                    {
                        std::lock_guard<std::mutex> lock(metrics_mutex_);
                        performance_metrics_.cache_hit_rate =
                            (performance_metrics_.cache_hit_rate * performance_metrics_.total_analyses + 1.0) /
                            (performance_metrics_.total_analyses + 1.0);
                    }

                    return it->second.report;
                }
            }
        }

        ProfessionalComprehensiveAnalysisReport report;
        report.analysis_id = analysis_id;
        report.table_name = options.count("table_name") ? options.at("table_name") : "unknown";
        report.row_count = data.size();
        report.column_count = data.empty() ? 0 : data[0].size();

        // Start timing
        auto start_time = high_resolution_clock::now();

        // Step 1: Extract columns
        log_analysis_progress(analysis_id, "Extracting columns", 5.0);

        std::map<std::string, std::vector<Datum>> column_data;
        for (const auto& row : data) {
            for (const auto& col_value : row) {
                //column_data[col_name].push_back(value);
                const std::string& col_name = col_value.first;
                const Datum& value = col_value.second;
                column_data[col_name].push_back(value);
            }
        }

        // Step 2: Column analysis (parallel)
        log_analysis_progress(analysis_id, "Analyzing columns", 20.0);

        if (config_.enable_parallel_processing) {
            std::vector<std::future<ProfessionalColumnAnalysis>> column_futures;
            for (const auto& col_entry : column_data) {
                const std::string& col_name = col_entry.first;
                const std::vector<Datum>& values = col_entry.second;

                column_futures.push_back(std::async(std::launch::async,
                    [this, col_name, &values, &options]() {
                        return analyze_column_professional(col_name, values, options);
                    }));
            }

            for (auto& future : column_futures) {
                ProfessionalColumnAnalysis column_analysis = future.get();
                report.column_analyses[column_analysis.name] = column_analysis;
            }
        } else {
            for (const auto& col_entry : column_data) {
                const std::string& col_name = col_entry.first;
                const std::vector<Datum>& values = col_entry.second;

                ProfessionalColumnAnalysis column_analysis =
                    analyze_column_professional(col_name, values, options);
                report.column_analyses[column_analysis.name] = column_analysis;
            }
            /*for (const auto& [col_name, values] : column_data) {
                ProfessionalColumnAnalysis column_analysis =
                    analyze_column_professional(col_name, values, options);
                report.column_analyses[column_analysis.name] = column_analysis;
            }*/
        }

        // Step 3: Correlation analysis
        log_analysis_progress(analysis_id, "Analyzing correlations", 40.0);

        if (analysis_type == "CORRELATION" || analysis_type == "COMPREHENSIVE") {
            // Analyze correlations between feature columns
            std::vector<std::string> features = feature_columns;
            if (features.empty()) {
                for (const auto& [col_name, _] : column_data) {
                    if (col_name != target_column) {
                        features.push_back(col_name);
                    }
                }
            }

            // Limit number of correlation pairs
            size_t max_pairs = config_.max_correlation_pairs;
            size_t actual_pairs = 0;

            for (size_t i = 0; i < features.size(); ++i) {
                for (size_t j = i + 1; j < features.size(); ++j) {
                    if (actual_pairs >= max_pairs) break;

                    if (column_data.count(features[i]) && column_data.count(features[j])) {
                        ProfessionalCorrelationAnalysis corr =
                            analyze_correlation_professional(
                                features[i], features[j],
                                column_data[features[i]],
                                column_data[features[j]],
                                options);

                        if (std::abs(corr.pearson_r) > 0.1 || corr.is_statistically_significant) {
                            report.correlations.push_back(corr);
                        }

                        actual_pairs++;
                    }
                }
                if (actual_pairs >= max_pairs) break;
            }
        }

        // Step 4: Feature importance (if target column specified)
        log_analysis_progress(analysis_id, "Analyzing feature importance", 60.0);

        if (!target_column.empty() && column_data.count(target_column)) {
            std::vector<std::string> features = feature_columns;
            if (features.empty()) {
                for (const auto& [col_name, _] : column_data) {
                    if (col_name != target_column) {
                        features.push_back(col_name);
                    }
                }
            }

            // Limit number of features
            size_t max_features = std::min(features.size(), config_.max_features_for_importance);

            for (size_t i = 0; i < max_features; ++i) {
                if (column_data.count(features[i])) {
                    ProfessionalFeatureImportance importance =
                        analyze_feature_importance_professional(
                            features[i],
                            column_data[features[i]],
                            column_data[target_column],
                            options);

                    report.feature_importance.push_back(importance);
                }
            }
        }

        // Step 5: Clustering analysis
        log_analysis_progress(analysis_id, "Performing clustering", 70.0);

        if (analysis_type == "CLUSTERING" || analysis_type == "COMPREHENSIVE") {
            std::vector<std::string> features = feature_columns;
            if (features.empty()) {
                // Use numeric columns for clustering
                for (const auto& [col_name, analysis] : report.column_analyses) {
                    if (analysis.detected_type == "numeric" ||
                        analysis.detected_type == "integer" ||
                        analysis.detected_type == "float" ||
                        analysis.detected_type == "double") {
                        features.push_back(col_name);
                    }
                }
            }

            if (features.size() >= 2) {
                report.clusters = perform_clustering_professional(data, features, options);
            }
        }

        // Step 6: Outlier detection
        log_analysis_progress(analysis_id, "Detecting outliers", 80.0);

        if (analysis_type == "OUTLIER" || analysis_type == "COMPREHENSIVE") {
            std::vector<std::string> features = feature_columns;
            if (features.empty()) {
                // Use numeric columns for outlier detection
                for (const auto& [col_name, analysis] : report.column_analyses) {
                    if (analysis.detected_type == "numeric" ||
                        analysis.detected_type == "integer" ||
                        analysis.detected_type == "float" ||
                        analysis.detected_type == "double") {
                        features.push_back(col_name);
                    }
                }
            }

            if (!features.empty()) {
                ProfessionalOutlierAnalysis outlier_analysis =
                    detect_outliers_professional(data, features, options);
                report.outliers.push_back(outlier_analysis);
            }
        }

        // Step 7: Data quality assessment
        log_analysis_progress(analysis_id, "Assessing data quality", 90.0);

        report.data_quality = assess_data_quality_professional(data, options);

        // Step 8: Generate insights and recommendations
        log_analysis_progress(analysis_id, "Generating insights", 95.0);

        generate_insights_and_recommendations_professional(report, target_column, options);

        // Step 9: Finalize report
        auto end_time = high_resolution_clock::now();
        report.analysis_duration = duration_cast<milliseconds>(end_time - start_time);
        report.analysis_timestamp = system_clock::to_time_t(system_clock::now());

        // Update performance metrics
        {
            std::lock_guard<std::mutex> lock(metrics_mutex_);
            performance_metrics_.total_analyses++;
            performance_metrics_.successful_analyses++;
            performance_metrics_.total_processing_time += report.analysis_duration;
            performance_metrics_.average_processing_time_per_row =
                performance_metrics_.total_processing_time.count() /
                (performance_metrics_.total_analyses * std::max(report.row_count, (size_t)1));

            // Estimate memory usage
            size_t current_memory = estimate_memory_usage(report.row_count, report.column_count);
            performance_metrics_.peak_memory_usage_mb =
                std::max(performance_metrics_.peak_memory_usage_mb, current_memory);

            // Update cache hit rate
            performance_metrics_.cache_hit_rate =
                (performance_metrics_.cache_hit_rate * (performance_metrics_.total_analyses - 1)) /
                performance_metrics_.total_analyses;
        }

        // Cache the result
        if (config_.enable_caching) {
            std::lock_guard<std::mutex> lock(cache_mutex_);

            // Check cache size
            size_t report_size = estimate_report_size(report);
            while (cache_size_bytes_ + report_size > config_.cache_size_mb * 1024 * 1024) {
                if (cache_.empty()) break;

                // Remove oldest entry
                auto oldest = std::min_element(cache_.begin(), cache_.end(),
                    [](const auto& a, const auto& b) {
                        return a.second.timestamp < b.second.timestamp;
                    });

                if (oldest != cache_.end()) {
                    cache_size_bytes_ -= oldest->second.size_bytes;
                    cache_.erase(oldest);
                }
            }

            // Add to cache
            AnalysisCache cache_entry;
            cache_entry.key = cache_key;
            cache_entry.report = report;
            cache_entry.timestamp = system_clock::now();
            cache_entry.size_bytes = report_size;

            cache_[cache_key] = cache_entry;
            cache_size_bytes_ += report_size;
        }

        log_analysis_progress(analysis_id, "Analysis completed", 100.0);

        {
            std::lock_guard<std::mutex> lock(analysis_mutex_);
            auto it = active_analyses_.find(analysis_id);
            if (it != active_analyses_.end()) {
                it->second.status.status = "completed";
                it->second.status.progress = 100.0;
                it->second.status.elapsed_time =
                    duration_cast<milliseconds>(high_resolution_clock::now() - analysis_start);
            }
        }

        return report;

    } catch (const std::exception& e) {
        handle_analysis_error(analysis_id, e);
        throw;
    }
}

bool ProfessionalDataAnalyzer::is_numeric_datum(const Datum& d) const {
    return d.is_integer() || d.is_float() || d.is_double() || d.is_boolean();
}

double ProfessionalDataAnalyzer::get_double_from_datum(const Datum& d) const {
    if (d.is_integer()) return static_cast<double>(d.as_int());
    if (d.is_float()) return static_cast<double>(d.as_float());
    if (d.is_double()) return d.as_double();
    if (d.is_boolean()) return d.as_bool() ? 1.0 : 0.0;
    if (d.is_string()) {
        try {
            return std::stod(d.as_string());
        } catch (...) {
            return 0.0;
        }
    }
    return 0.0;
}

std::string ProfessionalDataAnalyzer::get_string_from_datum(const Datum& d) const {
    if (d.is_string()) return d.as_string();
    return d.as_string_convert();
}

// Helper methods
std::string ProfessionalDataAnalyzer::detect_data_type_professional(const std::vector<Datum>& values, size_t distinct_count) const {
    if (values.empty()) return "unknown";

    // Check if all values are null
    bool all_null = true;
    for (const auto& value : values) {
        if (!value.is_null()) {
            all_null = false;
            break;
        }
    }
    if (all_null) return "null";

    // Check numeric types
    bool all_numeric = true;
    bool all_integer = true;
    bool has_float = false;

    for (const auto& value : values) {
        if (!value.is_null()) {
            if (!(value.is_integer() || value.is_float() || value.is_double() || value.is_boolean())) {
                all_numeric = false;
                all_integer = false;
                break;
            }

            double num = get_double_from_datum(value);;
            if (std::abs(num - std::round(num)) > 1e-10) {
                all_integer = false;
                has_float = true;
            }
        }
    }

    if (all_numeric) {
        if (all_integer) return "integer";
        if (has_float) return "float";
        return "numeric";
    }

    // Check boolean
    bool all_boolean = true;
    for (const auto& value : values) {
        if (!value.is_null()) {
            std::string str = value.as_string_convert();
            if (str != "true" && str != "false" && str != "TRUE" && str != "FALSE" &&
                str != "1" && str != "0") {
                all_boolean = false;
                break;
            }
        }
    }
    if (all_boolean) return "boolean";

    // Check date/time
    bool all_date = true;
    // Simple date pattern check
    // This would be more robust in production

    // Default to string/categorical
    double cardinality_ratio = static_cast<double>(distinct_count) / values.size();
    if (cardinality_ratio < 0.1 && distinct_count <= 100) {
        return "categorical";
    }

    return "string";
}

/*size_t ProfessionalDataAnalyzer::estimate_memory_usage(size_t row_count, size_t column_count) const {
    // Rough estimate: 100 bytes per cell
    size_t cells = row_count * column_count;
    return (cells * 100) / (1024 * 1024); // Convert to MB
}

std::string ProfessionalDataAnalyzer::generate_analysis_id() const {
    auto now = std::chrono::system_clock::now();
    auto timestamp = std::chrono::duration_cast<std::chrono::milliseconds>(
        now.time_since_epoch()).count();

    std::stringstream ss;
    ss << "analysis_" << timestamp << "_" << std::hex << std::rand();
    return ss.str();
}*/

void ProfessionalDataAnalyzer::detect_seasonality(
    ProfessionalTimeSeriesAnalysis::Seasonality& seasonality,
    const std::vector<double>& series,
    const std::vector<long>& timestamps) {

    // Implementation for seasonality detection
    // This should detect seasonal patterns in time series data

    // Check for multiple seasonal periods
    std::vector<size_t> candidate_periods = {7, 30, 365}; // Daily, weekly, yearly patterns

    for (size_t period : candidate_periods) {
        if (series.size() >= period * 2) {
            // Calculate seasonal strength using autocorrelation
            double strength = calculate_seasonal_strength(series, period);
            if (strength > 0.3) { // Threshold for meaningful seasonality
                seasonality.seasonal_periods.push_back(period);
                seasonality.seasonal_strengths.push_back(strength);
            }
        }
    }

    seasonality.has_seasonality = !seasonality.seasonal_periods.empty();

    if (seasonality.has_seasonality) {

	            // Find dominant period
        auto max_it = std::max_element(seasonality.seasonal_strengths.begin(),
                                      seasonality.seasonal_strengths.end());
        size_t idx = std::distance(seasonality.seasonal_strengths.begin(), max_it);
        seasonality.dominant_period = seasonality.seasonal_periods[idx];

        // Classify seasonality type
        if (seasonality.dominant_period == 7) {
            seasonality.seasonal_type = "weekly";
        } else if (seasonality.dominant_period == 30 || seasonality.dominant_period == 31) {
            seasonality.seasonal_type = "monthly";
        } else if (seasonality.dominant_period == 365 || seasonality.dominant_period == 366) {
            seasonality.seasonal_type = "yearly";
        } else {
            seasonality.seasonal_type = "custom";
        }
    }
}

double ProfessionalDataAnalyzer::calculate_seasonal_strength(
    const std::vector<double>& series, size_t period) {

    if (series.size() < period * 2) return 0.0;

    // Calculate autocorrelation at lag = period
    double mean = 0.0;
    for (double val : series) {
        mean += val;
    }
    mean /= series.size();

    double numerator = 0.0;
    double denominator = 0.0;

    for (size_t i = period; i < series.size(); ++i) {
        double diff1 = series[i] - mean;
        double diff2 = series[i - period] - mean;
        numerator += diff1 * diff2;
        denominator += diff1 * diff1;
    }
       if (denominator < EPSILON) return 0.0;
    return numerator / denominator;
}

bool ProfessionalDataAnalyzer::is_likely_categorical_professional(const std::vector<Datum>& values,
                                                                 size_t distinct_count) const {
    if (values.empty()) return false;

    double cardinality_ratio = static_cast<double>(distinct_count) / values.size();

    // Low cardinality suggests categorical
    if (cardinality_ratio < 0.1 && distinct_count <= 100) {
        return true;
    }

    // Check if values look like categories (short strings, limited set)
    size_t string_count = 0;
    size_t total_non_null = 0;

    for (const auto& value : values) {
        if (!value.is_null()) {
            total_non_null++;
            if (!is_numeric_datum(value)) {
                std::string str = get_string_from_datum(value);
                if (str.length() <= 50) { // Reasonable category length
                    string_count++;
                }
            }
        }
    }

    if (total_non_null > 0) {
        double string_ratio = static_cast<double>(string_count) / total_non_null;
        if (string_ratio > 0.8) {
            return true;
        }
    }

    return false;
}

std::vector<double> ProfessionalDataAnalyzer::extract_numeric_values_professional(const std::vector<Datum>& values) const {
    std::vector<double> numeric_values;
    numeric_values.reserve(values.size());

    for (const auto& value : values) {
        if (!value.is_null() && !is_numeric_datum(value)) {
            numeric_values.push_back(get_double_from_datum(value));
        } else {
            // For non-numeric or null, push NaN
            numeric_values.push_back(std::numeric_limits<double>::quiet_NaN());
        }
    }

    return numeric_values;
}

std::vector<std::string> ProfessionalDataAnalyzer::extract_string_values_professional(const std::vector<Datum>& values) const {
    std::vector<std::string> string_values;
    string_values.reserve(values.size());

    for (const auto& value : values) {
        if (!value.is_null()) {
            string_values.push_back(get_string_from_datum(value));
        } else {
            string_values.push_back(""); // Empty string for null
        }
    }

    return string_values;
}

// Parallel processing helper
template<typename T, typename Func>
std::vector<T> ProfessionalDataAnalyzer::parallel_map(const std::vector<std::vector<Datum>>& data_chunks,
                                                      Func func) {
    std::vector<T> results;
    results.reserve(data_chunks.size());

    if (config_.enable_parallel_processing && data_chunks.size() > 1) {
        std::vector<std::future<T>> futures;
        futures.reserve(data_chunks.size());

        for (const auto& chunk : data_chunks) {
            futures.push_back(std::async(std::launch::async, func, chunk));
        }

        for (auto& future : futures) {
            results.push_back(future.get());
        }
    } else {
        for (const auto& chunk : data_chunks) {
            results.push_back(func(chunk));
        }
    }

    return results;
}

// Resource management
bool ProfessionalDataAnalyzer::check_resource_availability(size_t estimated_memory_mb,
                                                          size_t estimated_cpu_cores) const {
    // Check memory (simplified - in production would check system memory)
    if (estimated_memory_mb > config_.max_memory_usage_mb) {
        log_performance_metric("memory_exceeded",
                              static_cast<double>(estimated_memory_mb) / config_.max_memory_usage_mb);
        return false;
    }

    // Check CPU cores (simplified)
    if (estimated_cpu_cores > config_.num_threads) {
        log_performance_metric("cpu_exceeded",static_cast<double>(estimated_cpu_cores) / config_.num_threads);
        return false;
    }

    return true;
}

void ProfessionalDataAnalyzer::cleanup_resources() {
    // Clean up cache if it's too large
    if (config_.enable_caching && cache_size_bytes_ > config_.cache_size_mb * 1024 * 1024 * 0.8) {
        std::lock_guard<std::mutex> lock(cache_mutex_);

        // Remove oldest entries until we're at 50% capacity
        while (cache_size_bytes_ > config_.cache_size_mb * 1024 * 1024 * 0.5) {
            if (cache_.empty()) break;

            auto oldest = std::min_element(cache_.begin(), cache_.end(),
                [](const auto& a, const auto& b) {
                    return a.second.timestamp < b.second.timestamp;
                });

            if (oldest != cache_.end()) {
                cache_size_bytes_ -= oldest->second.size_bytes;
                cache_.erase(oldest);
            }
        }
    }
}

// Error handling
void ProfessionalDataAnalyzer::handle_analysis_error(const std::string& analysis_id,
                                                    const std::exception& e) {
    {
        std::lock_guard<std::mutex> lock(analysis_mutex_);
        auto it = active_analyses_.find(analysis_id);
        if (it != active_analyses_.end()) {
            it->second.status.status = "failed";
            it->second.status.progress = 0.0;
            it->second.status.elapsed_time = milliseconds(0);
        }
    }

    {
        std::lock_guard<std::mutex> lock(metrics_mutex_);
        performance_metrics_.total_analyses++;
        performance_metrics_.failed_analyses++;
    }

    log_performance_metric("analysis_failed", 1.0);
}

// Logging
void ProfessionalDataAnalyzer::log_analysis_progress(const std::string& analysis_id,
                                                    const std::string& message,
                                                    double progress) {
    if (config_.enable_progress_reporting) {
        std::lock_guard<std::mutex> lock(analysis_mutex_);
        auto it = active_analyses_.find(analysis_id);
        if (it != active_analyses_.end()) {
            it->second.status.progress = progress;
            it->second.status.current_operation = message;

            // Estimate remaining time
            if (progress > 0) {
                auto now = high_resolution_clock::now();
                auto elapsed = duration_cast<milliseconds>(now - it->second.status.start_time);
                double total_estimated = elapsed.count() / (progress / 100.0);
                double remaining = total_estimated - elapsed.count();
                it->second.status.estimated_remaining_time = milliseconds(static_cast<long long>(remaining));
            }

            // Log at intervals
            static size_t last_logged_progress = 0;
            size_t current_progress = static_cast<size_t>(progress);
            if (current_progress >= last_logged_progress + config_.progress_report_interval) {
                std::cout << "[ProfessionalDataAnalyzer] Analysis " << analysis_id
                          << ": " << message << " (" << progress << "%)" << std::endl;
                last_logged_progress = current_progress;
            }
        }
    }
}

void ProfessionalDataAnalyzer::log_performance_metric(const std::string& metric, double value) const {
    // In production, this would log to a metrics system
    if (config_.enable_progress_reporting) {
        std::cout << "[ProfessionalDataAnalyzer] Metric: " << metric << " = " << value << std::endl;
    }
}

// Analysis status
ProfessionalDataAnalyzer::AnalysisStatus ProfessionalDataAnalyzer::get_analysis_status(
    const std::string& analysis_id) const {

    std::lock_guard<std::mutex> lock(analysis_mutex_);
    auto it = active_analyses_.find(analysis_id);
    if (it != active_analyses_.end()) {
        return it->second.status;
    }

    AnalysisStatus status;
    status.status = "not_found";
    return status;
}

bool ProfessionalDataAnalyzer::cancel_analysis(const std::string& analysis_id) {
    std::lock_guard<std::mutex> lock(analysis_mutex_);
    auto it = active_analyses_.find(analysis_id);
    if (it != active_analyses_.end()) {
        it->second.status.status = "cancelled";

        // Try to cancel the future if it's still running
        if (it->second.future.valid()) {
            // Note: std::future doesn't have cancel, but we can mark as cancelled
            // In production, we would use a cancellable future implementation
        }

        return true;
    }

    return false;
}

// Export analysis
bool ProfessionalDataAnalyzer::export_analysis(const std::string& analysis_id,
                                              const std::string& format,
                                              const std::string& output_path) {
    // In production, this would export the analysis report
    // For now, return success if the analysis exists
    std::lock_guard<std::mutex> lock(analysis_mutex_);
    return active_analyses_.find(analysis_id) != active_analyses_.end();
}

// Get analysis history
std::vector<std::string> ProfessionalDataAnalyzer::get_analysis_history() const {
    std::vector<std::string> history;
    std::lock_guard<std::mutex> lock(analysis_mutex_);

    for (const auto& [id, progress] : active_analyses_) {
        history.push_back(id + " - " + progress.status.status);
    }

    return history;
}

// Clear cache
void ProfessionalDataAnalyzer::clear_cache() {
    std::lock_guard<std::mutex> lock(cache_mutex_);
    cache_.clear();
    cache_size_bytes_ = 0;
}

// Performance metrics
ProfessionalDataAnalyzer::PerformanceMetrics ProfessionalDataAnalyzer::get_performance_metrics() const {
    std::lock_guard<std::mutex> lock(metrics_mutex_);
    return performance_metrics_;
}

// Private helper functions for the analyzer
std::string ProfessionalDataAnalyzer::generate_analysis_id() const {
    auto now = system_clock::now();
    auto duration = now.time_since_epoch();
    auto millis = duration_cast<milliseconds>(duration).count();

    // Add random component to avoid collisions
    std::random_device rd;
    std::mt19937_64 gen(rd());
    std::uniform_int_distribution<> dis(1000, 9999);

    return "analysis_" + std::to_string(millis) + "_" + std::to_string(dis(gen));
}

std::string ProfessionalDataAnalyzer::generate_cache_key(
    const std::vector<std::unordered_map<std::string, Datum>>& data,
    const std::string& target_column,
    const std::vector<std::string>& feature_columns,
    const std::string& analysis_type,
    const std::map<std::string, std::string>& options) const {

    // Create a hash from the input parameters
    std::stringstream ss;
    ss << data.size() << "_";
    ss << target_column << "_";

    for (const auto& feature : feature_columns) {
        ss << feature << ",";
    }

    ss << analysis_type << "_";

    for (const auto& [key, value] : options) {
        ss << key << "=" << value << ",";
    }

    // Hash the string
    std::hash<std::string> hasher;
    return std::to_string(hasher(ss.str()));
}

size_t ProfessionalDataAnalyzer::estimate_memory_usage(size_t row_count, size_t column_count) const {
    // Very rough estimate: each cell ~ 100 bytes, plus overhead
    size_t basic_memory = row_count * column_count * 100;

    // Analysis overhead
    size_t analysis_overhead = column_count * 1024 * 1024; // 1MB per column

    return (basic_memory + analysis_overhead) / (1024 * 1024); // Convert to MB
}

size_t ProfessionalDataAnalyzer::estimate_report_size(const ProfessionalComprehensiveAnalysisReport& report) const {
    // Rough estimate of report size in bytes
    size_t size = 0;

    // Column analyses
    size += report.column_analyses.size() * 1024; // ~1KB per column

    // Correlations
    size += report.correlations.size() * 512; // ~0.5KB per correlation

    // Feature importance
    size += report.feature_importance.size() * 2048; // ~2KB per feature

    // Data quality
    size += 1024 * 10; // ~10KB for data quality report

    return size;
}

bool ProfessionalDataAnalyzer::validate_input_data(
    const std::vector<std::unordered_map<std::string, Datum>>& data) const {

    if (data.empty()) {
        return false;
    }

    // Check that all rows have the same columns
    std::set<std::string> first_row_keys;
    for (const auto& [key, _] : data[0]) {
        first_row_keys.insert(key);
    }

    for (size_t i = 1; i < data.size(); ++i) {
        std::set<std::string> current_row_keys;
        for (const auto& [key, _] : data[i]) {
            current_row_keys.insert(key);
        }

        if (current_row_keys != first_row_keys) {
            std::cerr << "Warning: Row " << i << " has different columns than first row" << std::endl;
            // In production, we might want to handle this more gracefully
            return false;
        }
    }

    return true;
}


ProfessionalColumnAnalysis ProfessionalDataAnalyzer::analyze_column_professional(
    const std::string& column_name,
    const std::vector<Datum>& values,
    const std::map<std::string, std::string>& options) {

    ProfessionalColumnAnalysis analysis;
    analysis.name = column_name;

    // Count nulls and total
    analysis.total_count = values.size();
    analysis.null_count = std::count_if(values.begin(), values.end(),
                                       [](const Datum& d) { return d.is_null(); });
    analysis.missing_percentage = (analysis.null_count * 100.0) / analysis.total_count;

    // Extract non-null values
    std::vector<Datum> non_null_values;
    non_null_values.reserve(values.size() - analysis.null_count);
    for (const auto& value : values) {
        if (!value.is_null()) {
            non_null_values.push_back(value);
        }
    }

    // Count distinct values
    std::set<std::string> distinct_strings;
    for (const auto& value : non_null_values) {
        distinct_strings.insert(get_string_from_datum(value));
    }
    analysis.distinct_count = distinct_strings.size();

    // Detect data type
    analysis.detected_type = detect_data_type_professional(values, analysis.distinct_count);
    analysis.is_categorical = is_likely_categorical_professional(values, analysis.distinct_count);

    if (analysis.detected_type == "numeric" || analysis.detected_type == "integer" ||
        analysis.detected_type == "float" || analysis.detected_type == "double") {

        // Extract numeric values
        std::vector<double> numeric_values = extract_numeric_values_professional(values);

        // Remove NaN values for calculations
        std::vector<double> valid_numeric_values;
        valid_numeric_values.reserve(numeric_values.size());
        for (double val : numeric_values) {
            if (!std::isnan(val)) {
                valid_numeric_values.push_back(val);
            }
        }

        if (!valid_numeric_values.empty()) {
            // Basic statistics
            analysis.mean = ProfessionalStatisticalCalculator::calculate_mean(valid_numeric_values);
            analysis.median = ProfessionalStatisticalCalculator::calculate_median(valid_numeric_values);
            analysis.std_dev = ProfessionalStatisticalCalculator::calculate_std_dev(valid_numeric_values);
            analysis.variance = analysis.std_dev * analysis.std_dev;

            // Min and max
            auto [min_it, max_it] = std::minmax_element(valid_numeric_values.begin(),
                                                       valid_numeric_values.end());
            analysis.min_value = *min_it;
            analysis.max_value = *max_it;
            analysis.range = analysis.max_value - analysis.min_value;

            // Quartiles
            analysis.q1 = ProfessionalStatisticalCalculator::calculate_quantile(valid_numeric_values, 0.25);
            analysis.q2 = analysis.median;
            analysis.q3 = ProfessionalStatisticalCalculator::calculate_quantile(valid_numeric_values, 0.75);
            analysis.iqr = analysis.q3 - analysis.q1;

            // Percentiles
            analysis.p10 = ProfessionalStatisticalCalculator::calculate_quantile(valid_numeric_values, 0.10);
            analysis.p90 = ProfessionalStatisticalCalculator::calculate_quantile(valid_numeric_values, 0.90);
            analysis.p95 = ProfessionalStatisticalCalculator::calculate_quantile(valid_numeric_values, 0.95);
            analysis.p99 = ProfessionalStatisticalCalculator::calculate_quantile(valid_numeric_values, 0.99);

            // Shape measures
            analysis.skewness = ProfessionalStatisticalCalculator::calculate_skewness(valid_numeric_values);
            analysis.kurtosis = ProfessionalStatisticalCalculator::calculate_kurtosis(valid_numeric_values);

            // Other measures
            analysis.mad = ProfessionalStatisticalCalculator::calculate_mad(valid_numeric_values);
            analysis.coefficient_of_variation = analysis.std_dev / std::abs(analysis.mean);

            // Normality tests
            double p_value;
            analysis.shapiro_wilk = ProfessionalStatisticalCalculator::shapiro_wilk(valid_numeric_values, p_value);
            analysis.normality_p_value = p_value;
            analysis.is_normal = (p_value > 0.05); // Not rejecting null hypothesis of normality

            analysis.jarque_bera = ProfessionalStatisticalCalculator::jarque_bera(valid_numeric_values, p_value);

            // Outlier detection
            std::vector<size_t> outliers = ProfessionalStatisticalCalculator::detect_outliers_iqr(valid_numeric_values);
            analysis.has_outliers = !outliers.empty();
            analysis.outlier_percentage = (outliers.size() * 100.0) / valid_numeric_values.size();

            // Convert indices back to original indices
            size_t valid_idx = 0;
            for (size_t i = 0; i < values.size(); ++i) {
                if (!values[i].is_null() && !std::isnan(numeric_values[i])) {
                    if (std::find(outliers.begin(), outliers.end(), valid_idx) != outliers.end()) {
                        analysis.outlier_indices.push_back(i);
                        analysis.outliers.push_back(numeric_values[i]);
                    }
                    valid_idx++;
                }
            }

            // Histogram
            if (valid_numeric_values.size() >= 10) {
                size_t num_bins = std::min(static_cast<size_t>(20),
                                          static_cast<size_t>(std::sqrt(valid_numeric_values.size())));

                double bin_width = (analysis.max_value - analysis.min_value) / num_bins;
                analysis.histogram_bins.resize(num_bins);
                analysis.histogram_counts.resize(num_bins, 0);

                for (size_t i = 0; i < num_bins; ++i) {
                    analysis.histogram_bins[i] = analysis.min_value + (i + 0.5) * bin_width;
                }

                for (double val : valid_numeric_values) {
                    size_t bin_idx = static_cast<size_t>((val - analysis.min_value) / bin_width);
                    if (bin_idx >= num_bins) bin_idx = num_bins - 1;
                    analysis.histogram_counts[bin_idx]++;
                }
            }
        }

    } else if (analysis.is_categorical) {
        // Categorical analysis
        std::vector<std::string> string_values = extract_string_values_professional(values);

        // Count frequencies
        std::map<std::string, size_t> frequencies;
        for (const auto& str : string_values) {
            if (!str.empty()) { // Skip empty strings from null values
                frequencies[str]++;
            }
        }

        analysis.value_frequencies = frequencies;

        // Get top categories
        std::vector<std::pair<std::string, size_t>> sorted_categories(frequencies.begin(),
                                                                     frequencies.end());
        std::sort(sorted_categories.begin(), sorted_categories.end(),
                 [](const auto& a, const auto& b) { return a.second > b.second; });

        for (size_t i = 0; i < std::min(sorted_categories.size(), (size_t)10); ++i) {
            analysis.top_categories.push_back(sorted_categories[i].first);
        }

        // Categorical statistics
        analysis.categorical_stats.cardinality_ratio =
            static_cast<double>(analysis.distinct_count) / analysis.total_count;

        // Calculate average category length
        size_t total_length = 0;
        size_t count = 0;
        for (const auto& str : string_values) {
            if (!str.empty()) {
                total_length += str.length();
                count++;
            }
        }
        analysis.categorical_stats.average_category_length =
            count > 0 ? static_cast<double>(total_length) / count : 0.0;

        // Find rare categories (<1% frequency)
        double rare_threshold = analysis.total_count * 0.01;
        for (const auto& [category, freq] : frequencies) {
            if (freq < rare_threshold) {
                analysis.categorical_stats.rare_categories.push_back(category);
            }
        }

        // Find dominant categories (>10% frequency)
        double dominant_threshold = analysis.total_count * 0.10;
        for (const auto& [category, freq] : frequencies) {
            if (freq > dominant_threshold) {
                analysis.categorical_stats.dominant_categories.push_back(category);
            }
        }
    }

    // Data quality scores
    analysis.quality.completeness_score = 1.0 - (analysis.null_count / static_cast<double>(analysis.total_count));
    analysis.quality.overall_score = analysis.quality.completeness_score; // Simplified

    // Add issues and recommendations
    if (analysis.missing_percentage > 5.0) {
        analysis.quality.issues.push_back("High percentage of missing values (" +
                                         std::to_string(analysis.missing_percentage) + "%)");
        analysis.quality.recommendations.push_back("Consider imputation or data collection");
    }

    if (analysis.detected_type == "numeric" && analysis.has_outliers &&
        analysis.outlier_percentage > 1.0) {
        analysis.quality.issues.push_back("Significant outliers detected (" +
                                         std::to_string(analysis.outlier_percentage) + "%)");
        analysis.quality.recommendations.push_back("Investigate outliers for data quality issues");
    }

    return analysis;
}

// ============================================
// ProfessionalDataAnalyzer Core Methods
// ============================================

ProfessionalCorrelationAnalysis ProfessionalDataAnalyzer::analyze_correlation_professional(
    const std::string& col1,
    const std::string& col2,
    const std::vector<Datum>& values1,
    const std::vector<Datum>& values2,
    const std::map<std::string, std::string>& options) {

    ProfessionalCorrelationAnalysis analysis;
    analysis.column1 = col1;
    analysis.column2 = col2;

    if (values1.size() != values2.size() || values1.size() < 2) {
        return analysis;
    }

    // Extract numeric values
    std::vector<double> x_numeric, y_numeric;
    x_numeric.reserve(values1.size());
    y_numeric.reserve(values2.size());

    for (size_t i = 0; i < values1.size(); ++i) {
        if (!values1[i].is_null() && !values2[i].is_null() &&
            is_numeric_datum(values1[i]) && is_numeric_datum(values2[i])) {
            x_numeric.push_back(get_double_from_datum(values1[i]));
            y_numeric.push_back(get_double_from_datum(values2[i]));
        }
    }

    if (x_numeric.size() < 3) {
        return analysis;
    }

    // Calculate Pearson correlation
    double p_value, conf_lower, conf_upper;
    analysis.pearson_r = ProfessionalStatisticalCalculator::pearson_correlation(
        x_numeric, y_numeric, p_value, conf_lower, conf_upper);
    analysis.pearson_p_value = p_value;
    analysis.pearson_confidence_lower = conf_lower;
    analysis.pearson_confidence_upper = conf_upper;
    analysis.pearson_r_squared = analysis.pearson_r * analysis.pearson_r;

    // Calculate Spearman correlation
    analysis.spearman_rho = ProfessionalStatisticalCalculator::spearman_correlation(
        x_numeric, y_numeric, p_value);
    analysis.spearman_p_value = p_value;

    // Calculate Kendall's tau
    analysis.kendall_tau = ProfessionalStatisticalCalculator::kendall_tau(
        x_numeric, y_numeric, p_value);
    analysis.kendall_p_value = p_value;

    // Calculate distance correlation
    analysis.distance_correlation = ProfessionalStatisticalCalculator::distance_correlation(
        x_numeric, y_numeric);

    // Calculate mutual information
    analysis.mutual_information = ProfessionalStatisticalCalculator::mutual_information(
        x_numeric, y_numeric, 20); // 20 bins

    // Normalize mutual information
    double entropy_x = ProfessionalStatisticalCalculator::entropy(x_numeric, 20);
    double entropy_y = ProfessionalStatisticalCalculator::entropy(y_numeric, 20);
    if (entropy_x > 0 && entropy_y > 0) {
        analysis.normalized_mutual_information = analysis.mutual_information /
                                                std::sqrt(entropy_x * entropy_y);
    }

    // Calculate MIC
    analysis.mic = ProfessionalStatisticalCalculator::maximal_information_coefficient(
        x_numeric, y_numeric);

    // Determine relationship strength and direction
    if (std::abs(analysis.pearson_r) >= 0.8) {
        analysis.relationship_strength = "very_strong";
    } else if (std::abs(analysis.pearson_r) >= 0.6) {
        analysis.relationship_strength = "strong";
    } else if (std::abs(analysis.pearson_r) >= 0.4) {
        analysis.relationship_strength = "moderate";
    } else if (std::abs(analysis.pearson_r) >= 0.2) {
        analysis.relationship_strength = "weak";
    } else {
        analysis.relationship_strength = "very_weak";
    }

    analysis.relationship_direction = (analysis.pearson_r >= 0) ? "positive" : "negative";

    // Determine relationship type
    if (std::abs(analysis.pearson_r) >= 0.7 && analysis.pearson_p_value < 0.05) {
        analysis.relationship_type = "linear";
    } else if (std::abs(analysis.spearman_rho) >= 0.7 && analysis.spearman_p_value < 0.05) {
        analysis.relationship_type = "monotonic";
    } else if (analysis.mic >= 0.7) {
        analysis.relationship_type = "nonlinear";
    } else {
        analysis.relationship_type = "no_relationship";
    }

    // Statistical significance
    double significance_threshold = 0.05;
    if (options.count("significance_threshold")) {
        try {
            significance_threshold = std::stod(options.at("significance_threshold"));
        } catch (...) {
            // Use default
        }
    }

    analysis.is_statistically_significant = (
        analysis.pearson_p_value < significance_threshold ||
        analysis.spearman_p_value < significance_threshold ||
        analysis.kendall_p_value < significance_threshold
    );

    // Practical significance (Cohen's rules)
    double effect_size_threshold = 0.1;
    if (options.count("effect_size_threshold")) {
        try {
            effect_size_threshold = std::stod(options.at("effect_size_threshold"));
        } catch (...) {
            // Use default
        }
    }

    analysis.effect_size = std::abs(analysis.pearson_r);
    analysis.is_practically_significant = (analysis.effect_size >= effect_size_threshold);

    // Generate scatter data for visualization
    analysis.scatter_data.reserve(x_numeric.size());
    for (size_t i = 0; i < x_numeric.size(); ++i) {
        analysis.scatter_data.emplace_back(x_numeric[i], y_numeric[i]);
    }

    // Simple linear regression for regression line
    if (analysis.relationship_type == "linear" && analysis.scatter_data.size() >= 2) {
        // Calculate regression line: y = a + bx
        double sum_x = 0.0, sum_y = 0.0, sum_xy = 0.0, sum_xx = 0.0;
        for (const auto& [x, y] : analysis.scatter_data) {
            sum_x += x;
            sum_y += y;
            sum_xy += x * y;
            sum_xx += x * x;
        }

        double n = analysis.scatter_data.size();
        double b = (n * sum_xy - sum_x * sum_y) / (n * sum_xx - sum_x * sum_x);
        double a = (sum_y - b * sum_x) / n;

        // Generate points for regression line
        auto [min_x, max_x] = std::minmax_element(
            x_numeric.begin(), x_numeric.end());

        analysis.regression_line.push_back(a + b * (*min_x));
        analysis.regression_line.push_back(a + b * (*max_x));

        // Calculate confidence band (simplified)
        double residual_variance = 0.0;
        for (const auto& [x, y] : analysis.scatter_data) {
            double predicted = a + b * x;
            residual_variance += (y - predicted) * (y - predicted);
        }
        residual_variance /= (n - 2);

        double t_critical = 1.96; // 95% confidence for large n
        double se = std::sqrt(residual_variance);

        analysis.confidence_band.push_back(analysis.regression_line[0] - t_critical * se);
        analysis.confidence_band.push_back(analysis.regression_line[0] + t_critical * se);
        analysis.confidence_band.push_back(analysis.regression_line[1] - t_critical * se);
        analysis.confidence_band.push_back(analysis.regression_line[1] + t_critical * se);
    }

    return analysis;
}

ProfessionalFeatureImportance ProfessionalDataAnalyzer::analyze_feature_importance_professional(
    const std::string& feature_name,
    const std::vector<Datum>& feature_values,
    const std::vector<Datum>& target_values,
    const std::map<std::string, std::string>& options) {

    ProfessionalFeatureImportance analysis;
    analysis.feature_name = feature_name;

    if (feature_values.size() != target_values.size() || feature_values.size() < 3) {
        return analysis;
    }

    // Extract numeric values
    std::vector<double> x_numeric, y_numeric;
    x_numeric.reserve(feature_values.size());
    y_numeric.reserve(target_values.size());

    for (size_t i = 0; i < feature_values.size(); ++i) {
        if (!feature_values[i].is_null() && !target_values[i].is_null() &&
            is_numeric_datum(feature_values[i]) && is_numeric_datum(target_values[i])) {
            x_numeric.push_back(get_double_from_datum(feature_values[i]));
            y_numeric.push_back(get_double_from_datum(target_values[i]));
        }
    }

    if (x_numeric.size() < 3) {
        return analysis;
    }

    // Detect feature type
    std::set<double> unique_x(x_numeric.begin(), x_numeric.end());
    analysis.feature_type = (unique_x.size() <= 10) ? "categorical" : "continuous";

    // Calculate various importance scores

    // 1. Pearson correlation
    double p_value, conf_lower, conf_upper;
    analysis.scores.pearson = std::abs(ProfessionalStatisticalCalculator::pearson_correlation(
        x_numeric, y_numeric, p_value, conf_lower, conf_upper));
    analysis.significance.pearson_significant = (p_value < 0.05);

    // 2. Spearman correlation
    double spearman_p;
    analysis.scores.spearman = std::abs(ProfessionalStatisticalCalculator::spearman_correlation(
        x_numeric, y_numeric, spearman_p));
    analysis.significance.spearman_significant = (spearman_p < 0.05);

    // 3. Mutual information
    analysis.scores.mutual_information = ProfessionalStatisticalCalculator::mutual_information(
        x_numeric, y_numeric, 20);

    // 4. Chi-square test (for categorical features)
    if (analysis.feature_type == "categorical" && unique_x.size() > 1) {
        // Discretize target for chi-square
        std::vector<double> y_sorted = y_numeric;
        std::sort(y_sorted.begin(), y_sorted.end());

        size_t num_bins = std::min(static_cast<size_t>(5), y_numeric.size() / 10);
        if (num_bins < 2) num_bins = 2;

        // Create contingency table
        std::vector<std::vector<double>> contingency(unique_x.size(),
                                                    std::vector<double>(num_bins, 0.0));

        std::map<double, size_t> x_index_map;
        size_t idx = 0;
        for (double val : unique_x) {
            x_index_map[val] = idx++;
        }

        for (size_t i = 0; i < x_numeric.size(); ++i) {
            size_t x_idx = x_index_map[x_numeric[i]];

            // Find y bin
            size_t y_bin = 0;
            for (size_t b = 1; b < num_bins; ++b) {
                if (y_numeric[i] > y_sorted[(b * y_sorted.size()) / num_bins]) {
                    y_bin = b;
                }
            }

            contingency[x_idx][y_bin] += 1.0;
        }

        analysis.scores.chi_square = ProfessionalStatisticalCalculator::chi2_test(contingency);
        analysis.significance.chi2_significant = (analysis.scores.chi_square > 3.84); // p < 0.05 for df=1
    }

    // 5. ANOVA F-test (for categorical features)
    if (analysis.feature_type == "categorical" && unique_x.size() > 1) {
        std::map<double, std::vector<double>> groups;
        for (size_t i = 0; i < x_numeric.size(); ++i) {
            groups[x_numeric[i]].push_back(y_numeric[i]);
        }

        std::vector<std::vector<double>> group_vectors;
        for (const auto& [_, values] : groups) {
            group_vectors.push_back(values);
        }

        analysis.scores.anova_f = ProfessionalStatisticalCalculator::anova(group_vectors);
        analysis.significance.anova_significant = (analysis.scores.anova_f > 4.0); // Simplified threshold
    }

    // 6. Simple random forest importance (simulated)
    // In production, this would use an actual random forest implementation
    {
        std::random_device rd;
        std::mt19937_64 gen(rd());
        std::uniform_real_distribution<> dis(0.0, 1.0);

        // Simulate random forest importance based on correlations
        double base_importance = std::max(analysis.scores.pearson,
                                         analysis.scores.mutual_information);
        analysis.scores.random_forest = base_importance * (0.8 + 0.4 * dis(gen));
        analysis.scores.xgboost = base_importance * (0.7 + 0.5 * dis(gen));
        analysis.scores.lightgbm = base_importance * (0.75 + 0.45 * dis(gen));
    }

    // 7. Lasso coefficient (simulated with simple linear regression)
    {
        // Simple OLS coefficient as proxy for lasso
        double sum_x = 0.0, sum_y = 0.0, sum_xy = 0.0, sum_xx = 0.0;
        for (size_t i = 0; i < x_numeric.size(); ++i) {
            sum_x += x_numeric[i];
            sum_y += y_numeric[i];
            sum_xy += x_numeric[i] * y_numeric[i];
            sum_xx += x_numeric[i] * x_numeric[i];
        }

        double n = x_numeric.size();
        double b = (n * sum_xy - sum_x * sum_y) / (n * sum_xx - sum_x * sum_x);
        analysis.scores.lasso_coefficient = std::abs(b);
    }

    // 8. SHAP values (simulated)
    {
        // Simulate SHAP values based on feature variance and correlation
        double x_mean = ProfessionalStatisticalCalculator::calculate_mean(x_numeric);
        double x_var = ProfessionalStatisticalCalculator::calculate_variance(x_numeric);
        double y_var = ProfessionalStatisticalCalculator::calculate_variance(y_numeric);

        if (x_var > 0 && y_var > 0) {
            analysis.scores.shap = analysis.scores.pearson * std::sqrt(x_var / y_var);

            // Generate simulated SHAP values
            analysis.shapley.mean_abs_shap = analysis.scores.shap;
            analysis.shapley.std_shap = analysis.scores.shap * 0.3;
            analysis.shapley.min_shap = analysis.scores.shap * 0.5;
            analysis.shapley.max_shap = analysis.scores.shap * 1.5;

            analysis.shapley.shap_values.resize(std::min(x_numeric.size(), (size_t)1000));
            std::generate(analysis.shapley.shap_values.begin(),
                         analysis.shapley.shap_values.end(),
                         [&]() {
                             std::normal_distribution<> nd(analysis.shapley.mean_abs_shap,
                                                          analysis.shapley.std_shap);
                             std::random_device rd;
                             std::mt19937 gen(rd());
                             return std::abs(nd(gen));
                         });
        }
    }

    // 9. Other importance measures (simulated for demonstration)
    analysis.scores.permutation = analysis.scores.pearson * 0.9;
    analysis.scores.boruta = analysis.scores.mutual_information * 0.85;
    analysis.scores.relief = analysis.scores.pearson * 0.8;
    analysis.scores.mrmr = analysis.scores.mutual_information * 0.75;

    // Determine best p-value
    std::vector<std::pair<double, std::string>> p_values = {
        {p_value, "pearson"},
        {spearman_p, "spearman"},
        {0.01, "mutual_information"}, // Placeholder
        {0.05, "chi_square"}, // Placeholder
        {0.05, "anova"} // Placeholder
    };

    auto best_p = std::min_element(p_values.begin(), p_values.end(),
                                  [](const auto& a, const auto& b) { return a.first < b.first; });

    if (best_p != p_values.end()) {
        analysis.significance.best_p_value = best_p->first;
        analysis.significance.best_method = best_p->second;
    }

    // Calculate non-linear effects
    analysis.maximal_information_coefficient = ProfessionalStatisticalCalculator::maximal_information_coefficient(
        x_numeric, y_numeric);
    analysis.distance_correlation = ProfessionalStatisticalCalculator::distance_correlation(
        x_numeric, y_numeric);
    analysis.hoeffding_d = ProfessionalStatisticalCalculator::hoeffding_d(x_numeric, y_numeric);

    // Generate transformation suggestions
    if (analysis.feature_type == "continuous") {
        // Check if log transformation might help
        std::vector<double> log_x;
        bool all_positive = true;
        for (double val : x_numeric) {
            if (val <= 0) {
                all_positive = false;
                break;
            }
            log_x.push_back(std::log(val));
        }

        if (all_positive) {
            double log_corr = std::abs(ProfessionalStatisticalCalculator::pearson_correlation(
                log_x, y_numeric, p_value, conf_lower, conf_upper));

            if (log_corr > analysis.scores.pearson * 1.1) {
                analysis.transformation_suggestions.push_back("log transformation");
            }
        }

        // Check for skewness
        double skewness = ProfessionalStatisticalCalculator::calculate_skewness(x_numeric);
        if (std::abs(skewness) > 1.0) {
            analysis.transformation_suggestions.push_back("consider normalization due to high skewness");
        }
    } else if (analysis.feature_type == "categorical") {
        if (unique_x.size() > 5) {
            analysis.encoding_suggestions.push_back("one-hot encoding");
        } else {
            analysis.encoding_suggestions.push_back("label encoding");
        }

        // Check for rare categories
        std::map<double, size_t> counts;
        for (double val : x_numeric) {
            counts[val]++;
        }

        size_t total = x_numeric.size();
        for (const auto& [val, count] : counts) {
            if (count < total * 0.01) { // Less than 1%
                analysis.transformation_suggestions.push_back("group rare categories");
                break;
            }
        }
    }

    // Generate interaction suggestions based on feature name patterns
    std::vector<std::string> common_prefixes = {"age", "income", "price", "score", "rate"};
    for (const std::string& prefix : common_prefixes) {
        if (feature_name.find(prefix) != std::string::npos) {
            analysis.interaction_suggestions.push_back(prefix + "_squared");
            analysis.interaction_suggestions.push_back(prefix + "_log");
            break;
        }
    }

    // Determine recommended action
    double importance_score = std::max({analysis.scores.random_forest,
                                       analysis.scores.mutual_information,
                                       analysis.scores.pearson});

    if (importance_score > 0.3) {
        if (!analysis.transformation_suggestions.empty()) {
            analysis.recommended_action = ProfessionalFeatureImportance::TRANSFORM;
        } else if (!analysis.interaction_suggestions.empty()) {
            analysis.recommended_action = ProfessionalFeatureImportance::CREATE_INTERACTION;
        } else {
            analysis.recommended_action = ProfessionalFeatureImportance::KEEP_AS_IS;
        }
    } else if (importance_score > 0.1) {
        analysis.recommended_action = ProfessionalFeatureImportance::MONITOR;
    } else {
        analysis.recommended_action = ProfessionalFeatureImportance::DROP;
    }

    // Generate partial dependence data (simplified)
    if (analysis.feature_type == "continuous" && x_numeric.size() >= 100) {
        // Sort and bin x values
        std::vector<double> x_sorted = x_numeric;
        std::sort(x_sorted.begin(), x_sorted.end());

        size_t num_bins = 20;
        for (size_t i = 0; i < num_bins; ++i) {
            size_t idx = (i * x_sorted.size()) / num_bins;
            if (idx < x_sorted.size()) {
                double x_val = x_sorted[idx];

                // Calculate average y for x values near this bin
                double lower = (i > 0) ? x_sorted[((i-1) * x_sorted.size()) / num_bins] : x_val;
                double upper = (i < num_bins-1) ? x_sorted[((i+1) * x_sorted.size()) / num_bins] : x_val;

                double sum_y = 0.0;
                size_t count = 0;
                for (size_t j = 0; j < x_numeric.size(); ++j) {
                    if (x_numeric[j] >= lower && x_numeric[j] <= upper) {
                        sum_y += y_numeric[j];
                        count++;
                    }
                }

                if (count > 0) {
                    analysis.partial_dependence.emplace_back(x_val, sum_y / count);
                }
            }
        }
    }



    // Business impact scoring
    analysis.business_value_score = importance_score;
    analysis.data_collection_cost = (analysis.feature_type == "categorical") ? 0.3 : 0.5;
    analysis.feature_stability_score = 0.7 + 0.3 * dis(gen); // Random stability
    analysis.feature_freshness_score = 0.8 + 0.2 * dis(gen); // Random freshness

    return analysis;
}

std::vector<ProfessionalClusterAnalysis> ProfessionalDataAnalyzer::perform_clustering_professional(
    const std::vector<std::unordered_map<std::string, Datum>>& data,
    const std::vector<std::string>& feature_columns,
    const std::map<std::string, std::string>& options) {

    std::vector<ProfessionalClusterAnalysis> clusters;

    if (data.empty() || feature_columns.empty()) {
        return clusters;
    }

    // Extract numeric data matrix
    std::vector<std::vector<double>> data_matrix;
    data_matrix.reserve(data.size());

    for (const auto& row : data) {
        std::vector<double> point;
        point.reserve(feature_columns.size());

        bool valid_point = true;
        for (const std::string& feature : feature_columns) {
            auto it = row.find(feature);
            if (it == row.end() || it->second.is_null() || !is_numeric_datum(it->second))  {
                valid_point = false;
                break;
            }
            point.push_back(get_double_from_datum(it->second));
        }

        if (valid_point && !point.empty()) {
            data_matrix.push_back(point);
        }
    }

    if (data_matrix.size() < 3) {
        return clusters;
    }

    // Determine optimal number of clusters (elbow method)
    size_t max_clusters = std::min(config_.max_clusters, data_matrix.size() / 10);
    if (max_clusters < 2) max_clusters = 2;

    // Simple k-means clustering implementation
    size_t n_clusters = 3; // Default
    if (options.count("n_clusters")) {
        try {
            n_clusters = std::stoul(options.at("n_clusters"));
            n_clusters = std::min(n_clusters, max_clusters);
            n_clusters = std::max(n_clusters, (size_t)2);
        } catch (...) {
            // Use default
        }
    } else {
        // Use elbow method to determine optimal k
        std::vector<double> wcss_values; // Within-cluster sum of squares
        for (size_t k = 1; k <= std::min((size_t)10, max_clusters); ++k) {
            auto [centroids, labels] = perform_kmeans(data_matrix, k, 100);
            double wcss = calculate_wcss(data_matrix, centroids, labels);
            wcss_values.push_back(wcss);
        }

        // Find elbow point
        n_clusters = find_elbow_point(wcss_values);
        n_clusters = std::max((size_t)2, std::min(n_clusters, max_clusters));
    }

    // Perform k-means clustering
    auto [centroids, labels] = perform_kmeans(data_matrix, n_clusters, 100);

    // Create cluster analyses
    clusters.resize(n_clusters);
    std::vector<std::vector<size_t>> cluster_indices(n_clusters);

    // Count points per cluster
    for (size_t i = 0; i < labels.size(); ++i) {
        cluster_indices[labels[i]].push_back(i);
    }

    // Analyze each cluster
    for (size_t cluster_id = 0; cluster_id < n_clusters; ++cluster_id) {
        ProfessionalClusterAnalysis& cluster = clusters[cluster_id];
        cluster.cluster_id = cluster_id;
        cluster.cluster_label = "Cluster " + std::to_string(cluster_id + 1);
        cluster.size = cluster_indices[cluster_id].size();
        cluster.size_percentage = (cluster.size * 100.0) / data_matrix.size();

        // Store centroid
        cluster.centroid = centroids[cluster_id];

        // Calculate centroid standard deviation
        if (cluster.size > 1) {
            cluster.centroid_std.resize(feature_columns.size(), 0.0);
            for (size_t dim = 0; dim < feature_columns.size(); ++dim) {
                double sum = 0.0, sum_sq = 0.0;
                for (size_t point_idx : cluster_indices[cluster_id]) {
                    double val = data_matrix[point_idx][dim];
                    sum += val;
                    sum_sq += val * val;
                }
                double mean = sum / cluster.size;
                double variance = (sum_sq / cluster.size) - (mean * mean);
                cluster.centroid_std[dim] = std::sqrt(std::max(0.0, variance));
            }
        }

        // Find medoid (point closest to centroid)
        if (!cluster_indices[cluster_id].empty()) {
            size_t medoid_idx = cluster_indices[cluster_id][0];
            double min_distance = std::numeric_limits<double>::max();

            for (size_t point_idx : cluster_indices[cluster_id]) {
                double distance = 0.0;
                for (size_t dim = 0; dim < feature_columns.size(); ++dim) {
                    double diff = data_matrix[point_idx][dim] - cluster.centroid[dim];
                    distance += diff * diff;
                }
                distance = std::sqrt(distance);

                if (distance < min_distance) {
                    min_distance = distance;
                    medoid_idx = point_idx;
                }
            }

            cluster.medoid = data_matrix[medoid_idx];
        }

        // Find defining features (features with largest deviation from global mean)
        std::vector<double> global_means(feature_columns.size(), 0.0);
        for (const auto& point : data_matrix) {
            for (size_t dim = 0; dim < feature_columns.size(); ++dim) {
                global_means[dim] += point[dim];
            }
        }
        for (size_t dim = 0; dim < feature_columns.size(); ++dim) {
            global_means[dim] /= data_matrix.size();
        }

        std::vector<std::pair<size_t, double>> feature_deviations;
        for (size_t dim = 0; dim < feature_columns.size(); ++dim) {
            double deviation = std::abs(cluster.centroid[dim] - global_means[dim]);
            if (cluster.centroid_std.size() > dim && cluster.centroid_std[dim] > 0) {
                deviation /= cluster.centroid_std[dim]; // Standardized deviation
            }
            feature_deviations.emplace_back(dim, deviation);
        }

        // Sort by deviation (descending)
        std::sort(feature_deviations.begin(), feature_deviations.end(),
                 [](const auto& a, const auto& b) { return a.second > b.second; });

        // Take top 3 defining features
        for (size_t i = 0; i < std::min((size_t)3, feature_deviations.size()); ++i) {
            size_t feature_idx = feature_deviations[i].first;
            if (feature_idx < feature_columns.size()) {
                cluster.defining_features.push_back(feature_columns[feature_idx]);
                cluster.feature_importances.emplace_back(
                    feature_columns[feature_idx], feature_deviations[i].second);
            }
        }

        // Calculate cluster quality metrics
        calculate_cluster_quality(cluster, data_matrix, cluster_indices[cluster_id], centroids);

        // Generate business interpretation
        generate_cluster_interpretation(cluster, feature_columns);

        // Store member points (limited to avoid memory issues)
        size_t max_points = std::min(cluster.size, (size_t)1000);
        cluster.member_points.reserve(max_points);
        for (size_t i = 0; i < max_points; ++i) {
            size_t point_idx = cluster_indices[cluster_id][i % cluster.size];
            cluster.member_points.push_back(data_matrix[point_idx]);
        }
    }

    // Calculate inter-cluster distances
    for (size_t i = 0; i < n_clusters; ++i) {
        for (size_t j = i + 1; j < n_clusters; ++j) {
            double distance = 0.0;
            for (size_t dim = 0; dim < feature_columns.size(); ++dim) {
                double diff = clusters[i].centroid[dim] - clusters[j].centroid[dim];
                distance += diff * diff;
            }
            distance = std::sqrt(distance);

            clusters[i].distances_to_other_clusters.push_back(distance);
            clusters[j].distances_to_other_clusters.push_back(distance);
        }

        if (!clusters[i].distances_to_other_clusters.empty()) {
            clusters[i].minimum_inter_cluster_distance = *std::min_element(
                clusters[i].distances_to_other_clusters.begin(),
                clusters[i].distances_to_other_clusters.end());

            clusters[i].average_inter_cluster_distance =
                std::accumulate(clusters[i].distances_to_other_clusters.begin(),
                              clusters[i].distances_to_other_clusters.end(), 0.0) /
                clusters[i].distances_to_other_clusters.size();
        }
    }

    return clusters;
}

ProfessionalOutlierAnalysis ProfessionalDataAnalyzer::detect_outliers_professional(
    const std::vector<std::unordered_map<std::string, Datum>>& data,
    const std::vector<std::string>& feature_columns,
    const std::map<std::string, std::string>& options) {

    ProfessionalOutlierAnalysis analysis;

    if (data.empty() || feature_columns.empty()) {
        return analysis;
    }

    // Extract numeric data matrix
    std::vector<std::vector<double>> data_matrix;
    std::vector<size_t> valid_indices;

    for (size_t i = 0; i < data.size(); ++i) {
        const auto& row = data[i];
        std::vector<double> point;
        point.reserve(feature_columns.size());

        bool valid_point = true;
        for (const std::string& feature : feature_columns) {
            auto it = row.find(feature);
            if (it == row.end() || it->second.is_null() || !is_numeric_datum(it->second)) {
                valid_point = false;
                break;
            }
            point.push_back(get_double_from_datum(it->second));
        }

        if (valid_point && !point.empty()) {
            data_matrix.push_back(point);
            valid_indices.push_back(i);
        }
    }

    if (data_matrix.size() < 10) {
        return analysis;
    }

    // Multiple outlier detection methods

    // 1. Mahalanobis distance (multivariate)
    std::vector<double> mahalanobis_distances = calculate_mahalanobis_distances(data_matrix);

    // 2. Isolation Forest (simplified)
    std::vector<double> isolation_scores = calculate_isolation_scores(data_matrix);

    // 3. Local Outlier Factor (LOF) (simplified)
    std::vector<double> lof_scores = calculate_lof_scores(data_matrix);

    // Combine scores
    std::vector<double> combined_scores(data_matrix.size(), 0.0);
    for (size_t i = 0; i < data_matrix.size(); ++i) {
        // Normalize each score to [0, 1]
        double mahalanobis_norm = normalize_score(mahalanobis_distances[i], mahalanobis_distances);
        double isolation_norm = normalize_score(isolation_scores[i], isolation_scores);
        double lof_norm = normalize_score(lof_scores[i], lof_scores);

        // Weighted combination
        combined_scores[i] = 0.4 * mahalanobis_norm + 0.3 * isolation_norm + 0.3 * lof_norm;
    }

    // Determine threshold
    double contamination = 0.05; // Default 5%
    if (options.count("contamination_threshold")) {
        try {
            contamination = std::stod(options.at("contamination_threshold"));
            contamination = std::max(0.01, std::min(0.5, contamination));
        } catch (...) {
            // Use default
        }
    }

    // Find threshold (top contamination percentage)
    std::vector<double> sorted_scores = combined_scores;
    std::sort(sorted_scores.begin(), sorted_scores.end(), std::greater<double>());
    size_t outlier_count = std::min(static_cast<size_t>(contamination * combined_scores.size()),
                                   combined_scores.size());
    double threshold = (outlier_count > 0) ? sorted_scores[outlier_count - 1] : 0.0;

    // Identify outliers
    for (size_t i = 0; i < combined_scores.size(); ++i) {
        if (combined_scores[i] >= threshold) {
            analysis.outlier_indices.push_back(valid_indices[i]);
            analysis.outlier_scores.push_back(combined_scores[i]);

            // Determine outlier type
            if (mahalanobis_distances[i] > calculate_mahalanobis_threshold(mahalanobis_distances)) {
                analysis.outlier_types.push_back("global");
            } else if (lof_scores[i] > calculate_lof_threshold(lof_scores)) {
                analysis.outlier_types.push_back("local");
            } else {
                analysis.outlier_types.push_back("collective");
            }

            analysis.detection_methods.push_back("combined");
        }
    }

    // Classify severity
    if (!analysis.outlier_scores.empty()) {
        std::vector<double> sorted_outlier_scores = analysis.outlier_scores;
        std::sort(sorted_outlier_scores.begin(), sorted_outlier_scores.end(), std::greater<double>());

        size_t quarter = sorted_outlier_scores.size() / 4;

        for (size_t i = 0; i < analysis.outlier_scores.size(); ++i) {
            if (analysis.outlier_scores[i] >= sorted_outlier_scores[0]) {
                analysis.severity.critical_count++;
                analysis.severity.critical_indices.push_back(analysis.outlier_indices[i]);
            } else if (analysis.outlier_scores[i] >= sorted_outlier_scores[quarter]) {
                analysis.severity.high_count++;
                analysis.severity.high_indices.push_back(analysis.outlier_indices[i]);
            } else if (analysis.outlier_scores[i] >= sorted_outlier_scores[2 * quarter]) {
                analysis.severity.medium_count++;
                analysis.severity.medium_indices.push_back(analysis.outlier_indices[i]);
            } else {
                analysis.severity.low_count++;
                analysis.severity.low_indices.push_back(analysis.outlier_indices[i]);
            }
        }
    }

    // Root cause analysis
    if (!analysis.outlier_indices.empty() && !feature_columns.empty()) {
        // Find features that contribute most to outliers
        std::vector<double> feature_contributions(feature_columns.size(), 0.0);

        // Calculate mean and std for each feature
        std::vector<double> feature_means(feature_columns.size(), 0.0);
        std::vector<double> feature_stds(feature_columns.size(), 0.0);

        for (size_t dim = 0; dim < feature_columns.size(); ++dim) {
            double sum = 0.0, sum_sq = 0.0;
            for (const auto& point : data_matrix) {
                sum += point[dim];
                sum_sq += point[dim] * point[dim];
            }
            feature_means[dim] = sum / data_matrix.size();
            double variance = (sum_sq / data_matrix.size()) - (feature_means[dim] * feature_means[dim]);
            feature_stds[dim] = std::sqrt(std::max(0.0, variance));
        }

        // Calculate how much each outlier deviates in each dimension
        for (size_t outlier_idx = 0; outlier_idx < analysis.outlier_indices.size(); ++outlier_idx) {
            size_t data_idx = std::distance(valid_indices.begin(),
                                          std::find(valid_indices.begin(), valid_indices.end(),
                                                   analysis.outlier_indices[outlier_idx]));

            if (data_idx < data_matrix.size()) {
                const auto& point = data_matrix[data_idx];
                for (size_t dim = 0; dim < feature_columns.size(); ++dim) {
                    if (feature_stds[dim] > 0) {
                        double z_score = std::abs((point[dim] - feature_means[dim]) / feature_stds[dim]);
                        feature_contributions[dim] += z_score;
                    }
                }
            }
        }

        // Normalize contributions
        double total_contribution = std::accumulate(feature_contributions.begin(),
                                                   feature_contributions.end(), 0.0);
        if (total_contribution > 0) {
            for (size_t dim = 0; dim < feature_columns.size(); ++dim) {
                feature_contributions[dim] /= total_contribution;
            }
        }

        // Find top contributing features
        std::vector<std::pair<size_t, double>> sorted_contributions;
        for (size_t dim = 0; dim < feature_columns.size(); ++dim) {
            sorted_contributions.emplace_back(dim, feature_contributions[dim]);
        }

        std::sort(sorted_contributions.begin(), sorted_contributions.end(),
                 [](const auto& a, const auto& b) { return a.second > b.second; });

        // Take top 3 contributing features
        for (size_t i = 0; i < std::min((size_t)3, sorted_contributions.size()); ++i) {
            size_t feature_idx = sorted_contributions[i].first;
            analysis.root_cause.contributing_features.push_back(feature_columns[feature_idx]);
            analysis.root_cause.feature_contributions.push_back(sorted_contributions[i].second);
        }

        // Determine likely cause
        analysis.root_cause.likely_cause = determine_likely_cause(analysis, data_matrix, feature_columns);
        analysis.root_cause.confidence = 0.7 + 0.3 * (std::rand() / (double)RAND_MAX); // Random confidence
    }

    // Impact analysis
    if (!analysis.outlier_indices.empty() && data_matrix.size() > 10) {
        // Calculate impact on statistics with and without outliers
        std::vector<std::vector<double>> inlier_matrix;
        for (size_t i = 0; i < data_matrix.size(); ++i) {
            if (std::find(analysis.outlier_indices.begin(), analysis.outlier_indices.end(),
                         valid_indices[i]) == analysis.outlier_indices.end()) {
                inlier_matrix.push_back(data_matrix[i]);
            }
        }

        if (!inlier_matrix.empty()) {
            // Impact on means
            for (size_t dim = 0; dim < feature_columns.size(); ++dim) {
                double mean_with = 0.0, mean_without = 0.0;
                for (const auto& point : data_matrix) mean_with += point[dim];
                for (const auto& point : inlier_matrix) mean_without += point[dim];

                mean_with /= data_matrix.size();
                mean_without /= inlier_matrix.size();

                analysis.impact.on_mean += std::abs(mean_with - mean_without) /
                                          (std::abs(mean_without) + 1e-10);
            }
            analysis.impact.on_mean /= feature_columns.size();

            // Impact on variance (simplified)
            analysis.impact.on_variance = calculate_variance_impact(data_matrix, inlier_matrix);
        }
    }

    // Generate recommendations
    generate_outlier_recommendations(analysis, data_matrix.size());

    // Store multivariate analysis data
    analysis.multivariate.affected_dimensions = analysis.root_cause.contributing_features;
    analysis.multivariate.mahalanobis_distances = mahalanobis_distances;
    analysis.multivariate.cook_distances = calculate_cooks_distances(data_matrix); // Simplified
    analysis.multivariate.leverage_scores = calculate_leverage_scores(data_matrix);
    analysis.multivariate.influence_scores = combined_scores;

    // Store visualization data
    if (feature_columns.size() >= 2 && data_matrix.size() <= 1000) {
        // Store all points for visualization
        for (size_t i = 0; i < data_matrix.size(); ++i) {
            if (std::find(analysis.outlier_indices.begin(), analysis.outlier_indices.end(),
                         valid_indices[i]) != analysis.outlier_indices.end()) {
                analysis.outlier_points.push_back(data_matrix[i]);
            } else {
                analysis.inlier_points.push_back(data_matrix[i]);
            }
        }
    }

    return analysis;
}

ProfessionalDistributionAnalysis ProfessionalDataAnalyzer::analyze_distribution_professional(
    const std::string& column_name,
    const std::vector<Datum>& values,
    const std::map<std::string, std::string>& options) {

    ProfessionalDistributionAnalysis analysis;
    analysis.column_name = column_name;

    // Extract numeric values
    std::vector<double> numeric_values;
    for (const auto& value : values) {
        if (!value.is_null() && is_numeric_datum(value)) {
            numeric_values.push_back(get_double_from_datum(value));
        }
    }

    if (numeric_values.empty()) {
        return analysis;
    }

    // Fit multiple distributions
    auto fitted_dists = ProfessionalStatisticalCalculator::fit_multiple_distributions(numeric_values);
    analysis.fitted_distributions.clear();
    analysis.fitted_distributions.reserve(fitted_dists.size());

    for (const auto& dist : fitted_dists) {
        ProfessionalDistributionAnalysis::FittedDistribution fd;
        fd.name = dist.name;
        fd.parameters = dist.parameters;
        fd.log_likelihood = dist.log_likelihood;
        fd.aic = dist.aic;
        fd.bic = dist.bic;
        fd.ks_statistic = dist.ks_statistic;
        fd.ks_p_value = dist.ks_p_value;
        fd.ad_statistic = dist.ad_statistic;
        fd.ad_p_value = dist.ad_p_value;
        fd.cvm_statistic = dist.cvm_statistic;
        fd.cvm_p_value = dist.cvm_p_value;
        fd.chi2_statistic = dist.chi2_statistic;
        fd.chi2_p_value = dist.chi2_p_value;
        analysis.fitted_distributions.push_back(fd);
    }

    if (!analysis.fitted_distributions.empty()) {
        analysis.best_fit = analysis.fitted_distributions[0];
    }

    // Goodness of fit tests
    double p_value;
    analysis.goodness_of_fit.shapiro_wilk = ProfessionalStatisticalCalculator::shapiro_wilk(
        numeric_values, p_value);
    analysis.goodness_of_fit.shapiro_wilk_p = p_value;

    analysis.goodness_of_fit.jarque_bera = ProfessionalStatisticalCalculator::jarque_bera(
        numeric_values, p_value);
    analysis.goodness_of_fit.jarque_bera_p = p_value;

    // D'Agostino K test (simplified)
    double skewness = ProfessionalStatisticalCalculator::calculate_skewness(numeric_values);
    double kurtosis = ProfessionalStatisticalCalculator::calculate_kurtosis(numeric_values);
    analysis.goodness_of_fit.dagostino_k2 = skewness * skewness + kurtosis * kurtosis;
    analysis.goodness_of_fit.dagostino_k2_p = 1.0 - std::exp(-analysis.goodness_of_fit.dagostino_k2);

    analysis.goodness_of_fit.passes_normality = (
        analysis.goodness_of_fit.shapiro_wilk_p > 0.05 &&
        analysis.goodness_of_fit.jarque_bera_p > 0.05
    );

    // Moments
    analysis.moments.mean = ProfessionalStatisticalCalculator::calculate_mean(numeric_values);
    analysis.moments.variance = ProfessionalStatisticalCalculator::calculate_variance(numeric_values);
    analysis.moments.skewness = ProfessionalStatisticalCalculator::calculate_skewness(numeric_values);
    analysis.moments.kurtosis = ProfessionalStatisticalCalculator::calculate_kurtosis(numeric_values);
    analysis.moments.excess_kurtosis = analysis.moments.kurtosis - 3.0;

    // Calculate higher moments
    analysis.moments.central_moments = ProfessionalStatisticalCalculator::calculate_central_moments(
        numeric_values, 10);
    analysis.moments.standardized_moments = ProfessionalStatisticalCalculator::calculate_standardized_moments(
        numeric_values, 10);

    // L-moments
    analysis.l_moments.l1 = analysis.moments.mean;
    std::vector<double> l_moments = ProfessionalStatisticalCalculator::calculate_l_moments(numeric_values, 4);
    if (l_moments.size() >= 4) {
        analysis.l_moments.l2 = l_moments[1];
        analysis.l_moments.l3 = l_moments[2];
        analysis.l_moments.l4 = l_moments[3];

        if (analysis.l_moments.l2 > 0) {
            analysis.l_moments.l_cv = analysis.l_moments.l2 / analysis.l_moments.l1;
            analysis.l_moments.l_skew = analysis.l_moments.l3 / analysis.l_moments.l2;
            analysis.l_moments.l_kurt = analysis.l_moments.l4 / analysis.l_moments.l2;
        }
    }

    // Tail analysis
    perform_tail_analysis(analysis.tail_analysis, numeric_values);

    // Modality analysis
    perform_modality_analysis(analysis.modality, numeric_values);

    // Determine distribution properties
    determine_distribution_properties(analysis, numeric_values);

    // Transformation suggestions
    suggest_transformations(analysis.transformations, numeric_values);

    // Generate empirical CDF
    generate_empirical_cdf(analysis.empirical_cdf, numeric_values);

    // Generate theoretical CDF for best fit
    if (!analysis.best_fit.name.empty()) {
        ProfessionalStatisticalCalculator::DistributionFit dist_fit;
        dist_fit.name = analysis.best_fit.name;
        dist_fit.parameters = analysis.best_fit.parameters;
        dist_fit.log_likelihood = analysis.best_fit.log_likelihood;
        dist_fit.aic = analysis.best_fit.aic;
        dist_fit.bic = analysis.best_fit.bic;
        dist_fit.ks_statistic = analysis.best_fit.ks_statistic;
        dist_fit.ks_p_value = analysis.best_fit.ks_p_value;
        generate_theoretical_cdf(analysis.theoretical_cdf, numeric_values, dist_fit);
        //generate_theoretical_cdf(analysis.theoretical_cdf, numeric_values, analysis.best_fit);
    }

    // Generate Q-Q plot data
    generate_qq_plot_data(analysis.qq_plot_data, numeric_values);

    return analysis;
}

ProfessionalTimeSeriesAnalysis ProfessionalDataAnalyzer::analyze_time_series_professional(
    const std::string& timestamp_column,
    const std::string& value_column,
    const std::vector<Datum>& timestamps,
    const std::vector<Datum>& values,
    const std::map<std::string, std::string>& options) {

    ProfessionalTimeSeriesAnalysis analysis;
    analysis.timestamp_column = timestamp_column;
    analysis.value_column = value_column;

    if (timestamps.size() != values.size() || timestamps.size() < 10) {
        return analysis;
    }

    // Extract numeric values and timestamps
    std::vector<double> series_values;
    std::vector<time_t> series_timestamps;

    for (size_t i = 0; i < values.size(); ++i) {
        if (!values[i].is_null() && is_numeric_datum(values[i]) &&
            !timestamps[i].is_null()) {

            series_values.push_back(get_double_from_datum(values[i]));

            // Convert timestamp to time_t (simplified)
            // In production, use proper date/time parsing
            try {
                std::string ts_str = get_string_from_datum(timestamps[i]);
                // Simple parsing - adjust based on your timestamp format
                std::tm tm = {};
                std::istringstream ss(ts_str);
                ss >> std::get_time(&tm, "%Y-%m-%d %H:%M:%S");
                if (!ss.fail()) {
                    series_timestamps.push_back(std::mktime(&tm));
                } else {
                    // Use index as timestamp if parsing fails
                    series_timestamps.push_back(i);
                }
            } catch (...) {
                series_timestamps.push_back(i);
            }
        }
    }

    if (series_values.size() < 10) {
        return analysis;
    }

    // Stationarity tests
    double statistic, p_value;

    // ADF test
    analysis.stationarity.is_stationary_adf = ProfessionalStatisticalCalculator::adf_test(
        series_values, statistic, p_value);
    analysis.stationarity.adf_statistic = statistic;
    analysis.stationarity.adf_p_value = p_value;

    // KPSS test
    analysis.stationarity.is_stationary_kpss = ProfessionalStatisticalCalculator::kpss_test(
        series_values, statistic, p_value);
    analysis.stationarity.kpss_statistic = statistic;
    analysis.stationarity.kpss_p_value = p_value;

    // Determine differencing order
    if (!analysis.stationarity.is_stationary_adf) {
        // Try first difference
        std::vector<double> diff_values;
        for (size_t i = 1; i < series_values.size(); ++i) {
            diff_values.push_back(series_values[i] - series_values[i-1]);
        }

        bool diff_stationary = ProfessionalStatisticalCalculator::adf_test(
            diff_values, statistic, p_value);

        if (diff_stationary) {
            analysis.stationarity.differencing_order = 1;
        } else {
            analysis.stationarity.differencing_order = 2;
        }
    }

    // Decomposition
    perform_time_series_decomposition(analysis.decomposition, series_values, series_timestamps);

    // Autocorrelation analysis
    size_t max_lag = std::min((size_t)50, series_values.size() / 4);
    analysis.autocorrelation.acf_values = ProfessionalStatisticalCalculator::calculate_autocorrelation(
        series_values, max_lag);
    analysis.autocorrelation.pacf_values = ProfessionalStatisticalCalculator::calculate_partial_autocorrelation(
        series_values, max_lag);

    // Calculate confidence intervals for ACF/PACF
    analysis.autocorrelation.acf_confidence.resize(max_lag, 1.96 / std::sqrt(series_values.size()));
    analysis.autocorrelation.pacf_confidence.resize(max_lag, 1.96 / std::sqrt(series_values.size()));

    // Hurst exponent
    analysis.autocorrelation.hurst_exponent = ProfessionalStatisticalCalculator::calculate_hurst_exponent(
        series_values);
    analysis.autocorrelation.is_long_memory = (analysis.autocorrelation.hurst_exponent > 0.5);
    analysis.autocorrelation.is_short_memory = (analysis.autocorrelation.hurst_exponent < 0.5);

    // Lyapunov exponent (for chaos detection)
    analysis.autocorrelation.lyapunov_exponent = ProfessionalStatisticalCalculator::calculate_lyapunov_exponent(
        series_values);

    // Autocorrelation time
    analysis.autocorrelation.autocorrelation_time = calculate_autocorrelation_time(
        analysis.autocorrelation.acf_values);

    // Spectral analysis
    perform_spectral_analysis(analysis.spectral, series_values);

    // Volatility analysis
    perform_volatility_analysis(analysis.volatility, series_values);

    // Forecastability analysis
    perform_forecastability_analysis(analysis.forecastability, series_values);

    // Seasonality detection
    detect_seasonality(analysis.seasonality, series_values, series_timestamps);

    // Anomaly detection in time series
    detect_time_series_anomalies(analysis.anomalies, series_values);

    // Model suggestions
    suggest_time_series_models(analysis.model_suggestions, analysis);

    return analysis;
}

ProfessionalDataQualityReport ProfessionalDataAnalyzer::assess_data_quality_professional(
    const std::vector<std::unordered_map<std::string, Datum>>& data,
    const std::map<std::string, std::string>& options) {

    ProfessionalDataQualityReport report;

    if (data.empty()) {
        return report;
    }

    // Basic metadata
    report.metadata.row_count = data.size();
    report.metadata.column_count = data.empty() ? 0 : data[0].size();
    report.metadata.total_cells = report.metadata.row_count * report.metadata.column_count;

    // Get current timestamp
    auto now = std::chrono::system_clock::now();
    std::time_t now_time = std::chrono::system_clock::to_time_t(now);
    report.metadata.analysis_timestamp = std::ctime(&now_time);
    // Remove newline from ctime
    if (!report.metadata.analysis_timestamp.empty() &&
        report.metadata.analysis_timestamp.back() == '\n') {
        report.metadata.analysis_timestamp.pop_back();
    }

    // Extract column information
    std::vector<std::string> columns;
    if (!data.empty()) {
        for (const auto& [col_name, _] : data[0]) {
            columns.push_back(col_name);
        }
    }

    // Completeness analysis
    analyze_completeness(report.completeness, data, columns);

    // Consistency analysis
    analyze_consistency(report.consistency, data, columns);

    // Accuracy analysis (simplified - would require ground truth in production)
    analyze_accuracy(report.accuracy, data, columns);

    // Uniqueness analysis
    analyze_uniqueness(report.uniqueness, data);

    // Validity analysis
    analyze_validity(report.validity, data, columns);

    // Timeliness analysis (simplified)
    analyze_timeliness(report.timeliness, data);

    // Calculate scores
    calculate_data_quality_scores(report);

    // Identify issues
    identify_data_quality_issues(report.issues, report);

    // Generate recommendations
    generate_data_quality_recommendations(report.recommendations, report);

    return report;
}

// ============================================
// HELPER METHODS IMPLEMENTATION
// ============================================

// K-means clustering implementation
std::pair<std::vector<std::vector<double>>, std::vector<size_t>>
ProfessionalDataAnalyzer::perform_kmeans(const std::vector<std::vector<double>>& data,
                                        size_t k,
                                        size_t max_iterations) {

    std::vector<std::vector<double>> centroids(k);
    std::vector<size_t> labels(data.size(), 0);

    if (data.empty() || k == 0 || k > data.size()) {
        return {centroids, labels};
    }

    // Initialize centroids using k-means++
    centroids[0] = data[std::rand() % data.size()];

    for (size_t i = 1; i < k; ++i) {
        std::vector<double> distances(data.size(), std::numeric_limits<double>::max());

        for (size_t j = 0; j < data.size(); ++j) {
            for (size_t c = 0; c < i; ++c) {
                double dist = 0.0;
                for (size_t d = 0; d < data[j].size(); ++d) {
                    double diff = data[j][d] - centroids[c][d];
                    dist += diff * diff;
                }
                distances[j] = std::min(distances[j], dist);
            }
        }

        // Choose next centroid with probability proportional to distance^2
        double total_distance = std::accumulate(distances.begin(), distances.end(), 0.0);
        double random_value = (std::rand() / (double)RAND_MAX) * total_distance;

        double cumulative = 0.0;
        for (size_t j = 0; j < data.size(); ++j) {
            cumulative += distances[j];
            if (cumulative >= random_value) {
                centroids[i] = data[j];
                break;
            }
        }
    }

    // K-means iterations
    for (size_t iter = 0; iter < max_iterations; ++iter) {
        // Assign points to nearest centroid
        bool changed = false;
        for (size_t i = 0; i < data.size(); ++i) {
            size_t best_cluster = 0;
            double best_distance = std::numeric_limits<double>::max();

            for (size_t c = 0; c < k; ++c) {
                double distance = 0.0;
                for (size_t d = 0; d < data[i].size(); ++d) {
                    double diff = data[i][d] - centroids[c][d];
                    distance += diff * diff;
                }

                if (distance < best_distance) {
                    best_distance = distance;
                    best_cluster = c;
                }
            }

            if (labels[i] != best_cluster) {
                labels[i] = best_cluster;
                changed = true;
            }
        }

        if (!changed) {
            break;
        }

        // Update centroids
        std::vector<std::vector<double>> new_centroids(k, std::vector<double>(data[0].size(), 0.0));
        std::vector<size_t> counts(k, 0);

        for (size_t i = 0; i < data.size(); ++i) {
            size_t cluster = labels[i];
            counts[cluster]++;
            for (size_t d = 0; d < data[i].size(); ++d) {
                new_centroids[cluster][d] += data[i][d];
            }
        }

        for (size_t c = 0; c < k; ++c) {
            if (counts[c] > 0) {
                for (size_t d = 0; d < data[0].size(); ++d) {
                    new_centroids[c][d] /= counts[c];
                }
            } else {
                // Re-initialize empty cluster
                new_centroids[c] = data[std::rand() % data.size()];
            }
        }

        centroids = new_centroids;
    }

    return {centroids, labels};
}

double ProfessionalDataAnalyzer::calculate_wcss(const std::vector<std::vector<double>>& data,
                                               const std::vector<std::vector<double>>& centroids,
                                               const std::vector<size_t>& labels) {

    double wcss = 0.0;
    for (size_t i = 0; i < data.size(); ++i) {
        size_t cluster = labels[i];
        double distance = 0.0;
        for (size_t d = 0; d < data[i].size(); ++d) {
            double diff = data[i][d] - centroids[cluster][d];
            distance += diff * diff;
        }
        wcss += distance;
    }

    return wcss;
}

size_t ProfessionalDataAnalyzer::find_elbow_point(const std::vector<double>& wcss_values) {
    if (wcss_values.size() < 3) {
        return 2;
    }

    // Find point with maximum curvature
    size_t elbow_point = 2;
    double max_curvature = 0.0;

    for (size_t i = 1; i < wcss_values.size() - 1; ++i) {
        double prev = wcss_values[i-1];
        double curr = wcss_values[i];
        double next = wcss_values[i+1];

        // Calculate approximate second derivative (curvature)
        double curvature = std::abs(next - 2*curr + prev);

        if (curvature > max_curvature) {
            max_curvature = curvature;
            elbow_point = i + 1; // +1 because k starts at 1
        }
    }

    return elbow_point;
}

void ProfessionalDataAnalyzer::calculate_cluster_quality(
    ProfessionalClusterAnalysis& cluster,
    const std::vector<std::vector<double>>& data,
    const std::vector<size_t>& cluster_indices,
    const std::vector<std::vector<double>>& centroids) {

    if (cluster_indices.empty()) {
        return;
    }

    // Calculate within-cluster sum of squares
    cluster.within_cluster_sum_of_squares = 0.0;
    for (size_t idx : cluster_indices) {
        double distance = 0.0;
        for (size_t d = 0; d < data[idx].size(); ++d) {
            double diff = data[idx][d] - cluster.centroid[d];
            distance += diff * diff;
        }
        cluster.within_cluster_sum_of_squares += distance;
    }

    // Calculate intra-cluster distances
    std::vector<double> intra_distances;
    for (size_t i = 0; i < cluster_indices.size(); ++i) {
        for (size_t j = i + 1; j < cluster_indices.size(); ++j) {
            double distance = 0.0;
            for (size_t d = 0; d < data[cluster_indices[i]].size(); ++d) {
                double diff = data[cluster_indices[i]][d] - data[cluster_indices[j]][d];
                distance += diff * diff;
            }
            intra_distances.push_back(std::sqrt(distance));
        }
    }

    if (!intra_distances.empty()) {
        cluster.average_intra_cluster_distance = std::accumulate(
            intra_distances.begin(), intra_distances.end(), 0.0) / intra_distances.size();
        cluster.maximum_intra_cluster_distance = *std::max_element(
            intra_distances.begin(), intra_distances.end());
        cluster.diameter = cluster.maximum_intra_cluster_distance;
    }

    // Calculate silhouette score (simplified)
    if (centroids.size() > 1) {
        // Calculate average distance to own cluster (a)
        double a = cluster.average_intra_cluster_distance;

        // Calculate average distance to nearest other cluster (b)
        double min_inter_distance = std::numeric_limits<double>::max();
        for (size_t other_cluster = 0; other_cluster < centroids.size(); ++other_cluster) {
            if (other_cluster != cluster.cluster_id) {
                double distance = 0.0;
                for (size_t d = 0; d < centroids[cluster.cluster_id].size(); ++d) {
                    double diff = centroids[cluster.cluster_id][d] - centroids[other_cluster][d];
                    distance += diff * diff;
                }
                min_inter_distance = std::min(min_inter_distance, std::sqrt(distance));
            }
        }

        if (std::max(a, min_inter_distance) > 0) {
            cluster.silhouette_score = (min_inter_distance - a) / std::max(a, min_inter_distance);
        }
    }

    // Calculate density
    if (cluster.diameter > 0) {
        cluster.density = cluster.size / (cluster.diameter * cluster.diameter * 3.14159);
        cluster.sparsity = 1.0 / (cluster.density + 1.0);
    }
}

void ProfessionalDataAnalyzer::generate_cluster_interpretation(
    ProfessionalClusterAnalysis& cluster,
    const std::vector<std::string>& feature_columns) {

    if (cluster.defining_features.empty()) {
        return;
    }

    std::stringstream interpretation;
    interpretation << "This cluster represents ";

    if (cluster.size_percentage > 30.0) {
        interpretation << "a large segment (" << std::fixed << std::setprecision(1)
                      << cluster.size_percentage << "%) ";
    } else if (cluster.size_percentage > 10.0) {
        interpretation << "a medium-sized segment (" << std::fixed << std::setprecision(1)
                      << cluster.size_percentage << "%) ";
    } else {
        interpretation << "a small niche segment (" << std::fixed << std::setprecision(1)
                      << cluster.size_percentage << "%) ";
    }

    interpretation << "characterized by ";

    for (size_t i = 0; i < cluster.defining_features.size(); ++i) {
        if (i > 0) {
            if (i == cluster.defining_features.size() - 1) {
                interpretation << " and ";
            } else {
                interpretation << ", ";
            }
        }

        // Find feature index
        auto it = std::find(feature_columns.begin(), feature_columns.end(),
                           cluster.defining_features[i]);
        if (it != feature_columns.end()) {
            size_t feature_idx = std::distance(feature_columns.begin(), it);
            if (feature_idx < cluster.centroid.size()) {
                double value = cluster.centroid[feature_idx];
                interpretation << cluster.defining_features[i] << " around "
                             << std::fixed << std::setprecision(2) << value;
            }
        }
    }

    interpretation << ".";

    cluster.business_interpretation = interpretation.str();

    // Generate key characteristics
    cluster.key_characteristics.push_back(
        "Size: " + std::to_string(cluster.size) + " points (" +
        std::to_string(static_cast<int>(cluster.size_percentage)) + "%)");

    if (cluster.silhouette_score > 0.5) {
        cluster.key_characteristics.push_back("Well-separated from other clusters");
    } else if (cluster.silhouette_score > 0) {
        cluster.key_characteristics.push_back("Reasonably distinct from other clusters");
    } else {
        cluster.key_characteristics.push_back("Poorly separated from other clusters");
    }

    // Generate action items
    if (cluster.size_percentage < 5.0) {
        cluster.action_items.push_back("Consider merging with similar small clusters");
    } else if (cluster.size_percentage > 40.0) {
        cluster.action_items.push_back("Consider splitting this large cluster for better granularity");
    }

    if (cluster.silhouette_score < 0) {
        cluster.action_items.push_back("Review cluster assignment - may need re-clustering");
    }
}

// Outlier detection helper methods
std::vector<double> ProfessionalDataAnalyzer::calculate_mahalanobis_distances(
    const std::vector<std::vector<double>>& data) {

    std::vector<double> distances(data.size(), 0.0);

    if (data.empty()) {
        return distances;
    }

    size_t n = data.size();
    size_t d = data[0].size();

    // Calculate mean vector
    std::vector<double> mean(d, 0.0);
    for (const auto& point : data) {
        for (size_t i = 0; i < d; ++i) {
            mean[i] += point[i];
        }
    }
    for (size_t i = 0; i < d; ++i) {
        mean[i] /= n;
    }

    // Calculate covariance matrix
    std::vector<std::vector<double>> covariance(d, std::vector<double>(d, 0.0));
    for (const auto& point : data) {
        for (size_t i = 0; i < d; ++i) {
            for (size_t j = 0; j < d; ++j) {
                covariance[i][j] += (point[i] - mean[i]) * (point[j] - mean[j]);
            }
        }
    }
    for (size_t i = 0; i < d; ++i) {
        for (size_t j = 0; j < d; ++j) {
            covariance[i][j] /= (n - 1);
        }
    }

    // Calculate Mahalanobis distance for each point
    for (size_t p = 0; p < n; ++p) {
        double distance = 0.0;

        // Simple implementation - in production, use matrix inversion
        // For high dimensions, use regularized covariance or PCA

        // For now, use Euclidean distance normalized by variance in each dimension
        for (size_t i = 0; i < d; ++i) {
            if (covariance[i][i] > 0) {
                double diff = data[p][i] - mean[i];
                distance += (diff * diff) / covariance[i][i];
            }
        }
        distances[p] = std::sqrt(distance);
    }

    return distances;
}

std::vector<double> ProfessionalDataAnalyzer::calculate_isolation_scores(
    const std::vector<std::vector<double>>& data) {

    std::vector<double> scores(data.size(), 0.0);

    if (data.empty()) {
        return scores;
    }

    // Simplified Isolation Forest algorithm
    size_t num_trees = 100;
    size_t sub_sample_size = std::min((size_t)256, data.size());

    std::random_device rd;
    std::mt19937_64 gen(rd());

    for (size_t tree_idx = 0; tree_idx < num_trees; ++tree_idx) {
        // Sample subset
        std::vector<size_t> sample_indices(sub_sample_size);
        for (size_t i = 0; i < sub_sample_size; ++i) {
            sample_indices[i] = std::uniform_int_distribution<size_t>(0, data.size()-1)(gen);
        }

        // Build simple isolation tree (height-limited)
        for (size_t i = 0; i < data.size(); ++i) {
            // Simplified: points that are far from the sample get higher scores
            double min_distance = std::numeric_limits<double>::max();
            for (size_t sample_idx : sample_indices) {
                if (sample_idx != i) {
                    double distance = 0.0;
                    for (size_t dim = 0; dim < data[i].size(); ++dim) {
                        double diff = data[i][dim] - data[sample_idx][dim];
                        distance += diff * diff;
                    }
                    min_distance = std::min(min_distance, std::sqrt(distance));
                }
            }
            scores[i] += min_distance;
        }
    }

    // Normalize scores
    double max_score = *std::max_element(scores.begin(), scores.end());
    if (max_score > 0) {
        for (double& score : scores) {
            score /= max_score;
        }
    }

    return scores;
}

std::vector<double> ProfessionalDataAnalyzer::calculate_lof_scores(
    const std::vector<std::vector<double>>& data) {

    std::vector<double> scores(data.size(), 0.0);

    if (data.size() < 10) {
        return scores;
    }

    size_t k = std::min((size_t)10, data.size() / 2);

    // Calculate k-distance and reachability distance for each point
    std::vector<double> k_distances(data.size());
    std::vector<std::vector<size_t>> k_neighbors(data.size());

    for (size_t i = 0; i < data.size(); ++i) {
        // Calculate distances to all other points
        std::vector<std::pair<double, size_t>> distances;
        for (size_t j = 0; j < data.size(); ++j) {
            if (i != j) {
                double distance = 0.0;
                for (size_t dim = 0; dim < data[i].size(); ++dim) {
                    double diff = data[i][dim] - data[j][dim];
                    distance += diff * diff;
                }
                distances.emplace_back(std::sqrt(distance), j);
            }
        }

        // Sort by distance
        std::sort(distances.begin(), distances.end(),
                 [](const auto& a, const auto& b) { return a.first < b.first; });

        // Get k-distance and k-neighbors
        k_distances[i] = distances[k-1].first;
        for (size_t n = 0; n < k; ++n) {
            k_neighbors[i].push_back(distances[n].second);
        }
    }

    // Calculate local reachability density (LRD)
    std::vector<double> lrd(data.size(), 0.0);
    for (size_t i = 0; i < data.size(); ++i) {
        double sum_reach_dist = 0.0;
        for (size_t neighbor : k_neighbors[i]) {
            double reach_dist = std::max(k_distances[neighbor],
                                        calculate_distance(data[i], data[neighbor]));
            sum_reach_dist += reach_dist;
        }
        lrd[i] = k / sum_reach_dist;
    }

    // Calculate LOF scores
    for (size_t i = 0; i < data.size(); ++i) {
        double sum_lrd_ratio = 0.0;
        for (size_t neighbor : k_neighbors[i]) {
            sum_lrd_ratio += lrd[neighbor] / lrd[i];
        }
        scores[i] = sum_lrd_ratio / k;
    }

    // Normalize scores
    double max_score = *std::max_element(scores.begin(), scores.end());
    if (max_score > 0) {
        for (double& score : scores) {
            score /= max_score;
        }
    }

    return scores;
}

double ProfessionalDataAnalyzer::calculate_distance(const std::vector<double>& a,
                                                   const std::vector<double>& b) {
    double distance = 0.0;
    for (size_t i = 0; i < a.size(); ++i) {
        double diff = a[i] - b[i];
        distance += diff * diff;
    }
    return std::sqrt(distance);
}

double ProfessionalDataAnalyzer::normalize_score(double score,
                                                const std::vector<double>& all_scores) {
    if (all_scores.empty()) {
        return 0.0;
    }

    double min_score = *std::min_element(all_scores.begin(), all_scores.end());
    double max_score = *std::max_element(all_scores.begin(), all_scores.end());

    if (max_score - min_score < 1e-10) {
        return 0.5;
    }

    return (score - min_score) / (max_score - min_score);
}

double ProfessionalDataAnalyzer::calculate_mahalanobis_threshold(
    const std::vector<double>& distances) {

    if (distances.empty()) {
        return 0.0;
    }

    // Use median + 3 * MAD
    std::vector<double> sorted = distances;
    std::sort(sorted.begin(), sorted.end());

    double median = (sorted.size() % 2 == 0) ?
        (sorted[sorted.size()/2 - 1] + sorted[sorted.size()/2]) / 2.0 :
        sorted[sorted.size()/2];

    std::vector<double> absolute_deviations;
    for (double dist : distances) {
        absolute_deviations.push_back(std::abs(dist - median));
    }
    std::sort(absolute_deviations.begin(), absolute_deviations.end());

    double mad = (absolute_deviations.size() % 2 == 0) ?
        (absolute_deviations[absolute_deviations.size()/2 - 1] +
         absolute_deviations[absolute_deviations.size()/2]) / 2.0 :
        absolute_deviations[absolute_deviations.size()/2];

    return median + 3.0 * mad;
}

double ProfessionalDataAnalyzer::calculate_lof_threshold(
    const std::vector<double>& scores) {

    if (scores.empty()) {
        return 0.0;
    }

    // LOF > 1 indicates outlier
    return 1.0;
}

std::string ProfessionalDataAnalyzer::determine_likely_cause(
    const ProfessionalOutlierAnalysis& analysis,
    const std::vector<std::vector<double>>& data,
    const std::vector<std::string>& feature_columns) {

    if (analysis.outlier_indices.empty() || feature_columns.empty()) {
        return "unknown";
    }

    // Analyze patterns to determine likely cause

    // Check if outliers are clustered in time (if we had timestamps)
    bool clustered_in_time = false;
    if (analysis.temporal_pattern.is_clustered_in_time) {
        return "system_error";
    }

    // Check feature contributions
    if (!analysis.root_cause.contributing_features.empty()) {
        std::string top_feature = analysis.root_cause.contributing_features[0];

        // Check for measurement error patterns
        if (top_feature.find("sensor") != std::string::npos ||
            top_feature.find("measurement") != std::string::npos ||
            top_feature.find("reading") != std::string::npos) {
            return "measurement_error";
        }

        // Check for data entry patterns
        if (top_feature.find("age") != std::string::npos ||
            top_feature.find("date") != std::string::npos ||
            top_feature.find("id") != std::string::npos) {
            return "data_entry_error";
        }

        // Check for financial patterns
        if (top_feature.find("price") != std::string::npos ||
            top_feature.find("cost") != std::string::npos ||
            top_feature.find("revenue") != std::string::npos) {
            return "fraud";
        }
    }

    // Check outlier magnitude
    if (!analysis.outlier_scores.empty()) {
        double max_score = *std::max_element(analysis.outlier_scores.begin(),
                                           analysis.outlier_scores.end());
        if (max_score > 0.9) {
            return "measurement_error";
        }
    }

    // Default to natural variation
    return "natural_variation";
}

double ProfessionalDataAnalyzer::calculate_variance_impact(
    const std::vector<std::vector<double>>& data_with_outliers,
    const std::vector<std::vector<double>>& data_without_outliers) {

    if (data_with_outliers.empty() || data_without_outliers.empty()) {
        return 0.0;
    }

    size_t d = data_with_outliers[0].size();
    double total_impact = 0.0;

    for (size_t dim = 0; dim < d; ++dim) {
        // Calculate variance with outliers
        double sum_with = 0.0, sum_sq_with = 0.0;
        for (const auto& point : data_with_outliers) {
            sum_with += point[dim];
            sum_sq_with += point[dim] * point[dim];
        }
        double var_with = (sum_sq_with / data_with_outliers.size()) -
                         (sum_with / data_with_outliers.size()) *
                         (sum_with / data_with_outliers.size());

        // Calculate variance without outliers
        double sum_without = 0.0, sum_sq_without = 0.0;
        for (const auto& point : data_without_outliers) {
            sum_without += point[dim];
            sum_sq_without += point[dim] * point[dim];
        }
        double var_without = (sum_sq_without / data_without_outliers.size()) -
                            (sum_without / data_without_outliers.size()) *
                            (sum_without / data_without_outliers.size());

        if (var_without > 0) {
            total_impact += std::abs(var_with - var_without) / var_without;
        }
    }

    return total_impact / d;
}

std::vector<double> ProfessionalDataAnalyzer::calculate_cooks_distances(
    const std::vector<std::vector<double>>& data) {

    std::vector<double> distances(data.size(), 0.0);

    if (data.size() < 10) {
        return distances;
    }

    // Simplified Cook's distance calculation
    // In production, this would use actual regression

    // Calculate mean of each dimension
    size_t d = data[0].size();
    std::vector<double> means(d, 0.0);
    for (const auto& point : data) {
        for (size_t i = 0; i < d; ++i) {
            means[i] += point[i];
        }
    }
    for (size_t i = 0; i < d; ++i) {
        means[i] /= data.size();
    }

    // Calculate leverage (simplified)
    std::vector<double> leverages(data.size(), 0.0);
    for (size_t i = 0; i < data.size(); ++i) {
        double leverage = 0.0;
        for (size_t j = 0; j < d; ++j) {
            double diff = data[i][j] - means[j];
            leverage += diff * diff;
        }
        leverages[i] = leverage / (d * data.size());
    }

    // Calculate residuals (simplified)
    std::vector<double> residuals(data.size(), 0.0);
    for (size_t i = 0; i < data.size(); ++i) {
        double residual = 0.0;
        for (size_t j = 0; j < d; ++j) {
            double diff = data[i][j] - means[j];
            residual += diff * diff;
        }
        residuals[i] = std::sqrt(residual);
    }

    // Calculate Cook's distance: D_i = (r_i^2 / (k * MSE)) * (h_ii / (1 - h_ii)^2)
    double mse = 0.0;
    for (double residual : residuals) {
        mse += residual * residual;
    }
    mse /= (data.size() - d - 1);

    for (size_t i = 0; i < data.size(); ++i) {
        double h = leverages[i];
        double r = residuals[i];
        distances[i] = (r * r / (d * mse)) * (h / ((1 - h) * (1 - h)));
    }

    return distances;
}

std::vector<double> ProfessionalDataAnalyzer::calculate_leverage_scores(
    const std::vector<std::vector<double>>& data) {

    std::vector<double> scores(data.size(), 0.0);

    if (data.empty()) {
        return scores;
    }

    // Calculate leverage using hat matrix diagonal (simplified)
    size_t d = data[0].size();

    // Calculate covariance matrix
    std::vector<double> mean(d, 0.0);
    for (const auto& point : data) {
        for (size_t i = 0; i < d; ++i) {
            mean[i] += point[i];
        }
    }
    for (size_t i = 0; i < d; ++i) {
        mean[i] /= data.size();
    }

    // Simplified leverage: distance from center normalized by total variance
    double total_variance = 0.0;
    for (size_t i = 0; i < d; ++i) {
        double var = 0.0;
        for (const auto& point : data) {
            double diff = point[i] - mean[i];
            var += diff * diff;
        }
        var /= data.size();
        total_variance += var;
    }

    for (size_t i = 0; i < data.size(); ++i) {
        double distance = 0.0;
        for (size_t j = 0; j < d; ++j) {
            double diff = data[i][j] - mean[j];
            distance += diff * diff;
        }
        scores[i] = distance / (total_variance + 1e-10);
    }

    // Normalize
    double max_score = *std::max_element(scores.begin(), scores.end());
    if (max_score > 0) {
        for (double& score : scores) {
            score /= max_score;
        }
    }

    return scores;
}

void ProfessionalDataAnalyzer::generate_outlier_recommendations(
    ProfessionalOutlierAnalysis& analysis,
    size_t total_points) {

    double outlier_percentage = (analysis.outlier_indices.size() * 100.0) / total_points;

    // Determine overall strategy
    if (outlier_percentage > 10.0) {
        analysis.recommendations.overall_strategy = "investigate_systematically";
    } else if (outlier_percentage > 5.0) {
        analysis.recommendations.overall_strategy = "winsorize";
    } else if (outlier_percentage > 1.0) {
        analysis.recommendations.overall_strategy = "remove";
    } else {
        analysis.recommendations.overall_strategy = "keep";
    }

    // Categorize outliers for different actions
    for (size_t i = 0; i < analysis.outlier_indices.size(); ++i) {
        double score = analysis.outlier_scores[i];

        if (score > 0.9) {
            // Critical outliers - investigate
            analysis.recommendations.should_investigate.push_back(analysis.outlier_indices[i]);
        } else if (score > 0.7) {
            // High severity - remove
            analysis.recommendations.should_remove.push_back(analysis.outlier_indices[i]);
        } else if (score > 0.5) {
            // Medium severity - cap/winsorize
            analysis.recommendations.should_cap.push_back(analysis.outlier_indices[i]);
        } else if (score > 0.3) {
            // Low severity - impute
            analysis.recommendations.should_impute.push_back(analysis.outlier_indices[i]);
        } else {
            // Very low severity - keep
            analysis.recommendations.should_keep.push_back(analysis.outlier_indices[i]);
        }
    }
}

// Distribution analysis helper methods
void ProfessionalDataAnalyzer::perform_tail_analysis(
    ProfessionalDistributionAnalysis::TailAnalysis& tail_analysis,
    const std::vector<double>& values) {

    if (values.size() < 100) {
        return;
    }

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    // Calculate tail index using Hill estimator
    size_t k = std::min((size_t)20, sorted.size() / 10); // Number of tail observations

    double hill_sum = 0.0;
    for (size_t i = sorted.size() - k; i < sorted.size(); ++i) {
        hill_sum += std::log(sorted[i] / sorted[sorted.size() - k - 1]);
    }

    tail_analysis.hill_estimator = hill_sum / k;
    tail_analysis.tail_index = 1.0 / tail_analysis.hill_estimator;

    // Determine if heavy-tailed
    tail_analysis.heavy_tailed = (tail_analysis.tail_index < 4.0);
    tail_analysis.light_tailed = (tail_analysis.tail_index >= 4.0);

    // Calculate extreme value index
    tail_analysis.extreme_value_index = tail_analysis.tail_index;

    // Calculate VaR and Expected Shortfall
    size_t idx_95 = static_cast<size_t>(0.95 * sorted.size());
    size_t idx_99 = static_cast<size_t>(0.99 * sorted.size());

    if (idx_95 < sorted.size()) {
        tail_analysis.value_at_risk_95 = sorted[idx_95];

        // Calculate expected shortfall (average of tail beyond VaR)
        double tail_sum = 0.0;
        size_t tail_count = 0;
        for (size_t i = idx_95; i < sorted.size(); ++i) {
            tail_sum += sorted[i];
            tail_count++;
        }
        if (tail_count > 0) {
            tail_analysis.expected_shortfall_95 = tail_sum / tail_count;
        }
    }

    if (idx_99 < sorted.size()) {
        tail_analysis.value_at_risk_99 = sorted[idx_99];

        double tail_sum = 0.0;
        size_t tail_count = 0;
        for (size_t i = idx_99; i < sorted.size(); ++i) {
            tail_sum += sorted[i];
            tail_count++;
        }
        if (tail_count > 0) {
            tail_analysis.expected_shortfall_99 = tail_sum / tail_count;
        }
    }
}

void ProfessionalDataAnalyzer::perform_modality_analysis(
    ProfessionalDistributionAnalysis::Modality& modality,
    const std::vector<double>& values) {

    if (values.size() < 50) {
        modality.is_unimodal = true;
        modality.number_of_modes = 1;
        return;
    }

    // Create histogram
    size_t num_bins = std::min((size_t)50, static_cast<size_t>(std::sqrt(values.size())));

    auto [min_it, max_it] = std::minmax_element(values.begin(), values.end());
    double min_val = *min_it;
    double max_val = *max_it;
    double bin_width = (max_val - min_val) / num_bins;

    if (bin_width < 1e-10) {
        modality.is_unimodal = true;
        modality.number_of_modes = 1;
        return;
    }

    std::vector<size_t> histogram(num_bins, 0);
    for (double val : values) {
        size_t bin_idx = static_cast<size_t>((val - min_val) / bin_width);
        if (bin_idx >= num_bins) bin_idx = num_bins - 1;
        histogram[bin_idx]++;
    }

    // Find local maxima (modes)
    for (size_t i = 1; i < num_bins - 1; ++i) {
        if (histogram[i] > histogram[i-1] && histogram[i] > histogram[i+1]) {
            modality.number_of_modes++;
            modality.mode_locations.push_back(min_val + (i + 0.5) * bin_width);
            modality.mode_heights.push_back(histogram[i]);
        }
    }

    modality.is_unimodal = (modality.number_of_modes == 1);
    modality.is_bimodal = (modality.number_of_modes == 2);
    modality.is_multimodal = (modality.number_of_modes > 2);

    // Calculate dip statistic (simplified)
    modality.dip_statistic = calculate_dip_statistic(histogram);
    modality.dip_p_value = 1.0 - std::exp(-modality.dip_statistic * values.size());
}

double ProfessionalDataAnalyzer::calculate_dip_statistic(
    const std::vector<size_t>& histogram) {

    // Simplified dip test for unimodality
    if (histogram.size() < 3) {
        return 0.0;
    }

    // Calculate empirical CDF
    size_t total = std::accumulate(histogram.begin(), histogram.end(), 0);
    std::vector<double> ecdf(histogram.size(), 0.0);

    double cumulative = 0.0;
    for (size_t i = 0; i < histogram.size(); ++i) {
        cumulative += histogram[i];
        ecdf[i] = cumulative / total;
    }

    // Calculate greatest convex minorant and least concave majorant
    // Simplified: maximum deviation from linear interpolation
    double max_deviation = 0.0;
    for (size_t i = 0; i < histogram.size(); ++i) {
        double linear = static_cast<double>(i) / (histogram.size() - 1);
        double deviation = std::abs(ecdf[i] - linear);
        max_deviation = std::max(max_deviation, deviation);
    }

    return max_deviation;
}

void ProfessionalDataAnalyzer::determine_distribution_properties(
    ProfessionalDistributionAnalysis& analysis,
    const std::vector<double>& values) {

    // Check symmetry
    double skewness = analysis.moments.skewness;
    analysis.is_symmetric = (std::abs(skewness) < 0.5);

    // Check uniformity
    auto [min_it, max_it] = std::minmax_element(values.begin(), values.end());
    double range = *max_it - *min_it;

    // Divide into bins and check frequency
    size_t num_bins = 10;
    std::vector<size_t> histogram(num_bins, 0);
    for (double val : values) {
        size_t bin_idx = static_cast<size_t>((val - *min_it) / range * num_bins);
        if (bin_idx >= num_bins) bin_idx = num_bins - 1;
        histogram[bin_idx]++;
    }

    // Chi-square test for uniformity
    double expected = values.size() / static_cast<double>(num_bins);
    double chi2 = 0.0;
    for (size_t count : histogram) {
        double diff = count - expected;
        chi2 += (diff * diff) / expected;
    }

    analysis.is_uniform = (chi2 < 16.9); // p > 0.05 for df=9

    // Check exponential distribution
    // Exponential should have skewness ~2 and kurtosis ~6
    analysis.is_exponential = (std::abs(skewness - 2.0) < 1.0 &&
                              std::abs(analysis.moments.kurtosis - 6.0) < 2.0);

    // Check power law (simplified)
    // Power law has infinite mean/variance for certain parameters
    analysis.is_power_law = (skewness > 3.0 && analysis.moments.kurtosis > 10.0);
}

void ProfessionalDataAnalyzer::suggest_transformations(
    ProfessionalDistributionAnalysis::Transformations& transformations,
    const std::vector<double>& values) {

    double skewness = ProfessionalStatisticalCalculator::calculate_skewness(values);

    if (std::abs(skewness) > 1.0) {
        // Significant skewness - suggest transformations

        if (skewness > 0) {
            // Right-skewed
            transformations.log_recommended = true;
            transformations.sqrt_recommended = true;
            transformations.box_cox_recommended = true;
            transformations.best_transformation = "log";
        } else {
            // Left-skewed
            transformations.power_recommended = true;
            transformations.box_cox_recommended = true;
            transformations.best_transformation = "power";
        }

        // Estimate optimal Box-Cox lambda
        transformations.best_lambda = estimate_box_cox_lambda(values);
    }

    // Check for zeros/negatives
    bool all_positive = std::all_of(values.begin(), values.end(),
                                   [](double x) { return x > 0; });

    if (!all_positive) {
        transformations.yeo_johnson_recommended = true;
        transformations.box_cox_recommended = false; // Box-Cox requires positive values
    }
}

double ProfessionalDataAnalyzer::estimate_box_cox_lambda(
    const std::vector<double>& values) {

    // Simple estimation of Box-Cox lambda
    // In production, use maximum likelihood estimation

    double skewness = ProfessionalStatisticalCalculator::calculate_skewness(values);

    if (std::abs(skewness) < 0.5) {
        return 1.0; // No transformation needed
    } else if (skewness > 1.0) {
        return 0.0; // Log transformation
    } else if (skewness > 0.5) {
        return 0.5; // Square root transformation
    } else if (skewness < -1.0) {
        return 2.0; // Square transformation
    } else if (skewness < -0.5) {
        return 1.5; // Power transformation
    }

    return 1.0;
}

void ProfessionalDataAnalyzer::generate_empirical_cdf(
    std::vector<double>& ecdf,
    const std::vector<double>& values) {

    if (values.empty()) {
        return;
    }

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    ecdf.reserve(sorted.size());
    for (size_t i = 0; i < sorted.size(); ++i) {
        ecdf.push_back(static_cast<double>(i + 1) / sorted.size());
    }
}

void ProfessionalDataAnalyzer::generate_theoretical_cdf(
    std::vector<double>& tcdf,
    const std::vector<double>& values,
    const ProfessionalStatisticalCalculator::DistributionFit& distribution) {

    if (values.empty() || distribution.name.empty()) {
        return;
    }

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    tcdf.reserve(sorted.size());

    for (double val : sorted) {
        double cdf_value = 0.0;

        if (distribution.name == "normal") {
            double mu = distribution.parameters.at("mean");
            double sigma = distribution.parameters.at("std_dev");
            cdf_value = 0.5 * (1.0 + std::erf((val - mu) / (sigma * std::sqrt(2.0))));
        } else if (distribution.name == "lognormal") {
            double mu = distribution.parameters.at("mu");
            double sigma = distribution.parameters.at("sigma");
            if (val > 0) {
                cdf_value = 0.5 * (1.0 + std::erf((std::log(val) - mu) / (sigma * std::sqrt(2.0))));
            }
        } else if (distribution.name == "exponential") {
            double lambda = distribution.parameters.at("lambda");
            if (val >= 0) {
                cdf_value = 1.0 - std::exp(-lambda * val);
            }
        }

        tcdf.push_back(cdf_value);
    }
}

void ProfessionalDataAnalyzer::generate_qq_plot_data(
    std::vector<double>& qq_data,
    const std::vector<double>& values) {

    if (values.size() < 2) {
        return;
    }

    std::vector<double> sorted = values;
    std::sort(sorted.begin(), sorted.end());

    size_t n = sorted.size();
    qq_data.reserve(n * 2);

    for (size_t i = 0; i < n; ++i) {
        double quantile = static_cast<double>(i + 0.5) / n;
        double theoretical = ProfessionalStatisticalCalculator::normal_quantile(quantile);

        qq_data.push_back(theoretical);
        qq_data.push_back(sorted[i]);
    }
}

// Time series helper methods
void ProfessionalDataAnalyzer::perform_time_series_decomposition(
    ProfessionalTimeSeriesAnalysis::Decomposition& decomposition,
    const std::vector<double>& series,
    const std::vector<time_t>& timestamps) {

    if (series.size() < 50) {
        return;
    }

    // Simple moving average for trend
    size_t window_size = std::min((size_t)7, series.size() / 10);
    if (window_size % 2 == 0) window_size++;

    decomposition.trend.resize(series.size(), 0.0);
    for (size_t i = 0; i < series.size(); ++i) {
        size_t start = (i > window_size/2) ? i - window_size/2 : 0;
        size_t end = std::min(i + window_size/2 + 1, series.size());

        double sum = 0.0;
        for (size_t j = start; j < end; ++j) {
            sum += series[j];
        }
        decomposition.trend[i] = sum / (end - start);
    }

    // Check for seasonality (simplified)
    bool has_seasonality = detect_seasonal_pattern(series, timestamps);
    decomposition.has_seasonality = has_seasonality;

    if (has_seasonality) {
        // Estimate seasonal period
        decomposition.seasonal_period_length = estimate_seasonal_period(series);

        if (decomposition.seasonal_period_length > 0) {
            decomposition.seasonal_period = "period_" +
                                           std::to_string(decomposition.seasonal_period_length);

            // Calculate seasonal indices
            decomposition.seasonal_indices.resize(decomposition.seasonal_period_length, 0.0);
            std::vector<size_t> counts(decomposition.seasonal_period_length, 0);

            for (size_t i = 0; i < series.size(); ++i) {
                size_t period_idx = i % decomposition.seasonal_period_length;
                decomposition.seasonal_indices[period_idx] += series[i] - decomposition.trend[i];
                counts[period_idx]++;
            }

            for (size_t i = 0; i < decomposition.seasonal_period_length; ++i) {
                if (counts[i] > 0) {
                    decomposition.seasonal_indices[i] /= counts[i];
                }
            }

            // Create seasonal component
            decomposition.seasonal.resize(series.size());
            for (size_t i = 0; i < series.size(); ++i) {
                size_t period_idx = i % decomposition.seasonal_period_length;
                decomposition.seasonal[i] = decomposition.seasonal_indices[period_idx];
            }
        }
    } else {
        decomposition.seasonal.resize(series.size(), 0.0);
    }

    // Calculate residual
    decomposition.residual.resize(series.size());
    for (size_t i = 0; i < series.size(); ++i) {
        decomposition.residual[i] = series[i] - decomposition.trend[i] - decomposition.seasonal[i];
    }

    // Calculate strengths
    double var_original = calculate_variance(series);
    double var_residual = calculate_variance(decomposition.residual);
    double var_detrended = calculate_variance(decomposition.trend);
    double var_seasonal = calculate_variance(decomposition.seasonal);

    if (var_original > 0) {
        decomposition.trend_strength = std::max(0.0, 1.0 - var_residual / var_original);
        decomposition.seasonality_strength = std::max(0.0, 1.0 - var_residual / var_detrended);
        decomposition.residual_strength = var_residual / var_original;
    }
}

bool ProfessionalDataAnalyzer::detect_seasonal_pattern(
    const std::vector<double>& series,
    const std::vector<time_t>& timestamps) {

    if (series.size() < 100) {
        return false;
    }

    // Check autocorrelation at potential seasonal lags
    size_t max_lag = std::min((size_t)100, series.size() / 4);
    std::vector<double> acf = ProfessionalStatisticalCalculator::calculate_autocorrelation(
        series, max_lag);

    // Look for significant peaks at regular intervals
    std::vector<size_t> peaks;
    for (size_t lag = 2; lag < acf.size() - 1; ++lag) {
        if (acf[lag] > acf[lag-1] && acf[lag] > acf[lag+1] && acf[lag] > 0.3) {
            peaks.push_back(lag);
        }
    }

    // Check if peaks are at regular intervals (seasonality)
    if (peaks.size() >= 2) {
        std::vector<size_t> intervals;
        for (size_t i = 1; i < peaks.size(); ++i) {
            intervals.push_back(peaks[i] - peaks[i-1]);
        }

        // Check consistency of intervals
        size_t avg_interval = std::accumulate(intervals.begin(), intervals.end(), 0) / intervals.size();
        bool consistent = true;
        for (size_t interval : intervals) {
            if (std::abs(static_cast<int>(interval) - static_cast<int>(avg_interval)) > 1) {
                consistent = false;
                break;
            }
        }

        return consistent;
    }

    return false;
}

size_t ProfessionalDataAnalyzer::estimate_seasonal_period(
    const std::vector<double>& series) {

    if (series.size() < 100) {
        return 0;
    }

    // Use autocorrelation to estimate period
    size_t max_lag = std::min((size_t)100, series.size() / 4);
    std::vector<double> acf = ProfessionalStatisticalCalculator::calculate_autocorrelation(
        series, max_lag);

    // Find first significant peak after lag 0
    for (size_t lag = 1; lag < acf.size(); ++lag) {
        if (acf[lag] > 0.5) {
            // Check if this is a seasonal peak by looking for multiple peaks
            bool is_seasonal = false;
            for (size_t multiple = 2; multiple <= 5; ++multiple) {
                size_t next_peak = lag * multiple;
                if (next_peak < acf.size() && acf[next_peak] > 0.3) {
                    is_seasonal = true;
                    break;
                }
            }

            if (is_seasonal) {
                return lag;
            }
        }
    }

    return 0;
}

double ProfessionalDataAnalyzer::calculate_variance(const std::vector<double>& values) {
    if (values.size() < 2) {
        return 0.0;
    }

    double mean = ProfessionalStatisticalCalculator::calculate_mean(values);
    double sum_sq = 0.0;
    for (double val : values) {
        double diff = val - mean;
        sum_sq += diff * diff;
    }

    return sum_sq / (values.size() - 1);
}

double ProfessionalDataAnalyzer::calculate_autocorrelation_time(
    const std::vector<double>& acf_values) {

    if (acf_values.empty()) {
        return 0.0;
    }

    // Estimate autocorrelation time using integrated autocorrelation
    double tau = 1.0; // Start with 1 for lag 0
    for (size_t lag = 1; lag < acf_values.size(); ++lag) {
        if (acf_values[lag] > 0) {
            tau += 2.0 * acf_values[lag];
        } else {
            break; // Stop when autocorrelation becomes negative
        }
    }

    return tau;
}

void ProfessionalDataAnalyzer::perform_spectral_analysis(
    ProfessionalTimeSeriesAnalysis::Spectral& spectral,
    const std::vector<double>& series) {

    if (series.size() < 50) {
        return;
    }

    // Simple periodogram calculation
    size_t n = series.size();
    spectral.periodogram.resize(n/2 + 1, 0.0);

    // Remove mean
    double mean = ProfessionalStatisticalCalculator::calculate_mean(series);
    std::vector<double> centered = series;
    for (double& val : centered) {
        val -= mean;
    }

    // Simple DFT (in production, use FFTW)
    for (size_t k = 0; k <= n/2; ++k) {
        double real = 0.0, imag = 0.0;
        for (size_t t = 0; t < n; ++t) {
            double angle = 2.0 * M_PI * k * t / n;
            real += centered[t] * std::cos(angle);
            imag -= centered[t] * std::sin(angle);
        }
        spectral.periodogram[k] = (real * real + imag * imag) / n;
    }

    // Find dominant frequencies
    spectral.total_power = std::accumulate(spectral.periodogram.begin(),
                                          spectral.periodogram.end(), 0.0);

    // Find peaks in periodogram
    for (size_t k = 1; k < spectral.periodogram.size() - 1; ++k) {
        if (spectral.periodogram[k] > spectral.periodogram[k-1] &&
            spectral.periodogram[k] > spectral.periodogram[k+1] &&
            spectral.periodogram[k] > spectral.total_power * 0.01) { // Threshold

            double frequency = static_cast<double>(k) / n;
            spectral.dominant_frequencies.push_back(frequency);
            spectral.power_at_frequencies.push_back(spectral.periodogram[k]);
        }
    }

    // Find peak frequency
    if (!spectral.periodogram.empty()) {
        auto max_it = std::max_element(spectral.periodogram.begin() + 1,
                                      spectral.periodogram.end());
        if (max_it != spectral.periodogram.end()) {
            size_t k = std::distance(spectral.periodogram.begin(), max_it);
            spectral.peak_frequency = static_cast<double>(k) / n;
            spectral.peak_power = *max_it;
        }
    }

    // Spectral density (smoothed periodogram)
    spectral.spectral_density = spectral.periodogram;

    // Apply simple smoothing
    size_t window = 3;
    for (size_t k = window; k < spectral.spectral_density.size() - window; ++k) {
        double sum = 0.0;
        for (size_t w = k - window; w <= k + window; ++w) {
            sum += spectral.periodogram[w];
        }
        spectral.spectral_density[k] = sum / (2 * window + 1);
    }
}

void ProfessionalDataAnalyzer::perform_volatility_analysis(
    ProfessionalTimeSeriesAnalysis::Volatility& volatility,
    const std::vector<double>& series) {

    if (series.size() < 20) {
        return;
    }

    // Calculate returns
    std::vector<double> returns;
    for (size_t i = 1; i < series.size(); ++i) {
        if (series[i-1] != 0) {
            returns.push_back((series[i] - series[i-1]) / series[i-1]);
        }
    }

    if (returns.size() < 10) {
        return;
    }

    // Unconditional variance
    volatility.unconditional_variance = calculate_variance(returns);

    // Check for ARCH effects (autocorrelation in squared returns)
    std::vector<double> squared_returns;
    for (double ret : returns) {
        squared_returns.push_back(ret * ret);
    }

    size_t max_lag = std::min((size_t)10, squared_returns.size() / 4);
    std::vector<double> acf_squared = ProfessionalStatisticalCalculator::calculate_autocorrelation(
        squared_returns, max_lag);

    // Check if any autocorrelation is significant
    volatility.has_volatility_clustering = false;
    for (double acf : acf_squared) {
        if (std::abs(acf) > 2.0 / std::sqrt(squared_returns.size())) {
            volatility.has_volatility_clustering = true;
            break;
        }
    }

    // ARCH effect (simplified)
    if (volatility.has_volatility_clustering) {
        volatility.arch_effect = 0.8; // Placeholder
        volatility.garch_effect = 0.9; // Placeholder
    }

    // Conditional variance (simplified EWMA)
    volatility.conditional_variance = 0.0;
    double lambda = 0.94; // EWMA decay factor
    double variance = volatility.unconditional_variance;

    for (double ret : returns) {
        variance = lambda * variance + (1 - lambda) * ret * ret;
    }
    volatility.conditional_variance = variance;

    // Identify volatility clusters
    if (volatility.has_volatility_clustering) {
        double threshold = 2.0 * std::sqrt(volatility.unconditional_variance);
        bool in_cluster = false;
        size_t cluster_start = 0;

        for (size_t i = 0; i < returns.size(); ++i) {
            if (std::abs(returns[i]) > threshold) {
                if (!in_cluster) {
                    cluster_start = i;
                    in_cluster = true;
                }
            } else {
                if (in_cluster) {
                    volatility.volatility_clusters.push_back(cluster_start);
                    volatility.volatility_clusters.push_back(i-1);
                    in_cluster = false;
                }
            }
        }

        if (in_cluster) {
            volatility.volatility_clusters.push_back(cluster_start);
            volatility.volatility_clusters.push_back(returns.size() - 1);
        }
    }
}

void ProfessionalDataAnalyzer::perform_forecastability_analysis(
    ProfessionalTimeSeriesAnalysis::Forecastability& forecastability,
    const std::vector<double>& series) {

    if (series.size() < 50) {
        return;
    }

    // Calculate sample entropy
    forecastability.sample_entropy = calculate_sample_entropy(series, 2, 0.2);

    // Calculate approximate entropy
    forecastability.approximate_entropy = calculate_approximate_entropy(series, 2, 0.2);

    // Entropy rate (simplified)
    forecastability.entropy_rate = forecastability.sample_entropy;

    // Predictability (1 - normalized entropy)
    double max_entropy = std::log(series.size());
    if (max_entropy > 0) {
        forecastability.predictability = 1.0 - (forecastability.entropy_rate / max_entropy);
    }

    // Check for chaos (positive Lyapunov exponent)
    double lyapunov = ProfessionalStatisticalCalculator::calculate_lyapunov_exponent(series);
    forecastability.is_chaotic = (lyapunov > 0.1);

    // Determine if predictable
    forecastability.is_predictable = (forecastability.predictability > 0.3 && !forecastability.is_chaotic);

    // Forecast horizon (simplified)
    if (forecastability.is_predictable) {
        forecastability.forecast_horizon = 1.0 / (1.0 - forecastability.predictability);
    } else {
        forecastability.forecast_horizon = 1.0;
    }
}

double ProfessionalDataAnalyzer::calculate_sample_entropy(
    const std::vector<double>& series,
    int m,
    double r) {

    if (series.size() < m + 10) {
        return 0.0;
    }

    // Simplified sample entropy calculation
    int n = series.size();
    std::vector<double> normalized = series;

    // Normalize
    double mean = ProfessionalStatisticalCalculator::calculate_mean(series);
    double std = ProfessionalStatisticalCalculator::calculate_std_dev(series);
    if (std > 0) {
        for (double& val : normalized) {
            val = (val - mean) / std;
        }
    }

    // Count matches
    int matches_m = 0, matches_m1 = 0;
    double threshold = r * std;

    for (int i = 0; i < n - m; ++i) {
        for (int j = i + 1; j < n - m; ++j) {
            bool match_m = true;
            for (int k = 0; k < m; ++k) {
                if (std::abs(normalized[i+k] - normalized[j+k]) > threshold) {
                    match_m = false;
                    break;
                }
            }
            if (match_m) matches_m++;

            bool match_m1 = true;
            for (int k = 0; k <= m; ++k) {
                if (std::abs(normalized[i+k] - normalized[j+k]) > threshold) {
                    match_m1 = false;
                    break;
                }
            }
            if (match_m1) matches_m1++;
        }
    }

    if (matches_m > 0 && matches_m1 > 0) {
        return -std::log(static_cast<double>(matches_m1) / matches_m);
    }

    return 0.0;
}

double ProfessionalDataAnalyzer::calculate_approximate_entropy(
    const std::vector<double>& series,
    int m,
    double r) {

    // Similar to sample entropy but with different normalization
    return calculate_sample_entropy(series, m, r);
}

void ProfessionalDataAnalyzer::detect_time_series_anomalies(
    ProfessionalTimeSeriesAnalysis::TimeSeriesAnomalies& anomalies,
    const std::vector<double>& series) {

    if (series.size() < 20) {
        return;
    }

    // Simple anomaly detection using rolling statistics
    size_t window = std::min((size_t)10, series.size() / 10);

    std::vector<double> rolling_mean(series.size(), 0.0);
    std::vector<double> rolling_std(series.size(), 0.0);

    for (size_t i = 0; i < series.size(); ++i) {
        size_t start = (i > window) ? i - window : 0;
        size_t end = std::min(i + window + 1, series.size());

        double sum = 0.0, sum_sq = 0.0;
        for (size_t j = start; j < end; ++j) {
            sum += series[j];
            sum_sq += series[j] * series[j];
        }

        size_t count = end - start;
        rolling_mean[i] = sum / count;
        double variance = (sum_sq / count) - (rolling_mean[i] * rolling_mean[i]);
        rolling_std[i] = std::sqrt(std::max(0.0, variance));
    }

    // Detect point anomalies
    double threshold = 3.0; // 3 sigma
    for (size_t i = 0; i < series.size(); ++i) {
        if (rolling_std[i] > 0) {
            double z_score = std::abs(series[i] - rolling_mean[i]) / rolling_std[i];
            if (z_score > threshold) {
                anomalies.point_anomaly_indices.push_back(i);
            }
        }
    }

    // Detect trend change points using simple differencing
    std::vector<double> differences;
    for (size_t i = 1; i < series.size(); ++i) {
        differences.push_back(series[i] - series[i-1]);
    }

    // Look for significant changes in differences
    if (differences.size() > 10) {
        double diff_mean = ProfessionalStatisticalCalculator::calculate_mean(differences);
        double diff_std = ProfessionalStatisticalCalculator::calculate_std_dev(differences);

        for (size_t i = 1; i < differences.size(); ++i) {
            if (std::abs(differences[i] - diff_mean) > 2.0 * diff_std &&
                std::abs(differences[i-1] - diff_mean) <= 2.0 * diff_std) {
                anomalies.trend_change_points.push_back(i);
            }
        }
    }
}

void ProfessionalDataAnalyzer::suggest_time_series_models(
    ProfessionalTimeSeriesAnalysis::ModelSuggestions& suggestions,
    const ProfessionalTimeSeriesAnalysis& analysis) {

    // Determine best model based on characteristics

    bool has_seasonality = analysis.seasonality.has_seasonality;
    bool is_stationary = analysis.stationarity.is_stationary_adf;
    bool has_trend = (analysis.decomposition.trend_strength > 0.3);
    bool has_long_memory = analysis.autocorrelation.is_long_memory;
    bool is_chaotic = analysis.forecastability.is_chaotic;

    // ARIMA recommendations
    suggestions.arima_recommended = (is_stationary || !has_seasonality);

    // ETS recommendations
    suggestions.ets_recommended = (has_trend || has_seasonality);

    // Prophet recommendations
    suggestions.prophet_recommended = (has_seasonality || has_trend);

    // LSTM recommendations
    suggestions.lstm_recommended = (!is_stationary || is_chaotic || has_long_memory);

    // TBATS recommendations
    suggestions.tbats_recommended = (has_seasonality && analysis.seasonality.seasonal_periods.size() > 1);

    // Determine best model
    if (has_seasonality && has_trend) {
        suggestions.best_model_type = "Prophet";
    } else if (has_seasonality) {
        suggestions.best_model_type = "TBATS";
    } else if (!is_stationary) {
        suggestions.best_model_type = "LSTM";
    } else if (has_trend) {
        suggestions.best_model_type = "ETS";
    } else {
        suggestions.best_model_type = "ARIMA";
    }

    // Suggest parameters
    if (suggestions.best_model_type == "ARIMA") {
        suggestions.model_parameters = {
            {"p", "1"},
            {"d", std::to_string(analysis.stationarity.differencing_order)},
            {"q", "1"}
        };
    } else if (suggestions.best_model_type == "Prophet") {
        suggestions.model_parameters = {
            {"seasonality_mode", "additive"},
            {"changepoint_prior_scale", "0.05"},
            {"seasonality_prior_scale", "10.0"}
        };
    }
}

// Data quality assessment helper methods
void ProfessionalDataAnalyzer::analyze_completeness(
    ProfessionalDataQualityReport::Completeness& completeness,
    const std::vector<std::unordered_map<std::string, Datum>>& data,
    const std::vector<std::string>& columns) {

    completeness.total_cells = data.size() * columns.size();
    completeness.missing_cells = 0;

    // Calculate missing values per column
    std::map<std::string, size_t> column_missing;
    std::vector<size_t> rows_with_missing;

    for (size_t row_idx = 0; row_idx < data.size(); ++row_idx) {
        const auto& row = data[row_idx];
        bool row_has_missing = false;

        for (const std::string& col : columns) {
            auto it = row.find(col);
            if (it == row.end() || it->second.is_null()) {
                completeness.missing_cells++;
                column_missing[col]++;
                row_has_missing = true;
            }
        }

        if (row_has_missing) {
            rows_with_missing.push_back(row_idx);
        }
    }

    // Calculate percentages
    if (completeness.total_cells > 0) {
        completeness.missing_percentage = (completeness.missing_cells * 100.0) / completeness.total_cells;
    }

    // Identify columns with missing values
    for (const auto& [col, count] : column_missing) {
        completeness.columns_with_missing.push_back(col);
        completeness.column_missing_percentages[col] = (count * 100.0) / data.size();
    }

    // Determine missing pattern
    if (completeness.missing_percentage < 1.0) {
        completeness.missing_pattern = "MCAR"; // Missing Completely At Random
    } else if (completeness.columns_with_missing.size() == 1) {
        completeness.missing_pattern = "MAR"; // Missing At Random
    } else {
        completeness.missing_pattern = "MNAR"; // Missing Not At Random
    }

    completeness.rows_with_missing = rows_with_missing;

    if (data.size() > 0) {
        completeness.row_completeness = ((data.size() - rows_with_missing.size()) * 100.0) / data.size();
    }
}

void ProfessionalDataAnalyzer::analyze_consistency(
    ProfessionalDataQualityReport::Consistency& consistency,
    const std::vector<std::unordered_map<std::string, Datum>>& data,
    const std::vector<std::string>& columns) {

    if (data.empty()) {
        return;
    }

    // Check type consistency per column
    for (const std::string& col : columns) {
        std::string first_type = "";
        bool type_inconsistent = false;

        for (const auto& row : data) {
            auto it = row.find(col);
            if (it != row.end() && !it->second.is_null()) {
                std::string current_type = is_numeric_datum(it->second) ? "numeric" : "string";

                if (first_type.empty()) {
                    first_type = current_type;
                } else if (current_type != first_type) {
                    type_inconsistent = true;
                    break;
                }
            }
        }

        if (type_inconsistent) {
            consistency.type_inconsistencies.push_back(col);
        }
    }

    // Simple range check for numeric columns
    for (const std::string& col : columns) {
        bool has_numeric = false;
        double min_val = std::numeric_limits<double>::max();
        double max_val = std::numeric_limits<double>::lowest();

        for (const auto& row : data) {
            auto it = row.find(col);
            if (it != row.end() && !it->second.is_null() && is_numeric_datum(it->second)) {
                has_numeric = true;
                double val = get_double_from_datum(it->second);
                min_val = std::min(min_val, val);
                max_val = std::max(max_val, val);
            }
        }

        if (has_numeric) {
            // Check for unreasonable values
            if (min_val < -1e6 || max_val > 1e6) {
                consistency.range_violations.push_back(col + " has extreme values");
            }
        }
    }

    // Simple format check for string columns (check length)
    for (const std::string& col : columns) {
        size_t max_length = 0;

        for (const auto& row : data) {
            auto it = row.find(col);
            if (it != row.end() && !it->second.is_null() && !is_numeric_datum(it->second)) {
                std::string val = get_string_from_datum(it->second);
                max_length = std::max(max_length, val.length());
            }
        }

        if (max_length > 1000) {
            consistency.format_violations.push_back(col + " has very long strings");
        }
    }

    consistency.total_violations = consistency.type_inconsistencies.size() +
                                  consistency.range_violations.size() +
                                  consistency.format_violations.size();
}



void ProfessionalDataAnalyzer::analyze_accuracy(
    ProfessionalDataQualityReport::Accuracy& accuracy,
    const std::vector<std::unordered_map<std::string, Datum>>& data,
    const std::vector<std::string>& columns) {

    // Simplified accuracy assessment
    // In production, this would compare with ground truth or use domain rules

    // Check for obvious data entry errors
    for (const std::string& col : columns) {
        if (col.find("age") != std::string::npos) {
            for (const auto& row : data) {
                auto it = row.find(col);
                if (it != row.end() && !it->second.is_null() && is_numeric_datum(it->second)) {
                    double age = get_double_from_datum(it->second);
                    if (age > 150 || age < 0) {
                        accuracy.accuracy_issues.push_back("Invalid age in " + col);
                    }
                }
            }
        } else if (col.find("date") != std::string::npos || col.find("time") != std::string::npos) {
            // Check for future dates if timestamp is available
            // Simplified check
        }
    }

    // Calculate self-consistency (check if derived columns match)
    accuracy.self_consistency = 0.8; // Placeholder
    accuracy.ground_truth_accuracy = 0.9; // Placeholder
    accuracy.cross_source_agreement = 0.85; // Placeholder
}

void ProfessionalDataAnalyzer::analyze_uniqueness(
    ProfessionalDataQualityReport::Uniqueness& uniqueness,
    const std::vector<std::unordered_map<std::string, Datum>>& data) {

    uniqueness.total_rows = data.size();

    // Simple duplicate detection (exact match)
    std::set<std::string> unique_rows;
    std::map<std::string, std::vector<size_t>> row_groups;

    for (size_t i = 0; i < data.size(); ++i) {
        std::string row_signature;
        for (const auto& [col, value] : data[i]) {
            if (!value.is_null()) {
                row_signature += col + ":" + get_string_from_datum(value) + "|";
            }
        }

        unique_rows.insert(row_signature);
        row_groups[row_signature].push_back(i);
    }

    uniqueness.duplicate_rows = data.size() - unique_rows.size();
    uniqueness.duplicate_percentage = (uniqueness.duplicate_rows * 100.0) / data.size();

    // Identify duplicate groups
    for (const auto& [signature, indices] : row_groups) {
        if (indices.size() > 1) {
            uniqueness.duplicate_groups.push_back(indices);
        }
    }

    // Check for candidate keys (columns with all unique values)
    if (!data.empty()) {
        std::unordered_map<std::string, Datum> first_row = data[0];
        for (const auto& [col, _] : first_row) {
            std::set<std::string> unique_values;
            bool all_unique = true;

            for (const auto& row : data) {
                auto it = row.find(col);
                if (it != row.end() && !it->second.is_null()) {
                    std::string val = get_string_from_datum(it->second);
                    if (unique_values.find(val) != unique_values.end()) {
                        all_unique = false;
                        break;
                    }
                    unique_values.insert(val);
                } else {
                    all_unique = false;
                    break;
                }
            }

            if (all_unique) {
                uniqueness.candidate_keys.push_back(col);
            }
        }
    }

    uniqueness.has_primary_key = !uniqueness.candidate_keys.empty();
}

void ProfessionalDataAnalyzer::analyze_validity(
    ProfessionalDataQualityReport::Validity& validity,
    const std::vector<std::unordered_map<std::string, Datum>>& data,
    const std::vector<std::string>& columns) {

    // Simple validity checks based on column names
    for (const std::string& col : columns) {
        // Email pattern
        if (col.find("email") != std::string::npos || col.find("mail") != std::string::npos) {
            std::regex email_pattern(R"([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})");

            for (const auto& row : data) {
                auto it = row.find(col);
                if (it != row.end() && !it->second.is_null() && !is_numeric_datum(it->second)) {
                    std::string val = get_string_from_datum(it->second);
                    if (!std::regex_match(val, email_pattern)) {
                        validity.pattern_violations.push_back("Invalid email in " + col);
                    }
                }
            }
        }

        // URL pattern
        if (col.find("url") != std::string::npos || col.find("website") != std::string::npos) {
            std::regex url_pattern(R"((https?://)?([a-zA-Z0-9-]+\.)+[a-zA-Z]{2,}(/\S*)?)");

            for (const auto& row : data) {
                auto it = row.find(col);
                if (it != row.end() && !it->second.is_null() && !is_numeric_datum(it->second)) {
                    std::string val = get_string_from_datum(it->second);
                    if (!val.empty() && !std::regex_match(val, url_pattern)) {
                        validity.pattern_violations.push_back("Invalid URL in " + col);
                    }
                }
            }
        }

        // Numeric range checks
        if (col.find("percentage") != std::string::npos || col.find("percent") != std::string::npos) {
            for (const auto& row : data) {
                auto it = row.find(col);
                if (it != row.end() && !it->second.is_null() && is_numeric_datum(it->second)) {
                    double val = get_double_from_datum(it->second);
                    if (val < 0 || val > 100) {
                        validity.domain_violations.push_back("Percentage out of range in " + col);
                    }
                }
            }
        }
    }

    validity.total_invalid_values = validity.business_rule_violations.size() +
                                   validity.domain_violations.size() +
                                   validity.pattern_violations.size();

    if (data.size() > 0) {
        validity.validity_rate = 1.0 - (validity.total_invalid_values / (data.size() * columns.size()));
    }
}

void ProfessionalDataAnalyzer::analyze_timeliness(
    ProfessionalDataQualityReport::Timeliness& timeliness,
    const std::vector<std::unordered_map<std::string, Datum>>& data) {

    // Simplified timeliness analysis
    // In production, use actual timestamps

    timeliness.data_freshness = 24.0; // Assume 24 hours old
    timeliness.update_frequency = 1.0; // Assume daily updates
    timeliness.latency = 1.0; // Assume 1 hour processing latency
    timeliness.meets_sla = (timeliness.latency < 2.0); // SLA: 2 hours
}

void ProfessionalDataAnalyzer::calculate_data_quality_scores(
    ProfessionalDataQualityReport& report) {

    // Calculate individual scores
    report.scores.completeness = 1.0 - (report.completeness.missing_percentage / 100.0);

    // Consistency score
    double consistency_penalty = std::min(1.0, report.consistency.total_violations / 100.0);
    report.scores.consistency = 1.0 - consistency_penalty;

    // Accuracy score (placeholder)
    report.scores.accuracy = report.accuracy.ground_truth_accuracy;

    // Timeliness score
    report.scores.timeliness = (report.timeliness.meets_sla ? 0.9 : 0.5);

    // Validity score
    report.scores.validity = report.validity.validity_rate;

    // Uniqueness score
    report.scores.uniqueness = 1.0 - (report.uniqueness.duplicate_percentage / 100.0);

    // Calculate weighted overall score
    const double weights[] = {0.2, 0.15, 0.15, 0.1, 0.2, 0.2}; // completeness, consistency, accuracy, timeliness, validity, uniqueness

    report.scores.overall =
        weights[0] * report.scores.completeness +
        weights[1] * report.scores.consistency +
        weights[2] * report.scores.accuracy +
        weights[3] * report.scores.timeliness +
        weights[4] * report.scores.validity +
        weights[5] * report.scores.uniqueness;

    // Calculate weighted scores
    report.scores.weighted.completeness = report.scores.completeness * weights[0];
    report.scores.weighted.consistency = report.scores.consistency * weights[1];
    report.scores.weighted.accuracy = report.scores.accuracy * weights[2];
    report.scores.weighted.timeliness = report.scores.timeliness * weights[3];
    report.scores.weighted.validity = report.scores.validity * weights[4];
    report.scores.weighted.uniqueness = report.scores.uniqueness * weights[5];
    report.scores.weighted.overall = report.scores.overall;
}

void ProfessionalDataAnalyzer::identify_data_quality_issues(
    ProfessionalDataQualityReport::Issues& issues,
    const ProfessionalDataQualityReport& report) {

    // Critical issues
    if (report.completeness.missing_percentage > 20.0) {
        issues.critical.push_back("More than 20% of data is missing");
    }

    if (report.uniqueness.duplicate_percentage > 10.0) {
        issues.critical.push_back("More than 10% of rows are duplicates");
    }

    if (report.scores.validity < 0.7) {
        issues.critical.push_back("Less than 70% of data passes validity checks");
    }

    // High priority issues
    if (report.completeness.missing_percentage > 10.0) {
        issues.high.push_back("More than 10% of data is missing");
    }

    if (report.consistency.total_violations > 50) {
        issues.high.push_back("More than 50 consistency violations found");
    }

    // Medium priority issues
    if (report.completeness.missing_percentage > 5.0) {
        issues.medium.push_back("More than 5% of data is missing");
    }

    if (!report.timeliness.meets_sla) {
        issues.medium.push_back("Data processing latency exceeds SLA");
    }

    // Low priority issues
    if (report.completeness.missing_percentage > 1.0) {
        issues.low.push_back("More than 1% of data is missing");
    }

    if (report.consistency.total_violations > 10) {
        issues.low.push_back("More than 10 consistency violations found");
    }

    issues.total_issues = issues.critical.size() + issues.high.size() +
                         issues.medium.size() + issues.low.size();
}

void ProfessionalDataAnalyzer::generate_data_quality_recommendations(
    ProfessionalDataQualityReport::Recommendations& recommendations,
    const ProfessionalDataQualityReport& report) {

    // Immediate recommendations (critical issues)
    if (report.completeness.missing_percentage > 20.0) {
        recommendations.immediate.push_back(
            "Investigate source systems for missing data collection");
    }

    if (report.uniqueness.duplicate_percentage > 10.0) {
        recommendations.immediate.push_back(
            "Implement deduplication process before further analysis");
    }

    // Short-term recommendations (high/medium issues)
    if (report.completeness.missing_percentage > 5.0) {
        recommendations.short_term.push_back(
            "Implement data imputation strategies for missing values");
    }

    if (report.consistency.total_violations > 10) {
        recommendations.short_term.push_back(
            "Add data validation rules at ingestion point");
    }

    // Long-term recommendations
    if (report.scores.overall < 0.8) {
        recommendations.long_term.push_back(
            "Establish data governance framework with clear ownership");
        recommendations.long_term.push_back(
            "Implement automated data quality monitoring system");
    }

    // Monitoring recommendations
    recommendations.monitoring.push_back(
        "Set up regular data quality scorecard reporting");
    recommendations.monitoring.push_back(
        "Monitor key data quality metrics daily");
}

void ProfessionalDataAnalyzer::generate_insights_and_recommendations_professional(
    ProfessionalComprehensiveAnalysisReport& report,
    const std::string& target_column,
    const std::map<std::string, std::string>& options) {

    // Generate insights from the analysis

    // Data Quality Insights
    if (report.data_quality.scores.overall < 0.7) {
        report.insights.data_quality.push_back(
            "Data quality needs improvement. Overall score: " +
            std::to_string(static_cast<int>(report.data_quality.scores.overall * 100)) + "%");
    } else if (report.data_quality.scores.overall >= 0.9) {
        report.insights.data_quality.push_back(
            "Excellent data quality. Ready for production use.");
    }

    if (report.data_quality.completeness.missing_percentage > 5.0) {
        report.insights.data_quality.push_back(
            "Significant missing data detected: " +
            std::to_string(static_cast<int>(report.data_quality.completeness.missing_percentage)) + "%");
    }

    // Statistical Insights
    if (!report.column_analyses.empty()) {
        // Find columns with outliers
        for (const auto& [col_name, analysis] : report.column_analyses) {
            if (analysis.has_outliers && analysis.outlier_percentage > 1.0) {
                report.insights.statistical.push_back(
                    "Column '" + col_name + "' has " +
                    std::to_string(static_cast<int>(analysis.outlier_percentage)) +
                    "% outliers");
            }

            if (analysis.is_normal) {
                report.insights.statistical.push_back(
                    "Column '" + col_name + "' follows normal distribution");
            }
        }
    }

    // Correlation Insights
    if (!report.correlations.empty()) {
        // Find strongest correlations
        std::vector<ProfessionalCorrelationAnalysis> sorted_corrs = report.correlations;
        std::sort(sorted_corrs.begin(), sorted_corrs.end(),
                 [](const auto& a, const auto& b) {
                     return std::abs(a.pearson_r) > std::abs(b.pearson_r);
                 });

        for (size_t i = 0; i < std::min((size_t)3, sorted_corrs.size()); ++i) {
            if (std::abs(sorted_corrs[i].pearson_r) > 0.7) {
                report.insights.statistical.push_back(
                    "Strong correlation between '" + sorted_corrs[i].column1 +
                    "' and '" + sorted_corrs[i].column2 + "' (r=" +
                    std::to_string(sorted_corrs[i].pearson_r) + ")");
            }
        }
    }

    // Feature Importance Insights
    if (!report.feature_importance.empty() && !target_column.empty()) {
        // Find most important feature
        auto max_it = std::max_element(report.feature_importance.begin(),
                                      report.feature_importance.end(),
                                      [](const auto& a, const auto& b) {
                                          return a.scores.random_forest < b.scores.random_forest;
                                      });

        if (max_it != report.feature_importance.end() &&
            max_it->scores.random_forest > 0.1) {
            report.insights.predictive.push_back(
                "Most important feature for predicting '" + target_column + "': '" +
                max_it->feature_name + "'");
        }
    }

    // Clustering Insights
    if (!report.clusters.empty()) {
        report.insights.statistical.push_back(
            "Data naturally forms " + std::to_string(report.clusters.size()) + " clusters");

        // Check for imbalanced clusters
        size_t max_size = 0, min_size = std::numeric_limits<size_t>::max();
        for (const auto& cluster : report.clusters) {
            max_size = std::max(max_size, cluster.size);
            min_size = std::min(min_size, cluster.size);
        }

        if (min_size > 0 && (max_size / min_size) > 5) {
            report.insights.optimization.push_back(
                "Highly imbalanced clusters detected. Consider cluster balancing techniques.");
        }
    }

    // Outlier Insights
    if (!report.outliers.empty() && !report.outliers[0].outlier_indices.empty()) {
        size_t outlier_count = report.outliers[0].outlier_indices.size();
        double outlier_percentage = (outlier_count * 100.0) / report.row_count;

        if (outlier_percentage > 1.0) {
            report.insights.anomaly.push_back(
                std::to_string(outlier_count) + " outliers detected (" +
                std::to_string(static_cast<int>(outlier_percentage)) + "%)");
        }
    }

    // Business Insights
    if (report.row_count > 1000) {
        report.insights.business.push_back(
            "Dataset contains " + std::to_string(report.row_count) +
            " records, providing good statistical power");
    }

    if (report.column_count > 20) {
        report.insights.business.push_back(
            "Rich feature set with " + std::to_string(report.column_count) + " columns");
    }

    // Generate recommendations
    generate_comprehensive_recommendations(report.recommendations, report);
}

void ProfessionalDataAnalyzer::generate_comprehensive_recommendations(
    ProfessionalComprehensiveAnalysisReport::Recommendations& recommendations,
    const ProfessionalComprehensiveAnalysisReport& report) {

    ProfessionalComprehensiveAnalysisReport::Recommendations::Recommendation rec;

    // Data Quality Recommendations
    if (report.data_quality.scores.overall < 0.7) {
        rec.title = "Improve Data Quality";
        rec.description = "Address data quality issues before building models";
        rec.category = "data_quality";
        rec.priority = "critical";
        rec.expected_impact = 0.8;
        rec.implementation_effort = 0.6;
        rec.steps = {
            "Review data quality report",
            "Address critical missing data issues",
            "Implement data validation rules",
            "Establish data quality monitoring"
        };
        recommendations.critical.push_back(rec);
        recommendations.all.push_back(rec);
    }

    // Feature Engineering Recommendations
    if (!report.feature_importance.empty()) {
        rec.title = "Optimize Feature Set";
        rec.description = "Select and engineer features based on importance analysis";
        rec.category = "feature_engineering";
        rec.priority = "high";
        rec.expected_impact = 0.6;
        rec.implementation_effort = 0.4;
        rec.steps = {
            "Drop features with importance < 0.1",
            "Apply suggested transformations",
            "Create interaction features",
            "Encode categorical variables"
        };
        recommendations.high.push_back(rec);
        recommendations.all.push_back(rec);
    }

    // Modeling Recommendations
    if (!report.correlations.empty() && !report.feature_importance.empty()) {
        rec.title = "Build Predictive Model";
        rec.description = "Leverage strong correlations and feature importance for prediction";
        rec.category = "modeling";
        rec.priority = "medium";
        rec.expected_impact = 0.7;
        rec.implementation_effort = 0.5;
        rec.steps = {
            "Select appropriate algorithm",
            "Split data into train/validation/test",
            "Train model with cross-validation",
            "Evaluate and tune model"
        };
        recommendations.medium.push_back(rec);
        recommendations.all.push_back(rec);
    }

    // Monitoring Recommendations
    rec.title = "Establish Monitoring";
    rec.description = "Set up ongoing monitoring of data and model performance";
    rec.category = "monitoring";
    rec.priority = "low";
    rec.expected_impact = 0.4;
    rec.implementation_effort = 0.3;
    rec.steps = {
        "Define key metrics to monitor",
        "Set up automated reporting",
        "Establish alert thresholds",
        "Schedule regular reviews"
    };
    recommendations.low.push_back(rec);
    recommendations.all.push_back(rec);
}

// Streaming analysis (stub implementation)
ProfessionalComprehensiveAnalysisReport ProfessionalDataAnalyzer::analyze_data_streaming(
    const std::string& db_name,
    const std::string& table_name,
    const std::string& target_column,
    const std::vector<std::string>& feature_columns,
    const std::string& analysis_type,
    const std::map<std::string, std::string>& options) {

    // This would connect to the database and stream data
    // For now, return empty report
    ProfessionalComprehensiveAnalysisReport report;
    report.analysis_id = "streaming_" + generate_analysis_id();
    report.table_name = table_name;

    std::cout << "[ProfessionalDataAnalyzer] Streaming analysis not fully implemented" << std::endl;

    return report;
}

// Incremental analysis (stub implementation)
ProfessionalComprehensiveAnalysisReport ProfessionalDataAnalyzer::analyze_data_incremental(
    const ProfessionalComprehensiveAnalysisReport& previous_report,
    const std::vector<std::unordered_map<std::string, Datum>>& new_data,
    const std::map<std::string, std::string>& options) {

    // This would update the previous analysis with new data
    // For now, perform fresh analysis
    ProfessionalComprehensiveAnalysisReport report = previous_report;
    report.analysis_id = "incremental_" + generate_analysis_id();

    std::cout << "[ProfessionalDataAnalyzer] Incremental analysis not fully implemented" << std::endl;

    return report;
}

// Comparative analysis (stub implementation)
ProfessionalComprehensiveAnalysisReport ProfessionalDataAnalyzer::analyze_data_comparative(
    const std::vector<std::unordered_map<std::string, Datum>>& data1,
    const std::vector<std::unordered_map<std::string, Datum>>& data2,
    const std::string& analysis_type,
    const std::map<std::string, std::string>& options) {

    ProfessionalComprehensiveAnalysisReport report;
    report.analysis_id = "comparative_" + generate_analysis_id();
    report.table_name = "comparative_analysis";

    // Perform analysis on both datasets
    auto report1 = analyze_data(data1, "", {}, "SUMMARY", options);
    auto report2 = analyze_data(data2, "", {}, "SUMMARY", options);

    // Compare key metrics
    report.insights.statistical.push_back(
        "Dataset 1: " + std::to_string(data1.size()) + " rows, " +
        "Dataset 2: " + std::to_string(data2.size()) + " rows");

    if (!data1.empty() && !data2.empty()) {
        size_t cols1 = data1[0].size();
        size_t cols2 = data2[0].size();

        if (cols1 == cols2) {
            report.insights.statistical.push_back("Both datasets have " + std::to_string(cols1) + " columns");
        } else {
            report.insights.statistical.push_back(
                "Different column counts: " + std::to_string(cols1) + " vs " + std::to_string(cols2));
        }
    }

    return report;
}

} // namespace analysis
} // namespace esql
